{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#tribridrag-tri-brid-rag-engine","title":"TriBridRAG \u2014 Tri-Brid RAG Engine","text":"<p>TriBridRAG is a tri-brid Retrieval-Augmented Generation (RAG) engine for code understanding. \u201cTri-brid\u201d means we run three retrieval legs in parallel\u2014vector, sparse, and graph\u2014then fuse their results into a single ranked candidate set for answering and analysis.</p> <p>This design is intentional: code search quality fails in different ways depending on the query. TriBridRAG reduces those failure modes by combining complementary signals:</p> <ul> <li>Vector search (pgvector in PostgreSQL) captures semantic similarity (e.g., \u201cauth flow\u201d \u2192 \u201ctoken exchange\u201d).</li> <li>Sparse search (PostgreSQL FTS / BM25-style ranking) captures exact terms, identifiers, and filenames.</li> <li>Graph search (Neo4j) captures relationships between entities (calls/imports/inherits/contains/references) and can surface \u201cconnected\u201d code even when lexical similarity is low.</li> </ul> <p>Pydantic is the law</p> <p>TriBridRAG\u2019s behavior is defined by <code>server/models/tribrid_config_model.py</code>. If a feature is configurable, it exists there first. The frontend types are generated from that model\u2014never hand-written.</p>"},{"location":"#tri-brid-retrieval-pipeline","title":"Tri-brid retrieval pipeline","text":"<pre><code>flowchart LR\n    Query[User Query]:::input --&gt; Vector[\"Vector Search\\n(pgvector in PostgreSQL)\"]:::search\n    Query --&gt; Sparse[\"Sparse Search\\n(PostgreSQL FTS/BM25)\"]:::search\n    Query --&gt; Graph[\"Graph Search\\n(Neo4j traversal)\"]:::search\n\n    Vector --&gt; Fusion[\"Fusion Layer\\n(RRF or Weighted Scoring)\"]:::process\n    Sparse --&gt; Fusion\n    Graph --&gt; Fusion\n\n    Fusion --&gt; Rerank[\"Reranker\\n(local/cloud/trained/none)\"]:::process\n    Rerank --&gt; Results[\"Final Results\\n(chunks + citations)\"]:::output\n\n    classDef input fill:#3b82f6,stroke:#2563eb,stroke-width:3px,color:#fff\n    classDef search fill:#8b5cf6,stroke:#7c3aed,stroke-width:2px,color:#fff\n    classDef process fill:#ec4899,stroke:#db2777,stroke-width:2px,color:#fff\n    classDef output fill:#10b981,stroke:#059669,stroke-width:3px,color:#fff</code></pre>"},{"location":"#screenshots","title":"Screenshots","text":"<p>Live Demo</p> <p>Visit the running application at http://localhost:5175/ to explore all features.</p>"},{"location":"#rag-configuration-interface","title":"RAG Configuration Interface","text":""},{"location":"#search-results","title":"Search &amp; Results","text":""},{"location":"#knowledge-graph-visualization","title":"Knowledge Graph Visualization","text":""},{"location":"#model-selection-cost-tracking","title":"Model Selection &amp; Cost Tracking","text":""},{"location":"#key-features","title":"Key features","text":"<ul> <li>Three retrieval legs, fused</li> <li> <p>Vector + sparse + graph retrieval run together and are combined via Reciprocal Rank Fusion (RRF) or weighted scoring.</p> </li> <li> <p>Single-database simplicity for dense + sparse</p> </li> <li> <p>Dense embeddings live in PostgreSQL via pgvector, and sparse retrieval uses PostgreSQL Full-Text Search\u2014fewer moving parts, consistent operational model.</p> </li> <li> <p>Graph RAG with Neo4j</p> </li> <li> <p>Entities (functions/classes/modules/etc.), relationships (calls/imports/inherits/contains/references), and optional community detection support graph-aware retrieval.</p> </li> <li> <p>Config-driven behavior</p> </li> <li> <p>Thresholds, weights, top-k values, timeouts, and feature flags are all controlled through the Pydantic config model.</p> </li> <li> <p>Type-safe frontend</p> </li> <li> <p>TypeScript types are generated from Pydantic (via <code>scripts/generate_types.py</code>). The UI imports <code>web/src/types/generated.ts</code> and does not define parallel interfaces.</p> </li> <li> <p>Optional reranking</p> </li> <li>After fusion, candidates can be reranked using a local cross-encoder, a cloud provider, a learning-based model, or disabled entirely.</li> </ul> <p>When to expect the biggest gains</p> <ul> <li>Sparse search excels at exact identifiers, filenames, and error strings.  </li> <li>Vector search excels at conceptual queries and paraphrases.  </li> <li>Graph search excels at \u201cwhat calls/uses this?\u201d and \u201cwhere does this flow go next?\u201d questions.</li> </ul>"},{"location":"#quick-start","title":"Quick start","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>Node.js 18+</li> <li>Docker + Docker Compose</li> <li><code>uv</code> (Python package manager)</li> </ul>"},{"location":"#commands","title":"Commands","text":"<pre><code>git clone https://github.com/your-org/tribrid-rag.git\ncd tribrid-rag\ncp .env.example .env\ndocker compose up -d postgres neo4j grafana\nuv sync\nuv run uvicorn server.main:app --reload\n</code></pre> <p>Frontend (optional, separate terminal):</p> <pre><code>cd web\nnpm install\nnpm run dev\n</code></pre>"},{"location":"#core-api-endpoints","title":"Core API endpoints","text":"Endpoint Method Purpose <code>/search</code> POST Tri-brid retrieval + fusion (optionally reranked) <code>/answer</code> POST RAG answer generation with citations <code>/index</code> POST Index a repository (chunking + embeddings + graph build) <code>/graph/{repo_id}/entities</code> GET Inspect graph entities <code>/config</code> GET/PUT Read/update config (validated by Pydantic) <code>/eval/run</code> POST Run evaluation suite <p>Observability</p> <ul> <li>FastAPI docs: <code>http://localhost:8000/docs</code> </li> <li>Grafana: <code>http://localhost:3000</code> (admin/admin)  </li> <li>Prometheus: <code>http://localhost:9090</code></li> </ul>"},{"location":"#configuration-the-derivation-chain-non-negotiable","title":"Configuration: the derivation chain (non-negotiable)","text":"<p>TriBridRAG treats configuration and types as a strict pipeline:</p> <ol> <li>Pydantic model defines the schema: <code>server/models/tribrid_config_model.py</code></li> <li>TypeScript types are generated: <code>web/src/types/generated.ts</code></li> <li>UI stores/hooks/components import generated types (no parallel interfaces)</li> </ol> Click to expand: regenerate TypeScript types <pre><code>uv run python scripts/generate_types.py\n</code></pre> PythonTypeScript <pre><code># server/models/tribrid_config_model.py is the source of truth\nfrom server.models.tribrid_config_model import TriBridConfigRoot\n\ndef load_config(path: str) -&gt; TriBridConfigRoot:\n    # Pydantic validates types, ranges, and defaults\n    return TriBridConfigRoot.model_validate_json(open(path, \"r\").read())\n</code></pre> <pre><code>// web/src/types/generated.ts is generated from Pydantic\nimport type { TriBridConfigRoot } from \"../types/generated\";\n\n// Use generated types in stores/hooks/components (never hand-written interfaces)\n</code></pre> <p>Do not hand-write API/config types</p> <p>If the backend returns a shape the frontend doesn\u2019t like, we fix the Pydantic model and regenerate types. We do not add adapters/transformers to \u201cpatch\u201d mismatches.</p>"},{"location":"#architecture-highlights-from-tribrid_config_modelpy","title":"Architecture highlights (from <code>tribrid_config_model.py</code>)","text":"<p>The landing page is intentionally config-aware: these are the knobs that define TriBridRAG\u2019s behavior.</p>"},{"location":"#retrieval-vector-sparse-behavior","title":"Retrieval (vector + sparse behavior)","text":"<p>Key parameters (definition list):</p> <code>retrieval.topk_dense</code> Top-K candidates retrieved from dense vector search (pgvector). <code>retrieval.topk_sparse</code> Top-K candidates retrieved from sparse search (PostgreSQL FTS / BM25-style). <code>retrieval.bm25_k1</code> / <code>retrieval.bm25_b</code> BM25 tuning parameters (term frequency saturation and length normalization). <code>retrieval.query_expansion_enabled</code> / <code>retrieval.max_query_rewrites</code> Multi-query and rewrite controls for recall improvements. <code>retrieval.hydration_mode</code> / <code>retrieval.hydration_max_chars</code> Controls how much content is hydrated into results (lazy/eager/none). <p>Weight normalization behavior</p> <p>In <code>RetrievalConfig</code>, <code>bm25_weight</code> and <code>vector_weight</code> are normalized to sum to 1.0 (rather than hard failing). This prevents misconfiguration from silently breaking retrieval.</p>"},{"location":"#graph-storage-traversal-neo4j","title":"Graph storage + traversal (Neo4j)","text":"<code>graph_storage.max_hops</code> Maximum traversal depth for graph search (controls recall vs. noise). <code>graph_storage.entity_types</code> Entity extraction scope (e.g., function/class/module/variable/import). <code>graph_storage.relationship_types</code> Relationship extraction scope (calls/imports/inherits/contains/references). <code>graph_storage.graph_search_top_k</code> Candidate count returned from graph traversal before fusion. <code>graph_storage.include_communities</code> / <code>graph_storage.community_algorithm</code> Optional community detection for higher-level structure."},{"location":"#fusion-the-tri-brid-join-point","title":"Fusion (the \u201ctri-brid\u201d join point)","text":"<code>fusion.method</code> Fusion strategy: <code>rrf</code> or <code>weighted</code>. <code>fusion.vector_weight</code> / <code>fusion.sparse_weight</code> / <code>fusion.graph_weight</code> Tri-brid weights (normalized to sum to 1.0). <code>fusion.rrf_k</code> RRF smoothing constant. <code>fusion.normalize_scores</code> Normalize leg scores to [0,1] before weighted fusion. <p>Two fusion modes</p> <ul> <li>RRF is robust when each leg\u2019s score scales differ or are hard to calibrate.  </li> <li>Weighted scoring is useful when you trust score calibration and want explicit control.</li> </ul>"},{"location":"#reranking-optional-refinement","title":"Reranking (optional refinement)","text":"<code>reranking.reranker_mode</code> <code>local</code>, <code>cloud</code>, <code>learning</code>, or <code>none</code>. <code>reranking.tribrid_reranker_topn</code> / <code>reranking.reranker_cloud_top_n</code> How many fused candidates to rerank. <code>reranking.rerank_input_snippet_chars</code> How much text is sent to the reranker per candidate."},{"location":"#where-to-go-next","title":"Where to go next","text":"<p>Use these pages to deepen from \u201cwhat is it?\u201d to \u201chow do I operate it?\u201d:</p> <ul> <li>Configuration model (Pydantic is the law)</li> <li>Retrieval &amp; fusion (vector + sparse + graph)</li> <li>Graph RAG with Neo4j (entities, relationships, communities)</li> <li>Indexing pipeline (chunking, embeddings, chunk_summaries)</li> <li>Reranking (modes, performance, tuning)</li> <li>API reference (search, answer, index, config)</li> <li>Observability (metrics, dashboards, tracing)</li> </ul> <p>If it\u2019s not in Pydantic, it doesn\u2019t exist</p> <p>Before adding a UI control, endpoint field, or new behavior: add it to <code>server/models/tribrid_config_model.py</code>, regenerate TypeScript types, then implement. This is how TriBridRAG stays coherent as it grows.</p>"},{"location":"api/endpoints/","title":"Endpoints","text":""},{"location":"api/endpoints/#api-endpoints","title":"API Endpoints","text":"<p>This page documents the TriBridRAG HTTP API exposed by the FastAPI server. Endpoints are grouped by category and reflect the routers registered in <code>server/main.py</code> (see also: Configuration, Indexing, and Search &amp; Answering pages).</p> <p>Note</p> <p>Several endpoints are currently stubbed (<code>raise NotImplementedError</code>) in the source. The schemas and paths are still authoritative because FastAPI uses them for OpenAPI generation and client typing.</p> <p>Tip</p> <p>Pydantic is the law: request/response schemas are defined by Pydantic models (e.g., <code>server.models.retrieval.*</code>, <code>server.models.index.*</code>, <code>server.models.config.TriBridConfig</code>). TypeScript types must be generated from these models\u2014never hand-written. See: Configuration.</p>"},{"location":"api/endpoints/#search","title":"Search","text":"<p>Source: <code>server/api/search.py</code></p> <p>These endpoints drive retrieval and answering. Internally, TriBridRAG fuses three retrieval legs\u2014vector (pgvector), sparse (PostgreSQL FTS/BM25), and graph (Neo4j)\u2014using either RRF or weighted scoring (see: Search &amp; Fusion).</p>"},{"location":"api/endpoints/#post-search","title":"<code>POST /search</code>","text":"<p>Description Run retrieval and return ranked results (typically chunks and metadata) without generating a final answer.</p> Request body schema: <code>SearchRequest</code> (Pydantic) Defined in <code>server.models.retrieval.SearchRequest</code>. Response schema: <code>SearchResponse</code> (Pydantic) Defined in <code>server.models.retrieval.SearchResponse</code>. Click to expand: Example curl <pre><code>curl -sS -X POST \"http://localhost:8000/search\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"query\": \"How does TriBridRAG fuse vector, sparse, and graph retrieval?\"\n  }'\n</code></pre> Click to expand: Example response <pre><code>{\n  \"results\": [\n    {\n      \"id\": \"chunk_001\",\n      \"score\": 0.873,\n      \"text\": \"TriBridRAG combines vector search, sparse search, and graph search...\",\n      \"metadata\": {\n        \"repo_id\": \"example-repo\",\n        \"path\": \"docs/architecture.md\",\n        \"chunk_index\": 12\n      }\n    }\n  ],\n  \"debug\": {\n    \"fusion_method\": \"rrf\",\n    \"legs\": {\n      \"vector\": { \"hits\": 10 },\n      \"sparse\": { \"hits\": 10 },\n      \"graph\": { \"hits\": 5 }\n    }\n  }\n}\n</code></pre> <p>Warning</p> <p>The exact JSON fields depend on <code>SearchResponse</code>. Treat the example as illustrative; the authoritative schema is the Pydantic model.</p>"},{"location":"api/endpoints/#post-answer","title":"<code>POST /answer</code>","text":"<p>Description Run retrieval + generation and return a final answer (non-streaming). This is the standard \u201cRAG answer\u201d endpoint.</p> Request body schema: <code>AnswerRequest</code> (Pydantic) Defined in <code>server.models.retrieval.AnswerRequest</code>. Response schema: <code>AnswerResponse</code> (Pydantic) Defined in <code>server.models.retrieval.AnswerResponse</code>. Click to expand: Example curl <pre><code>curl -sS -X POST \"http://localhost:8000/answer\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"query\": \"What are the three retrieval legs in TriBridRAG?\",\n    \"repo_id\": \"example-repo\"\n  }'\n</code></pre> Click to expand: Example response <pre><code>{\n  \"answer\": \"TriBridRAG fuses three retrieval legs: vector search (pgvector in PostgreSQL), sparse search (PostgreSQL full-text/BM25), and graph search (Neo4j entity relationships).\",\n  \"citations\": [\n    {\n      \"id\": \"chunk_014\",\n      \"path\": \"docs/overview.md\",\n      \"score\": 0.81\n    }\n  ],\n  \"debug\": {\n    \"fusion_method\": \"weighted\",\n    \"model\": \"provider:model-name\"\n  }\n}\n</code></pre>"},{"location":"api/endpoints/#post-answerstream","title":"<code>POST /answer/stream</code>","text":"<p>Description Run retrieval + generation and stream the answer incrementally.</p> Request body schema: <code>AnswerRequest</code> (Pydantic) Defined in <code>server.models.retrieval.AnswerRequest</code>. Response schema: <code>StreamingResponse</code> Starlette streaming response. The stream format (SSE vs newline-delimited JSON vs raw tokens) is an implementation detail of the server. Click to expand: Example curl <pre><code>curl -N -sS -X POST \"http://localhost:8000/answer/stream\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"query\": \"Explain Reciprocal Rank Fusion in TriBridRAG.\",\n    \"repo_id\": \"example-repo\"\n  }'\n</code></pre> Click to expand: Example response (illustrative stream) <pre><code>data: {\"type\":\"token\",\"value\":\"Reciprocal\"}\ndata: {\"type\":\"token\",\"value\":\" Rank\"}\ndata: {\"type\":\"token\",\"value\":\" Fusion\"}\ndata: {\"type\":\"final\",\"answer\":\"Reciprocal Rank Fusion (RRF) combines ranked lists by summing reciprocal ranks...\"}\n</code></pre> <p>Danger</p> <p>Do not build clients that assume a specific streaming payload without checking the server implementation. Treat the stream as opaque unless the project defines a formal streaming contract.</p>"},{"location":"api/endpoints/#indexing","title":"Indexing","text":"<p>Source: <code>server/api/index.py</code></p> <p>Indexing builds/updates the retrieval stores used by TriBridRAG: - Vector: embeddings stored in PostgreSQL via pgvector - Sparse: PostgreSQL full-text / BM25-compatible ranking - Graph: Neo4j entities, relationships, and communities</p>"},{"location":"api/endpoints/#post-index","title":"<code>POST /index</code>","text":"<p>Description Start an indexing job for a repository/dataset.</p> Request body schema: <code>IndexRequest</code> (Pydantic) Defined in <code>server.models.index.IndexRequest</code>. Response schema: <code>IndexStatus</code> (Pydantic) Defined in <code>server.models.index.IndexStatus</code>. Click to expand: Example curl <pre><code>curl -sS -X POST \"http://localhost:8000/index\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"repo_id\": \"example-repo\",\n    \"force_reindex\": false\n  }'\n</code></pre> Click to expand: Example response <pre><code>{\n  \"repo_id\": \"example-repo\",\n  \"status\": \"running\",\n  \"started_at\": \"2026-01-29T12:00:00Z\"\n}\n</code></pre>"},{"location":"api/endpoints/#get-indexrepo_idstatus","title":"<code>GET /index/{repo_id}/status</code>","text":"<p>Description Get the current indexing status for a repository.</p> Path parameters repo_id <code>str</code> \u2014 Repository identifier. Response schema: <code>IndexStatus</code> (Pydantic) Defined in <code>server.models.index.IndexStatus</code>. Click to expand: Example curl <pre><code>curl -sS \"http://localhost:8000/index/example-repo/status\"\n</code></pre> Click to expand: Example response <pre><code>{\n  \"repo_id\": \"example-repo\",\n  \"status\": \"complete\",\n  \"started_at\": \"2026-01-29T12:00:00Z\",\n  \"finished_at\": \"2026-01-29T12:03:10Z\"\n}\n</code></pre>"},{"location":"api/endpoints/#get-indexrepo_idstats","title":"<code>GET /index/{repo_id}/stats</code>","text":"<p>Description Return indexing statistics (counts, sizes, and other summary metrics).</p> Path parameters repo_id <code>str</code> \u2014 Repository identifier. Response schema: <code>IndexStats</code> (Pydantic) Defined in <code>server.models.index.IndexStats</code>. Click to expand: Example curl <pre><code>curl -sS \"http://localhost:8000/index/example-repo/stats\"\n</code></pre> Click to expand: Example response <pre><code>{\n  \"repo_id\": \"example-repo\",\n  \"documents\": 128,\n  \"chunks\": 2048,\n  \"entities\": 512,\n  \"relationships\": 2040\n}\n</code></pre>"},{"location":"api/endpoints/#delete-indexrepo_id","title":"<code>DELETE /index/{repo_id}</code>","text":"<p>Description Delete all index data for a repository (vector, sparse, and graph artifacts).</p> Path parameters repo_id <code>str</code> \u2014 Repository identifier. Request body schema None. Response schema <code>dict</code> (untyped JSON object). Click to expand: Example curl <pre><code>curl -sS -X DELETE \"http://localhost:8000/index/example-repo\"\n</code></pre> Click to expand: Example response <pre><code>{\n  \"ok\": true,\n  \"repo_id\": \"example-repo\",\n  \"deleted\": true\n}\n</code></pre> <p>Warning</p> <p>This is a destructive operation. Clients should confirm intent and/or require elevated permissions in production deployments.</p>"},{"location":"api/endpoints/#configuration","title":"Configuration","text":"<p>Source: <code>server/api/config.py</code></p> <p>TriBridRAG configuration is represented by a single Pydantic model: <code>TriBridConfig</code>.</p> <p>Note</p> <p>Pydantic is the law. All configuration flows from <code>tribrid_config_model.py</code> and is exposed via <code>TriBridConfig</code>. Any UI types must be generated from the Pydantic schema (see: Configuration).</p>"},{"location":"api/endpoints/#get-config","title":"<code>GET /config</code>","text":"<p>Description Return the current runtime configuration.</p> Request body schema None. Response schema: <code>TriBridConfig</code> (Pydantic) Defined in <code>server.models.config.TriBridConfig</code>. Click to expand: Example curl <pre><code>curl -sS \"http://localhost:8000/config\"\n</code></pre> Click to expand: Example response <pre><code>{\n  \"retrieval\": {\n    \"fusion_method\": \"rrf\",\n    \"top_k\": 20\n  },\n  \"vector\": {\n    \"enabled\": true\n  },\n  \"sparse\": {\n    \"enabled\": true\n  },\n  \"graph\": {\n    \"enabled\": true\n  }\n}\n</code></pre>"},{"location":"api/endpoints/#put-config","title":"<code>PUT /config</code>","text":"<p>Description Replace the entire configuration with the provided <code>TriBridConfig</code>.</p> Request body schema: <code>TriBridConfig</code> (Pydantic) Defined in <code>server.models.config.TriBridConfig</code>. <p>Response schema: <code>TriBridConfig</code> (Pydantic)</p> Click to expand: Example curl <pre><code>curl -sS -X PUT \"http://localhost:8000/config\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"retrieval\": { \"fusion_method\": \"weighted\", \"top_k\": 30 },\n    \"vector\": { \"enabled\": true },\n    \"sparse\": { \"enabled\": true },\n    \"graph\": { \"enabled\": true }\n  }'\n</code></pre> Click to expand: Example response <pre><code>{\n  \"retrieval\": { \"fusion_method\": \"weighted\", \"top_k\": 30 },\n  \"vector\": { \"enabled\": true },\n  \"sparse\": { \"enabled\": true },\n  \"graph\": { \"enabled\": true }\n}\n</code></pre> <p>Warning</p> <p><code>PUT /config</code> is a full replacement. If you only want to change one section, prefer <code>PATCH /config/{section}</code>.</p>"},{"location":"api/endpoints/#patch-configsection","title":"<code>PATCH /config/{section}</code>","text":"<p>Description Partially update a single configuration section.</p> Path parameters section <code>str</code> \u2014 Name of the config section to update (must correspond to a field in <code>TriBridConfig</code>). Request body schema: <code>dict</code> Arbitrary JSON object containing updates for the selected section. <p>Response schema: <code>TriBridConfig</code> (Pydantic)</p> Click to expand: Example curl <pre><code>curl -sS -X PATCH \"http://localhost:8000/config/retrieval\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"fusion_method\": \"rrf\",\n    \"top_k\": 25\n  }'\n</code></pre> Click to expand: Example response <pre><code>{\n  \"retrieval\": { \"fusion_method\": \"rrf\", \"top_k\": 25 },\n  \"vector\": { \"enabled\": true },\n  \"sparse\": { \"enabled\": true },\n  \"graph\": { \"enabled\": true }\n}\n</code></pre> <p>Danger</p> <p>Because <code>updates</code> is an untyped <code>dict</code> at the API boundary, server-side validation must be strict. The server should validate that: - <code>section</code> is a valid <code>TriBridConfig</code> field - <code>updates</code> keys/types match the corresponding Pydantic sub-model</p>"},{"location":"api/endpoints/#post-configreset","title":"<code>POST /config/reset</code>","text":"<p>Description Reset configuration to defaults.</p> Request body schema None. <p>Response schema: <code>TriBridConfig</code> (Pydantic)</p> Click to expand: Example curl <pre><code>curl -sS -X POST \"http://localhost:8000/config/reset\"\n</code></pre> Click to expand: Example response <pre><code>{\n  \"retrieval\": { \"fusion_method\": \"rrf\", \"top_k\": 20 },\n  \"vector\": { \"enabled\": true },\n  \"sparse\": { \"enabled\": true },\n  \"graph\": { \"enabled\": true }\n}\n</code></pre>"},{"location":"api/endpoints/#health","title":"Health","text":"<p>Source: <code>server/api/health.py</code></p> <p>Health endpoints are used for liveness/readiness checks and metrics scraping.</p>"},{"location":"api/endpoints/#get-health","title":"<code>GET /health</code>","text":"<p>Description Liveness probe. Confirms the API process is running.</p> Request body schema None. Response schema <code>dict</code> (untyped JSON object). Click to expand: Example curl <pre><code>curl -sS \"http://localhost:8000/health\"\n</code></pre> Click to expand: Example response <pre><code>{\n  \"status\": \"ok\"\n}\n</code></pre>"},{"location":"api/endpoints/#get-ready","title":"<code>GET /ready</code>","text":"<p>Description Readiness probe. Confirms dependencies are ready (e.g., PostgreSQL + pgvector, Neo4j).</p> Request body schema None. Response schema <code>dict</code> (untyped JSON object). Click to expand: Example curl <pre><code>curl -sS \"http://localhost:8000/ready\"\n</code></pre> Click to expand: Example response <pre><code>{\n  \"ready\": true,\n  \"dependencies\": {\n    \"postgres\": \"ok\",\n    \"neo4j\": \"ok\"\n  }\n}\n</code></pre> <p>Tip</p> <p>In production, this endpoint should fail fast if any required retrieval leg is enabled in config but its backing service is unavailable.</p>"},{"location":"api/endpoints/#get-metrics","title":"<code>GET /metrics</code>","text":"<p>Description Prometheus metrics endpoint.</p> Request body schema None. Response schema: <code>Response</code> Typically <code>text/plain; version=0.0.4</code> Prometheus exposition format. Click to expand: Example curl <pre><code>curl -sS \"http://localhost:8000/metrics\"\n</code></pre> Click to expand: Example response <pre><code># HELP tribridrag_requests_total Total HTTP requests\n# TYPE tribridrag_requests_total counter\ntribridrag_requests_total{path=\"/search\",method=\"POST\"} 123\n</code></pre>"},{"location":"api/endpoints/#related-pages","title":"Related pages","text":"<ul> <li>Search &amp; Fusion</li> <li>Indexing</li> <li>Configuration</li> <li>Graph RAG (Neo4j)</li> </ul> <p>Note</p> <p>Additional routers exist (chat, graph, eval, dataset, cost, docker, reranker, repos, models). This page documents the categories requested: Search, Indexing, Configuration, Health.</p>"},{"location":"assets/images/","title":"Screenshots Guide","text":""},{"location":"assets/images/#screenshots-guide","title":"Screenshots Guide","text":""},{"location":"assets/images/#how-to-add-screenshots-to-docs","title":"How to Add Screenshots to Docs","text":"<ol> <li>Take screenshots of the TriBridRAG UI at http://localhost:5175/</li> <li>Save them to this directory with descriptive names:</li> <li><code>rag-config-interface.png</code> - RAG tab with fusion weights</li> <li><code>search-results.png</code> - Search interface with results</li> <li><code>graph-visualization.png</code> - Neo4j graph view</li> <li><code>model-selector.png</code> - Model picker interface</li> <li> <p><code>cost-calculator.png</code> - Cost tracking dashboard</p> </li> <li> <p>Add to docs using this format:</p> </li> </ol> <pre><code>![RAG Configuration Interface](./assets/images/rag-config-interface.png)\n*Configure fusion weights, reranking, and search parameters through the intuitive UI*\n</code></pre> <p>Or with lightbox zoom:</p> <pre><code>&lt;figure markdown&gt;\n  ![RAG Configuration](./assets/images/rag-config-interface.png){ loading=lazy }\n  &lt;figcaption&gt;Configure tri-brid fusion weights and reranking options&lt;/figcaption&gt;\n&lt;/figure&gt;\n</code></pre>"},{"location":"assets/images/#recommended-screenshots","title":"Recommended Screenshots","text":""},{"location":"assets/images/#1-rag-configuration-tab","title":"1. RAG Configuration Tab","text":"<ul> <li>Show the sliders for fusion weights (vector/sparse/graph)</li> <li>Reranker dropdown</li> <li>Top-K settings</li> <li>Confidence thresholds</li> </ul>"},{"location":"assets/images/#2-search-interface","title":"2. Search Interface","text":"<ul> <li>Query input</li> <li>Results panel with chunks</li> <li>Relevance scores</li> <li>Citation links</li> </ul>"},{"location":"assets/images/#3-graph-visualization","title":"3. Graph Visualization","text":"<ul> <li>Neo4j entities and relationships</li> <li>Community detection</li> <li>Entity details panel</li> </ul>"},{"location":"assets/images/#4-model-management","title":"4. Model Management","text":"<ul> <li>Model picker dropdown (embedding/generation/reranker)</li> <li>Provider selection</li> <li>Cost calculator</li> <li>Context window info</li> </ul>"},{"location":"assets/images/#5-glossarytooltips","title":"5. Glossary/Tooltips","text":"<ul> <li>Show the tooltip system with definitions</li> <li>Searchable glossary tab</li> </ul>"},{"location":"assets/images/#screenshot-tips","title":"Screenshot Tips","text":"<ul> <li>Use dark mode (matches docs theme)</li> <li>Capture at ~1920x1080 or similar</li> <li>Show real data/results (not empty states)</li> <li>Highlight key features with cursor or annotations</li> <li>Keep UI clean (close unnecessary panels)</li> </ul>"},{"location":"configuration/models/","title":"Models","text":""},{"location":"configuration/models/#model-configuration","title":"Model Configuration","text":"<p>TriBridRAG treats model selection as data, not code. The single source of truth is <code>data/models.json</code>, served verbatim by the backend at <code>/api/models</code>. The UI must populate every model dropdown from that endpoint\u2014no hardcoded lists.</p> <p>Related pages</p> <ul> <li>Configuration Overview</li> <li>Tri-Brid Retrieval &amp; Fusion</li> <li>Reranking</li> </ul>"},{"location":"configuration/models/#why-modelsjson-exists","title":"Why <code>models.json</code> exists","text":"<p><code>data/models.json</code> is the canonical registry for:</p> <ul> <li>Which models exist (across providers)</li> <li>What each model can be used for (EMB, GEN, RERANK)</li> <li>Context window / embedding dimensions</li> <li>Pricing metadata for cost estimation and reporting</li> </ul> <p>The backend exposes this registry via a dedicated API module:</p> <ul> <li><code>server/api/models.py</code> loads <code>data/models.json</code></li> <li>The API returns the JSON content (or filtered subsets) to the frontend</li> </ul> <p>No hardcoded model lists</p> <p>The backend docstring is explicit: every dropdown (embedding, generation, reranker) MUST use <code>/api/models</code>. If you add a model anywhere else, it will drift and eventually break.</p>"},{"location":"configuration/models/#modelsjson-structure","title":"<code>models.json</code> structure","text":"<p>At the top level, <code>models.json</code> is a dictionary with metadata plus a <code>models</code> array:</p> <ul> <li><code>currency</code>: Currency used for pricing fields (e.g., <code>\"USD\"</code>)</li> <li><code>last_updated</code>: Human-managed timestamp</li> <li><code>sources</code>: List of pricing sources and dates</li> <li><code>models</code>: List of model definition objects</li> </ul> Click to expand: minimal schema sketch <ul> <li><code>currency</code> (string)</li> <li><code>last_updated</code> (string)</li> <li><code>sources</code> (string[])</li> <li><code>models</code> (object[]), where each object typically includes:<ul> <li><code>provider</code> (string)</li> <li><code>family</code> (string)</li> <li><code>model</code> (string)</li> <li><code>components</code> (string[]) \u2014 includes one or more of <code>EMB</code>, <code>GEN</code>, <code>RERANK</code></li> <li><code>unit</code> (string) \u2014 pricing unit (e.g., <code>1k_tokens</code>, <code>request</code>)</li> <li>Pricing fields (vary by component)</li> <li>Capability fields (e.g., <code>context</code>, <code>dimensions</code>)</li> <li><code>notes</code> (string)</li> </ul> </li> </ul> JSON <pre><code>{\n  \"currency\": \"USD\",\n  \"last_updated\": \"2025-11-29\",\n  \"sources\": [\n    \"https://openai.com/api/pricing/ (2025-11-29)\"\n  ],\n  \"models\": [\n    {\n      \"provider\": \"openai\",\n      \"family\": \"text-embedding-3-large\",\n      \"model\": \"text-embedding-3-large\",\n      \"components\": [\"EMB\"],\n      \"unit\": \"1k_tokens\",\n      \"embed_per_1k\": 0.00013,\n      \"dimensions\": 3072,\n      \"notes\": \"Large embeddings\"\n    }\n  ]\n}\n</code></pre> <p>Naming conventions</p> <ul> <li><code>provider</code> is used for grouping/filtering in the UI and API.</li> <li><code>model</code> is the identifier you pass into runtime configuration (e.g., <code>embedding.embedding_model</code>, <code>generation.gen_model</code>, reranker model fields).</li> <li><code>family</code> is a UI-friendly grouping label; it can match <code>model</code> but doesn\u2019t have to.</li> </ul>"},{"location":"configuration/models/#model-component-types-emb-gen-rerank","title":"Model component types: <code>EMB</code>, <code>GEN</code>, <code>RERANK</code>","text":"<p>Each model advertises one or more capabilities via <code>components</code>.</p>"},{"location":"configuration/models/#emb-embeddings","title":"<code>EMB</code> \u2014 Embeddings","text":"<p>Embedding models are used during indexing and query-time vectorization.</p> <p>Common fields: - <code>embed_per_1k</code>: Cost per 1k tokens (if applicable) - <code>dimensions</code>: Embedding dimensionality (important for pgvector schema compatibility)</p> <p>Example: - OpenAI <code>text-embedding-3-large</code> has <code>dimensions: 3072</code></p> <p>Embedding dimensions must match your index</p> <p>If you change embedding models (and therefore dimensions), you typically must rebuild the pgvector index/table to match the new dimension.</p>"},{"location":"configuration/models/#gen-generation","title":"<code>GEN</code> \u2014 Generation","text":"<p>Generation models are used for chat answers, enrichment prompts, and analysis steps.</p> <p>Common fields: - <code>input_per_1k</code>, <code>output_per_1k</code>: Token pricing - <code>context</code>: Context window size</p>"},{"location":"configuration/models/#rerank-reranking","title":"<code>RERANK</code> \u2014 Reranking","text":"<p>Rerankers refine candidate results after retrieval.</p> <p>Common fields: - Either <code>per_request</code> (common for \u201csearch request\u201d pricing) - Or <code>rerank_per_1k</code> (token-based reranking pricing)</p>"},{"location":"configuration/models/#provider-support","title":"Provider support","text":"<p>TriBridRAG supports multiple providers in <code>models.json</code>. The registry is intentionally provider-agnostic: the UI and backend treat providers as data.</p> <p>Common providers you\u2019ll see:</p> <ul> <li>openai</li> <li>anthropic</li> <li>voyage</li> <li>local (self-hosted / local runtime)</li> <li>huggingface (local model identifiers)</li> <li>ollama (local runtime via Ollama)</li> </ul> <p>Provider vs runtime backend</p> <p>Provider labels in <code>models.json</code> are for selection and display. Actual runtime behavior is controlled by configuration (see <code>EmbeddingConfig</code>, <code>GenerationConfig</code>, <code>RerankingConfig</code> in <code>server/models/tribrid_config_model.py</code>) and the code paths that implement each backend.</p>"},{"location":"configuration/models/#cost-tracking-fields","title":"Cost tracking fields","text":"<p>Pricing fields are optional but strongly recommended. They enable consistent cost estimation across providers.</p> <p>Use the correct fields for the component type:</p> <ul> <li>GEN</li> <li><code>unit</code>: typically <code>\"1k_tokens\"</code></li> <li><code>input_per_1k</code>: numeric</li> <li><code>output_per_1k</code>: numeric</li> <li> <p><code>context</code>: integer (tokens)</p> </li> <li> <p>EMB</p> </li> <li><code>unit</code>: typically <code>\"1k_tokens\"</code></li> <li><code>embed_per_1k</code>: numeric</li> <li> <p><code>dimensions</code>: integer</p> </li> <li> <p>RERANK</p> </li> <li><code>unit</code>: either <code>\"request\"</code> or <code>\"1k_tokens\"</code></li> <li>If request-based: <code>per_request</code></li> <li>If token-based: <code>rerank_per_1k</code></li> </ul> <p>Keep <code>sources</code> and <code>last_updated</code> current</p> <p>When you change pricing, update: - <code>last_updated</code> - <code>sources</code> (include the URL and date you used)</p>"},{"location":"configuration/models/#adding-a-new-model-to-modelsjson","title":"Adding a new model to <code>models.json</code>","text":"<ol> <li>Pick the correct <code>components</code></li> <li> <p><code>[\"EMB\"]</code>, <code>[\"GEN\"]</code>, <code>[\"RERANK\"]</code>, or a combination if the provider/model truly supports multiple roles.</p> </li> <li> <p>Choose a stable <code>model</code> identifier</p> </li> <li> <p>This is what the UI will send and what config will store.</p> </li> <li> <p>Add capability fields</p> </li> <li><code>dimensions</code> for embeddings</li> <li> <p><code>context</code> for generation (and optionally rerankers if meaningful)</p> </li> <li> <p>Add pricing fields</p> </li> <li> <p>Use the correct pricing keys for the component type (see above).</p> </li> <li> <p>Validate via the API</p> </li> <li>Start the server and confirm the model appears in:<ul> <li><code>GET /api/models</code></li> <li><code>GET /api/models/by-type/EMB</code> (or GEN/RERANK)</li> <li><code>GET /api/models/providers/{provider}</code></li> </ul> </li> </ol> JSON <pre><code>{\n  \"provider\": \"voyage\",\n  \"family\": \"voyage-code-3\",\n  \"model\": \"voyage-code-3\",\n  \"components\": [\"EMB\"],\n  \"unit\": \"1k_tokens\",\n  \"embed_per_1k\": 0.00018,\n  \"dimensions\": 1024,\n  \"notes\": \"Voyage Code embeddings\"\n}\n</code></pre> <p>Do not add new model lists in code</p> <p>If you need a new dropdown option, add it to <code>models.json</code>. The UI must discover it through <code>/api/models</code>.</p>"},{"location":"configuration/models/#api-endpoints-apimodels","title":"API endpoints: <code>/api/models</code>","text":"<p>The backend serves <code>models.json</code> through <code>server/api/models.py</code>.</p>"},{"location":"configuration/models/#endpoints","title":"Endpoints","text":"<ul> <li><code>GET /api/models</code></li> <li> <p>Returns all model definitions (the <code>models</code> array from <code>models.json</code>)</p> </li> <li> <p><code>GET /api/models/by-type/{component_type}</code></p> </li> <li>Filters by component type: <code>EMB</code>, <code>GEN</code>, <code>RERANK</code> (case-insensitive)</li> <li> <p>Returns only models whose <code>components</code> include that type</p> </li> <li> <p><code>GET /api/models/providers</code></p> </li> <li> <p>Returns sorted unique provider names</p> </li> <li> <p><code>GET /api/models/providers/{provider}</code></p> </li> <li>Returns all models for a given provider</li> </ul> Python <pre><code>from fastapi import APIRouter, HTTPException\nfrom pathlib import Path\nimport json\n\nrouter = APIRouter(prefix=\"/api/models\", tags=[\"models\"])\nMODELS_PATH = Path(__file__).parent.parent.parent / \"data\" / \"models.json\"\n\ndef _load_models() -&gt; list[dict]:\n    if not MODELS_PATH.exists():\n        raise HTTPException(status_code=500, detail=f\"models.json not found at {MODELS_PATH}\")\n    data = json.loads(MODELS_PATH.read_text())\n    if isinstance(data, dict) and \"models\" in data:\n        return data[\"models\"]\n    return data\n\n@router.get(\"\")\nasync def get_all_models() -&gt; list[dict]:\n    return _load_models()\n\n@router.get(\"/by-type/{component_type}\")\nasync def get_models_by_type(component_type: str) -&gt; list[dict]:\n    models = _load_models()\n    comp = component_type.upper()\n    if comp not in (\"EMB\", \"GEN\", \"RERANK\"):\n        raise HTTPException(status_code=400, detail=\"Invalid component_type\")\n    return [m for m in models if comp in m.get(\"components\", [])]\n</code></pre> <p>Breaking change risk</p> <p>If you rename or remove a model entry that is already referenced by saved configuration, the UI may still load but runtime selection can fail. Prefer deprecating via <code>notes</code> first, then removing later.</p>"},{"location":"configuration/models/#how-configuration-references-models-pydantic-is-the-law","title":"How configuration references models (Pydantic is the law)","text":"<p>Runtime configuration lives in <code>server/models/tribrid_config_model.py</code> under <code>TriBridConfigRoot</code>. Model selection fields include:</p> <ul> <li><code>embedding.embedding_type</code>, <code>embedding.embedding_model</code>, <code>embedding.embedding_dim</code></li> <li><code>generation.gen_model</code>, plus backend-specific overrides (Ollama, HTTP, MCP)</li> <li><code>reranking.reranker_mode</code> and reranker model/provider fields</li> </ul> <p>Dynamic validation</p> <p><code>EmbeddingConfig.embedding_type</code> is described as \u201cvalidated against models.json at runtime\u201d. The intent is: models.json defines what\u2019s selectable, and config chooses among those options.</p>"},{"location":"configuration/models/#frontend-usage-usemodels-hook","title":"Frontend usage: <code>useModels</code> hook","text":"<p>The frontend should treat <code>/api/models</code> as the only source of model options. The typical flow is:</p> <ol> <li>Call <code>GET /api/models</code> once (or cache it)</li> <li>Derive:</li> <li>Provider lists</li> <li>Component-specific lists (EMB/GEN/RERANK)</li> <li>Display labels (often <code>family</code> + <code>model</code>)</li> <li>Populate dropdowns and persist the selected <code>model</code> string into configuration</li> </ol> Click to expand: expected hook responsibilities <ul> <li>Fetch <code>/api/models</code></li> <li>Provide helpers like:<ul> <li><code>getByType(\"EMB\" | \"GEN\" | \"RERANK\")</code></li> <li><code>getProviders()</code></li> <li><code>getByProvider(provider)</code></li> </ul> </li> <li>Keep UI logic free of hardcoded provider/model assumptions</li> </ul> TypeScript <pre><code>type ModelComponent = \"EMB\" | \"GEN\" | \"RERANK\";\n\nexport type ModelDef = {\n  provider: string;\n  family: string;\n  model: string;\n  components: ModelComponent[];\n  unit?: string;\n  input_per_1k?: number;\n  output_per_1k?: number;\n  embed_per_1k?: number;\n  rerank_per_1k?: number;\n  per_request?: number;\n  context?: number;\n  dimensions?: number;\n  notes?: string;\n};\n\nexport async function fetchModels(): Promise&lt;ModelDef[]&gt; {\n  const res = await fetch(\"/api/models\");\n  if (!res.ok) throw new Error(\"Failed to load models\");\n  return (await res.json()) as ModelDef[];\n}\n\nexport function filterByType(models: ModelDef[], t: ModelComponent): ModelDef[] {\n  return models.filter(m =&gt; (m.components ?? []).includes(t));\n}\n</code></pre> <p>TypeScript types are generated</p> <p>In TriBridRAG, TypeScript types are generated from Pydantic (never hand-written). The snippet above illustrates shape/usage only; in the actual codebase, prefer the generated types and keep the hook thin.</p>"},{"location":"configuration/models/#architecture-where-models-fit","title":"Architecture: where models fit","text":"<pre><code>flowchart LR\n  A[data/models.json&lt;br/&gt;Model registry] --&gt; B[FastAPI&lt;br/&gt;/api/models]\n  B --&gt; C[Frontend useModels hook&lt;br/&gt;dropdowns + selection]\n  C --&gt; D[tribrid_config.json&lt;br/&gt;selected model IDs]\n  D --&gt; E[TriBridConfigRoot (Pydantic)&lt;br/&gt;server/models/tribrid_config_model.py]\n  E --&gt; F[Runtime components&lt;br/&gt;Embedding / Generation / Reranking]</code></pre> <p>Operational checklist</p> <ul> <li>Add model to <code>data/models.json</code></li> <li>Confirm it appears in <code>GET /api/models/by-type/...</code></li> <li>Select it in the UI (via <code>useModels</code>)</li> <li>Ensure config references the <code>model</code> string exactly</li> <li>Rebuild indexes if embedding dimensions changed</li> </ul>"},{"location":"configuration/settings/","title":"Settings","text":""},{"location":"features/neo4j-graph/","title":"Graph (Neo4j)","text":""},{"location":"features/neo4j-graph/#graph-search-neo4j","title":"Graph Search (Neo4j)","text":"<p>TriBridRAG\u2019s graph search leg uses Neo4j to model entities (symbols) and relationships (edges) extracted from code. This enables \u201cGraph RAG\u201d: retrieval driven by structure (who calls what, what imports what, what contains what), not just lexical overlap (sparse) or semantic similarity (dense).</p> <p>This page documents the Neo4j integration points, configuration (Pydantic is the law), and the Cypher patterns we use/expect to use.</p> <p>Tri-brid architecture context</p> <p>Graph search is the third leg in TriBridRAG\u2019s tri-brid fusion:</p> <ul> <li>Dense: pgvector (PostgreSQL)</li> <li>Sparse: PostgreSQL Full-Text Search / BM25</li> <li>Graph: Neo4j (entities + relationships + communities)</li> </ul> <p>Results from all three legs are fused via RRF or weighted scoring. See Fusion (RRF vs Weighted).</p>"},{"location":"features/neo4j-graph/#why-graphs-for-code-search-graph-rag-approach","title":"Why graphs for code search (Graph RAG approach)","text":"<p>Codebases are inherently relational:</p> <ul> <li>A function is relevant because it is called by the entrypoint you searched for.</li> <li>A class is relevant because it is inherited by the type you\u2019re debugging.</li> <li>A module is relevant because it is imported by the file you\u2019re editing.</li> <li>A chunk is relevant because it contains the symbol that matches your query.</li> </ul> <p>Dense and sparse retrieval are great at finding matching text, but they can miss \u201cone-hop-away\u201d context that is crucial for correct answers. Graph RAG fills that gap by:</p> <ul> <li>Expanding around matched entities (neighbors within N hops)</li> <li>Surfacing structural context (call chains, import graphs, containment)</li> <li>Providing community-level grouping (clusters of related symbols)</li> </ul> <p>When graph search shines</p> <p>Graph search is most valuable for:</p> <ul> <li>\u201cWhere is this function called?\u201d</li> <li>\u201cWhat depends on this module?\u201d</li> <li>\u201cWhat is the lifecycle / flow through these components?\u201d</li> <li>\u201cShow me related code around symbol X\u201d</li> </ul>"},{"location":"features/neo4j-graph/#architecture-overview","title":"Architecture overview","text":"<pre><code>flowchart LR\n  Q[User Query] --&gt; GR[GraphRetriever]\n  GR --&gt; NC[Neo4jClient]\n  NC --&gt; N4J[(Neo4j)]\n\n  subgraph GraphIndexing[Indexing / Graph Build]\n    GB[GraphBuilder] --&gt;|extract_entities| E[Entity nodes]\n    GB --&gt;|extract_relationships| R[Relationship edges]\n    GB --&gt;|detect_communities| C[Community nodes/labels]\n  end\n\n  GraphIndexing --&gt; N4J\n\n  N4J --&gt;|graph_search| M[ChunkMatch[]]\n  M --&gt; F[Fusion (RRF / Weighted)]\n  F --&gt; Final[Final ranked chunks]</code></pre> <p>Related pages:</p> <ul> <li>Tri-brid Retrieval Overview</li> <li>Fusion (RRF vs Weighted)</li> <li>Configuration Model (Pydantic)</li> </ul>"},{"location":"features/neo4j-graph/#code-entry-points","title":"Code entry points","text":""},{"location":"features/neo4j-graph/#neo4j-client","title":"Neo4j client","text":"<p><code>server/db/neo4j.py</code> defines the async Neo4j client wrapper:</p> <ul> <li>Connection lifecycle: <code>connect()</code>, <code>disconnect()</code></li> <li>CRUD operations for entities/relationships/communities</li> <li>Search: <code>graph_search(...) -&gt; list[ChunkMatch]</code></li> <li>Debug/ops: <code>execute_cypher(...)</code></li> <li>Stats: <code>get_graph_stats(...)</code></li> </ul> <pre><code>from neo4j import AsyncGraphDatabase\n\nfrom server.models.graph import Community, Entity, GraphStats, Relationship\nfrom server.models.retrieval import ChunkMatch\n\n\nclass Neo4jClient:\n    def __init__(self, uri: str, user: str, password: str):\n        self.uri = uri\n        self.user = user\n        self.password = password\n        self._driver = None\n\n    async def connect(self) -&gt; None:\n        self._driver = AsyncGraphDatabase.driver(self.uri, auth=(self.user, self.password))\n\n    async def disconnect(self) -&gt; None:\n        if self._driver:\n            await self._driver.close()\n            self._driver = None\n\n    async def graph_search(self, repo_id: str, query: str, max_hops: int, top_k: int) -&gt; list[ChunkMatch]:\n        raise NotImplementedError\n</code></pre> <p>Implementation status</p> <p>Many methods are currently <code>NotImplementedError</code>. This page documents the intended behavior and the configuration contract so the implementation can be completed consistently.</p>"},{"location":"features/neo4j-graph/#retriever-integration","title":"Retriever integration","text":"<p><code>server/retrieval/graph.py</code> wires Neo4j into the retrieval layer:</p> <pre><code>from server.db.neo4j import Neo4jClient\nfrom server.indexing.embedder import Embedder\nfrom server.models.config import GraphSearchConfig\nfrom server.models.retrieval import ChunkMatch\n\n\nclass GraphRetriever:\n    def __init__(self, neo4j: Neo4jClient, embedder: Embedder):\n        self.neo4j = neo4j\n        self.embedder = embedder\n\n    async def search(self, repo_id: str, query: str, config: GraphSearchConfig) -&gt; list[ChunkMatch]:\n        return await self.neo4j.graph_search(repo_id, query, config.max_hops, config.top_k)\n</code></pre>"},{"location":"features/neo4j-graph/#entities-what-we-extract-and-store","title":"Entities: what we extract and store","text":"<p>Entities are the \u201cthings\u201d in the code graph: symbols and structural units. The canonical list is configured in Pydantic via <code>GraphStorageConfig.entity_types</code>.</p> <p>From <code>server/models/tribrid_config_model.py</code>:</p> <pre><code>class GraphStorageConfig(BaseModel):\n    \"\"\"Configuration for Neo4j graph storage and traversal.\"\"\"\n\n    entity_types: List[str] = Field(\n        default=[\"function\", \"class\", \"module\", \"variable\", \"import\"],\n        description=\"Entity types to extract and store in graph\"\n    )\n</code></pre> <p>Entity types (default):</p> <ul> <li><code>function</code></li> <li><code>class</code></li> <li><code>module</code></li> <li><code>variable</code></li> <li><code>import</code></li> </ul> <p>Entity granularity vs chunks</p> <p>Graph nodes represent symbols/units, while retrieval ultimately returns chunks (<code>ChunkMatch</code>). The graph layer typically maps:</p> <ul> <li>query \u2192 entity candidates (by name/type/metadata)</li> <li>entity candidates \u2192 related entities (traversal)</li> <li>entities \u2192 chunk IDs (where the entity is defined/used)</li> </ul>"},{"location":"features/neo4j-graph/#suggested-neo4j-node-model-recommended","title":"Suggested Neo4j node model (recommended)","text":"<p>While the exact schema is not yet implemented in <code>Neo4jClient</code>, the following conventions keep queries fast and predictable:</p> <ul> <li><code>(:Repo {repo_id})</code> (optional anchor)</li> <li><code>(:Entity {id, repo_id, name, type, file_path, chunk_id, ...})</code></li> <li><code>(:Chunk {id, repo_id, file_path, start_line, end_line, ...})</code> (optional but useful)</li> <li><code>(:Community {repo_id, level, community_id, algorithm, ...})</code> (optional)</li> </ul>"},{"location":"features/neo4j-graph/#relationships-what-edges-we-store","title":"Relationships: what edges we store","text":"<p>Relationships represent how entities connect. The canonical list is configured in Pydantic via <code>GraphStorageConfig.relationship_types</code>.</p> <pre><code>class GraphStorageConfig(BaseModel):\n    relationship_types: List[str] = Field(\n        default=[\"calls\", \"imports\", \"inherits\", \"contains\", \"references\"],\n        description=\"Relationship types to extract\"\n    )\n</code></pre> <p>Relationship types (default):</p> <ul> <li><code>calls</code> \u2014 function/method invocation edges</li> <li><code>imports</code> \u2014 module import edges</li> <li><code>inherits</code> \u2014 class inheritance edges</li> <li><code>contains</code> \u2014 containment edges (module\u2192class, class\u2192method, file\u2192symbol)</li> <li><code>references</code> \u2014 general symbol reference edges (fallback when call/import/inherit is unknown)</li> </ul> <p>Keep relationship types small and stable</p> <p>A small, stable edge taxonomy makes traversal and scoring easier. If you add new relationship types, update:</p> <ul> <li><code>GraphStorageConfig.relationship_types</code></li> <li>graph builder extraction logic</li> <li>Cypher traversal patterns (filtering by relationship type)</li> </ul>"},{"location":"features/neo4j-graph/#community-detection-louvain-label-propagation","title":"Community detection (Louvain / Label Propagation)","text":"<p>Communities are clusters of densely connected entities. They\u2019re useful for:</p> <ul> <li>grouping related subsystems</li> <li>boosting results that belong to the same \u201ctopic cluster\u201d</li> <li>summarizing neighborhoods for context expansion</li> </ul> <p>Configuration lives in <code>GraphStorageConfig</code>:</p> <pre><code>class GraphStorageConfig(BaseModel):\n    include_communities: bool = Field(\n        default=True,\n        description=\"Include community detection in graph analysis\"\n    )\n\n    community_algorithm: Literal[\"louvain\", \"label_propagation\"] = Field(\n        default=\"louvain\",\n        description=\"Community detection algorithm\"\n    )\n</code></pre>"},{"location":"features/neo4j-graph/#algorithms","title":"Algorithms","text":"<ul> <li>Louvain</li> <li>Optimizes modularity; typically produces higher-quality clusters.</li> <li>Often more stable for larger graphs.</li> <li>Label propagation</li> <li>Fast and simple; good for quick clustering.</li> <li>Can be less stable across runs depending on implementation details.</li> </ul> <p>Neo4j GDS dependency</p> <p>Community detection is typically implemented via Neo4j Graph Data Science (GDS) procedures (e.g., <code>gds.louvain.*</code>, <code>gds.labelPropagation.*</code>). If GDS is not available, set <code>include_communities=false</code> to avoid runtime failures.</p>"},{"location":"features/neo4j-graph/#configuration-graphstorageconfig-pydantic-is-the-law","title":"Configuration: <code>GraphStorageConfig</code> (Pydantic is the law)","text":"<p>All graph storage and traversal tuning flows from <code>server/models/tribrid_config_model.py</code>. Do not introduce ad-hoc environment variables or hand-maintained TypeScript types.</p>"},{"location":"features/neo4j-graph/#parameters","title":"Parameters","text":"<code>neo4j_uri</code> Neo4j connection URI (e.g., <code>bolt://localhost:7687</code>). <code>neo4j_user</code> Neo4j username. <code>neo4j_password</code> Neo4j password. Prefer injecting via environment variable and loading into <code>tribrid_config.json</code>. <code>neo4j_database</code> Neo4j database name (default: <code>neo4j</code>). <code>max_hops</code> Maximum traversal depth for graph search (default: <code>2</code>, allowed: <code>1..5</code>). <code>include_communities</code> Whether to run community detection and store community metadata. <code>community_algorithm</code> Community detection algorithm: <code>louvain</code> or <code>label_propagation</code>. <code>entity_types</code> Entity types to extract/store. <code>relationship_types</code> Relationship types to extract/store. <code>graph_search_top_k</code> Number of results returned from graph traversal (default: <code>30</code>). <p>Single source of truth</p> <p><code>GraphStorageConfig</code> is the contract. If you need a new graph feature, add it here first, then regenerate TypeScript types (never hand-write them). See Configuration Model (Pydantic).</p> Config snippet example (tribrid_config.json) <pre><code>{\n  \"graph_storage\": {\n    \"neo4j_uri\": \"bolt://localhost:7687\",\n    \"neo4j_user\": \"neo4j\",\n    \"neo4j_password\": \"${NEO4J_PASSWORD}\",\n    \"neo4j_database\": \"neo4j\",\n    \"max_hops\": 2,\n    \"include_communities\": true,\n    \"community_algorithm\": \"louvain\",\n    \"entity_types\": [\"function\", \"class\", \"module\", \"variable\", \"import\"],\n    \"relationship_types\": [\"calls\", \"imports\", \"inherits\", \"contains\", \"references\"],\n    \"graph_search_top_k\": 30\n  }\n}\n</code></pre>"},{"location":"features/neo4j-graph/#traversal-depth-configuring-max_hops","title":"Traversal depth: configuring <code>max_hops</code>","text":"<p>Traversal depth controls how far we expand from initial matches.</p> <ul> <li><code>max_hops=1</code>: direct neighbors only (high precision, lower recall)</li> <li><code>max_hops=2</code>: neighbors-of-neighbors (balanced; default)</li> <li><code>max_hops&gt;=3</code>: can explode result set; use carefully</li> </ul> <p>In retrieval, <code>GraphRetriever.search()</code> passes <code>config.max_hops</code> directly to <code>Neo4jClient.graph_search(...)</code>.</p> <pre><code>async def search(self, repo_id: str, query: str, config: GraphSearchConfig) -&gt; list[ChunkMatch]:\n    return await self.neo4j.graph_search(repo_id, query, config.max_hops, config.top_k)\n</code></pre> <p>Graph blow-up risk</p> <p>Increasing <code>max_hops</code> increases the branching factor dramatically on dense graphs (especially with <code>references</code>). Keep <code>max_hops</code> small and prefer filtering by relationship type and/or node type.</p>"},{"location":"features/neo4j-graph/#cypher-query-patterns-expected","title":"Cypher query patterns (expected)","text":"<p><code>Neo4jClient.execute_cypher(query, params)</code> is the escape hatch for debugging and operational queries. The production path is <code>graph_search(...)</code>, which should be implemented using a small set of predictable Cypher patterns.</p> <p>Below are the recommended patterns TriBridRAG should use.</p>"},{"location":"features/neo4j-graph/#1-entity-lookup-by-name-seed-selection","title":"1) Entity lookup by name (seed selection)","text":"<p>Goal: find candidate entities matching the query string (exact, prefix, or fuzzy).</p> <pre><code>MATCH (e:Entity {repo_id: $repo_id})\nWHERE toLower(e.name) CONTAINS toLower($q)\n  AND ($entity_types IS NULL OR e.type IN $entity_types)\nRETURN e\nORDER BY e.name\nLIMIT $seed_k\n</code></pre> <p>Notes:</p> <ul> <li>In practice, you\u2019ll want indexes/constraints on <code>(repo_id, name)</code> and possibly full-text indexes for <code>Entity.name</code>.</li> <li>Seed selection should be conservative; traversal expands recall.</li> </ul>"},{"location":"features/neo4j-graph/#2-bounded-traversal-from-seeds-n-hops","title":"2) Bounded traversal from seeds (N hops)","text":"<p>Goal: expand from seed entities across allowed relationship types.</p> <pre><code>MATCH (seed:Entity {repo_id: $repo_id})\nWHERE seed.id IN $seed_ids\n\nMATCH p = (seed)-[r*1..$max_hops]-(nbr:Entity {repo_id: $repo_id})\nWHERE ALL(rel IN r WHERE type(rel) IN $rel_types)\nRETURN nbr, length(p) AS hops\nORDER BY hops ASC\nLIMIT $top_k\n</code></pre> <p>Notes:</p> <ul> <li>Use undirected <code>-(...)-</code> if you want \u201crelatedness\u201d regardless of direction.</li> <li>Use directed traversal if you want semantics (e.g., <code>calls</code> direction).</li> </ul>"},{"location":"features/neo4j-graph/#3-map-entities-to-chunks-return-chunkmatch","title":"3) Map entities to chunks (return <code>ChunkMatch</code>)","text":"<p>Goal: convert graph hits into retrievable chunk IDs.</p> <p>Common approaches:</p> <ul> <li>store <code>chunk_id</code> directly on <code>Entity</code></li> <li>or connect <code>(:Entity)-[:DEFINED_IN]-&gt;(:Chunk)</code> / <code>(:Entity)-[:MENTIONED_IN]-&gt;(:Chunk)</code></li> </ul> <p>Example with <code>chunk_id</code> property:</p> <pre><code>MATCH (e:Entity {repo_id: $repo_id})\nWHERE e.id IN $entity_ids AND e.chunk_id IS NOT NULL\nRETURN e.chunk_id AS chunk_id, count(*) AS support\nORDER BY support DESC\nLIMIT $top_k\n</code></pre> <p>Example with explicit chunk nodes:</p> <pre><code>MATCH (e:Entity {repo_id: $repo_id})-[:DEFINED_IN|MENTIONED_IN]-&gt;(c:Chunk {repo_id: $repo_id})\nWHERE e.id IN $entity_ids\nRETURN c.id AS chunk_id, count(*) AS support\nORDER BY support DESC\nLIMIT $top_k\n</code></pre>"},{"location":"features/neo4j-graph/#4-community-aware-boosting-optional","title":"4) Community-aware boosting (optional)","text":"<p>If communities are enabled, we can boost entities/chunks that share a community with the seeds.</p> <pre><code>MATCH (seed:Entity {repo_id: $repo_id})-[:IN_COMMUNITY]-&gt;(comm:Community {repo_id: $repo_id})\nWHERE seed.id IN $seed_ids\n\nMATCH (e:Entity {repo_id: $repo_id})-[:IN_COMMUNITY]-&gt;(comm)\nRETURN e, count(comm) AS shared_communities\nORDER BY shared_communities DESC\nLIMIT $top_k\n</code></pre> <p>Scoring strategy</p> <p>Graph search typically yields a support score (e.g., number of paths, shared communities, inverse hop distance). That score becomes the graph leg\u2019s contribution during tri-brid fusion. See Fusion (RRF vs Weighted).</p>"},{"location":"features/neo4j-graph/#implementation-checklist-what-neo4jclient-should-do","title":"Implementation checklist (what <code>Neo4jClient</code> should do)","text":"<p>Even though methods are currently stubbed, the intended responsibilities are clear:</p> <ol> <li>Upsert entities (<code>upsert_entity</code>, <code>upsert_entities</code>)</li> <li>Merge on stable IDs (e.g., <code>repo_id + entity_id</code>)</li> <li> <p>Set/update name/type/file_path/chunk_id metadata</p> </li> <li> <p>Upsert relationships (<code>upsert_relationship</code>, <code>upsert_relationships</code>)</p> </li> <li>Merge edges between entity IDs</li> <li> <p>Store relationship type and optional metadata (call site, line, etc.)</p> </li> <li> <p>Community detection (<code>detect_communities</code>, <code>get_communities</code>)</p> </li> <li>Run configured algorithm when <code>include_communities=true</code></li> <li> <p>Persist community membership for traversal/boosting</p> </li> <li> <p>Graph search (<code>graph_search</code>)</p> </li> <li>Seed entities from query</li> <li>Traverse up to <code>max_hops</code></li> <li>Map entities \u2192 chunk IDs</li> <li> <p>Return <code>ChunkMatch[]</code> (with graph-derived score + provenance)</p> </li> <li> <p>Stats (<code>get_graph_stats</code>)</p> </li> <li>Count entities, relationships, communities per repo</li> </ol> <p>Keep graph search deterministic</p> <p>For stable evaluation and debugging, prefer deterministic ordering:</p> <ul> <li>order by hop distance, then support count, then stable IDs</li> <li>avoid nondeterministic procedures unless explicitly controlled</li> </ul>"},{"location":"features/neo4j-graph/#troubleshooting","title":"Troubleshooting","text":"<p>Validate config first</p> <p>If Neo4j connectivity fails, check <code>graph_storage.neo4j_uri</code>, <code>neo4j_user</code>, and <code>neo4j_password</code> in <code>tribrid_config.json</code>. Because Pydantic validates ranges/types, most failures here are runtime connectivity rather than schema issues.</p> <p>Database selection</p> <p><code>GraphStorageConfig.neo4j_database</code> exists, but <code>Neo4jClient.connect()</code> currently does not select a database explicitly. When implementing queries, ensure sessions target the configured database.</p>"},{"location":"features/neo4j-graph/#next-steps-related-work","title":"Next steps / related work","text":"<ul> <li>Implement <code>GraphBuilder.build_graph()</code> to:</li> <li>extract entities from chunks</li> <li>infer relationships</li> <li>upsert into Neo4j</li> <li> <p>optionally run community detection</p> </li> <li> <p>Implement <code>GraphRetriever.expand_context()</code> to:</p> </li> <li>take initial chunk IDs</li> <li>find entities inside those chunks</li> <li>expand neighbors within <code>max_hops</code></li> <li>return additional <code>ChunkMatch</code> context candidates</li> </ul> <p>See also:</p> <ul> <li>Indexing Pipeline</li> <li>Retrieval Pipeline</li> <li>Fusion (RRF vs Weighted)</li> <li>Configuration Model (Pydantic)</li> </ul>"},{"location":"features/pgvector/","title":"Vector (pgvector)","text":""},{"location":"features/pgvector/#vector-search-pgvector","title":"Vector Search (pgvector)","text":"<p>TriBridRAG\u2019s dense retrieval leg is implemented on PostgreSQL + pgvector, using the same Postgres instance that also powers metadata and (optionally) sparse retrieval. This keeps the system operationally simple while still delivering strong semantic search performance.</p> <p>Tri-brid context</p> <p>Vector search is one of three retrieval legs in TriBridRAG (vector + sparse + graph). Results are fused downstream via RRF or weighted scoring. See also: Fusion (RRF / Weighted), Sparse Search (FTS/BM25), and Graph Search (Neo4j).</p>"},{"location":"features/pgvector/#where-vector-search-lives-in-the-code","title":"Where vector search lives in the code","text":"<p>At runtime, vector retrieval is a small, explicit pipeline:</p> <ul> <li><code>VectorRetriever.search()</code> embeds the query and calls Postgres.</li> <li><code>PostgresClient.vector_search()</code> performs the pgvector similarity query.</li> </ul> <pre><code>from server.db.postgres import PostgresClient\nfrom server.indexing.embedder import Embedder\nfrom server.models.config import VectorSearchConfig\nfrom server.models.retrieval import ChunkMatch\n\n\nclass VectorRetriever:\n    def __init__(self, postgres: PostgresClient, embedder: Embedder):\n        self.postgres = postgres\n        self.embedder = embedder\n\n    async def search(self, repo_id: str, query: str, config: VectorSearchConfig) -&gt; list[ChunkMatch]:\n        embedding = await self.embedder.embed(query)\n        results = await self.postgres.vector_search(repo_id, embedding, config.top_k)\n        if config.similarity_threshold &gt; 0:\n            results = [r for r in results if r.score &gt;= config.similarity_threshold]\n        return results\n</code></pre> <p>PostgresClient is the contract</p> <p><code>server/db/postgres.py</code> defines the interface (<code>upsert_embeddings</code>, <code>vector_search</code>, <code>delete_embeddings</code>). The concrete SQL implementation must honor this contract and return <code>ChunkMatch</code> objects with a consistent <code>score</code> meaning (typically cosine similarity or a normalized distance transform).</p>"},{"location":"features/pgvector/#why-pgvector-vs-a-dedicated-vector-database","title":"Why pgvector (vs a dedicated vector database)","text":"<p>We use pgvector because it aligns with TriBridRAG\u2019s architecture goals:</p> <ol> <li>Operational simplicity</li> <li>One Postgres instance can host:<ul> <li>chunk metadata</li> <li>embeddings (pgvector)</li> <li>optional sparse search structures (FTS/BM25-style ranking)</li> </ul> </li> <li> <p>Fewer moving parts means fewer failure modes and simpler deployments.</p> </li> <li> <p>Transactional consistency</p> </li> <li>Indexing pipelines often need \u201cupsert chunk + upsert embedding + update stats\u201d as a coherent unit.</li> <li> <p>Postgres gives us familiar transactional semantics and tooling.</p> </li> <li> <p>Good-enough performance with the right index</p> </li> <li>pgvector supports approximate nearest neighbor (ANN) indexes (notably HNSW and IVFFlat) that cover most RAG workloads well.</li> </ol> <p>When pgvector is the right choice</p> <p>pgvector is especially strong when you want: - a single datastore for retrieval + metadata - predictable operations (backups, migrations, monitoring) - moderate-to-large corpora with ANN indexing</p>"},{"location":"features/pgvector/#index-types-hnsw-vs-ivfflat","title":"Index types: HNSW vs IVFFlat","text":"<p>pgvector supports multiple index strategies. The two you should care about for RAG are HNSW and IVFFlat.</p>"},{"location":"features/pgvector/#hnsw-hierarchical-navigable-small-world","title":"HNSW (Hierarchical Navigable Small World)","text":"<p>Best for: low-latency queries, high recall, frequently queried corpora.</p> <ul> <li>Pros:</li> <li>Typically excellent recall/latency tradeoff</li> <li>No \u201ctraining\u201d step like IVF clustering</li> <li>Works well as your corpus grows incrementally</li> <li>Cons:</li> <li>Higher memory and index build cost than IVFFlat</li> <li>More write overhead during indexing</li> </ul> <p>Typical SQL (conceptual): - Distance ops: cosine (<code>vector_cosine_ops</code>) or L2 (<code>vector_l2_ops</code>) - Index: <code>USING hnsw (embedding vector_cosine_ops)</code></p>"},{"location":"features/pgvector/#ivfflat-inverted-file-flat","title":"IVFFlat (Inverted File Flat)","text":"<p>Best for: large corpora where you can tolerate slightly lower recall or you want cheaper indexing.</p> <ul> <li>Pros:</li> <li>Often smaller/faster to build than HNSW</li> <li>Good performance when tuned (<code>lists</code>, <code>probes</code>)</li> <li>Cons:</li> <li>Requires choosing clustering parameters (<code>lists</code>)</li> <li>Query-time recall depends heavily on <code>probes</code></li> <li>Works best when the dataset is relatively stable (or you periodically rebuild)</li> </ul> <p>Typical SQL (conceptual): - Index: <code>USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100)</code> - Query tuning: <code>SET ivfflat.probes = 10</code></p> <p>Pick one per workload</p> <p>HNSW and IVFFlat are not interchangeable defaults. If you don\u2019t know which to pick: - start with HNSW for developer-facing interactive search - consider IVFFlat for very large corpora or constrained environments</p> Click to expand: recommended starting points <ul> <li>HNSW:</li> <li>Use cosine distance for embeddings that are approximately normalized.</li> <li>Start with moderate <code>m</code> and <code>ef_construction</code>; tune later.</li> <li>IVFFlat:</li> <li>Start with <code>lists \u2248 sqrt(N)</code> (rule of thumb).</li> <li>Start with <code>probes</code> around <code>sqrt(lists)</code> and tune for recall vs latency.</li> </ul>"},{"location":"features/pgvector/#configuration-pydantic-is-the-law","title":"Configuration (Pydantic is the law)","text":"<p>All tunable configuration flows from <code>server/models/tribrid_config_model.py</code>. Do not hand-roll config parsing; do not hand-write TypeScript types\u2014those are generated from Pydantic.</p>"},{"location":"features/pgvector/#indexingconfig-pgvector-related-fields","title":"IndexingConfig (pgvector-related fields)","text":"<p>These fields live under <code>TriBridConfigRoot.indexing</code>.</p> <pre><code>class IndexingConfig(BaseModel):\n    \"\"\"Indexing and vector storage configuration.\"\"\"\n\n    postgres_url: str = Field(\n        default=\"http://127.0.0.1:6333\",\n        description=\"PostgreSQL pgvector URL\"\n    )\n    table_name: str = Field(\n        default=\"code_chunks_{repo}\",\n        description=\"pgvector table name template\"\n    )\n    collection_suffix: str = Field(\n        default=\"default\",\n        description=\"Collection suffix for multi-index scenarios\"\n    )\n    indexing_batch_size: int = Field(default=100, ge=10, le=1000)\n    indexing_workers: int = Field(default=4, ge=1, le=16)\n    skip_dense: int = Field(default=0, ge=0, le=1, description=\"Skip dense vector indexing\")\n</code></pre> <p>Parameter reference (definition list):</p> postgres_url Connection string/URL for Postgres hosting pgvector. Note: despite the name, this must be a Postgres connection target in your deployment. Ensure it matches what <code>asyncpg.create_pool()</code> expects in <code>PostgresClient.connect()</code>. table_name Template for the per-repo chunk table. The <code>{repo}</code> token is substituted with the repository identifier. collection_suffix Suffix for multi-index scenarios (e.g., multiple embedding models or environments). Use this to avoid collisions. indexing_batch_size Batch size for embedding upserts. Larger batches improve throughput but increase memory pressure. indexing_workers Parallelism for indexing. Tune based on CPU, embedding provider throughput, and Postgres capacity. skip_dense Set to <code>1</code> to disable dense indexing entirely (vector leg off). Useful for debugging or sparse-only runs. <p>Config correctness is enforced</p> <p>Pydantic validates ranges and types at load time. If you need a new pgvector tuning knob (e.g., HNSW <code>m</code>), add it to <code>IndexingConfig</code> (or a dedicated <code>VectorIndexConfig</code>) and regenerate TypeScript types from the Pydantic schema.</p>"},{"location":"features/pgvector/#embedding-dimensions-supported-models","title":"Embedding dimensions &amp; supported models","text":"<p>Vector search quality and correctness depend on embedding configuration. Dimensions must match the stored vectors.</p>"},{"location":"features/pgvector/#embeddingconfig-essentials","title":"EmbeddingConfig essentials","text":"<pre><code>class EmbeddingConfig(BaseModel):\n    embedding_type: str = Field(default=\"openai\")\n    embedding_model: str = Field(default=\"text-embedding-3-large\")\n    embedding_dim: int = Field(default=3072, ge=128, le=4096)\n\n    voyage_model: str = Field(default=\"voyage-code-3\")\n    embedding_model_local: str = Field(default=\"all-MiniLM-L6-v2\")\n\n    @field_validator('embedding_dim')\n    @classmethod\n    def validate_dim_matches_model(cls, v):\n        common_dims = [128, 256, 384, 512, 768, 1024, 1536, 2048, 3072, 4096]\n        if v not in common_dims:\n            raise ValueError(f'Uncommon embedding dimension: {v}. Expected one of {common_dims}')\n        return v\n</code></pre> <p>What this means in practice:</p> <ul> <li><code>embedding_dim</code> must match the actual output dimension of your embedding model.</li> <li>TriBridRAG validates that the dimension is one of a known set of common sizes.</li> <li>If you change <code>embedding_model</code>, you must ensure:</li> <li>the index schema uses the same dimension</li> <li>existing stored vectors are rebuilt (or stored in a separate table/collection suffix)</li> </ul> <p>Dimension mismatch will break retrieval</p> <p>If the table column is <code>vector(3072)</code> but you embed to 1536 (or vice versa), inserts and/or queries will fail. Treat embedding dimension changes as a schema migration + full reindex.</p>"},{"location":"features/pgvector/#query-examples-sql-python","title":"Query examples (SQL + Python)","text":""},{"location":"features/pgvector/#1-sql-cosine-similarity-top-k-conceptual","title":"1) SQL: cosine similarity top-k (conceptual)","text":"<p>Assuming a table like:</p> <ul> <li><code>chunk_id text primary key</code></li> <li><code>repo_id text</code></li> <li><code>content text</code></li> <li><code>embedding vector(3072)</code></li> </ul> <p>You can query:</p> <pre><code>SELECT\n  chunk_id,\n  1 - (embedding &lt;=&gt; $1::vector) AS score\nFROM code_chunks_myrepo\nWHERE repo_id = $2\nORDER BY embedding &lt;=&gt; $1::vector\nLIMIT $3;\n</code></pre> <p>Notes:</p> <ul> <li><code>&lt;=&gt;</code> is commonly used for cosine distance in pgvector (operator depends on your pgvector version/operator class).</li> <li>We often convert distance to similarity via <code>1 - distance</code> to produce a \u201chigher is better\u201d <code>score</code>.</li> </ul> <p>Score semantics</p> <p><code>VectorRetriever</code> filters by <code>config.similarity_threshold</code> assuming higher score is better. Ensure your <code>vector_search()</code> implementation returns a score aligned with that expectation.</p>"},{"location":"features/pgvector/#2-python-calling-the-vector-retriever","title":"2) Python: calling the vector retriever","text":"<pre><code>from server.retrieval.vector import VectorRetriever\nfrom server.models.config import VectorSearchConfig\n\nconfig = VectorSearchConfig(top_k=25, similarity_threshold=0.2)\n\nresults = await VectorRetriever(postgres, embedder).search(\n    repo_id=\"myrepo\",\n    query=\"Where is the vector search implemented?\",\n    config=config,\n)\n</code></pre>"},{"location":"features/pgvector/#3-postgresclient-contract-what-you-implement","title":"3) PostgresClient contract (what you implement)","text":"<pre><code>class PostgresClient:\n    async def upsert_embeddings(self, repo_id: str, chunks: list[Chunk]) -&gt; int:\n        raise NotImplementedError\n\n    async def vector_search(self, repo_id: str, embedding: list[float], top_k: int) -&gt; list[ChunkMatch]:\n        raise NotImplementedError\n\n    async def delete_embeddings(self, repo_id: str) -&gt; int:\n        raise NotImplementedError\n</code></pre> Click to expand: minimal expectations for vector_search() <ul> <li>Accepts:</li> <li><code>repo_id</code> to scope results</li> <li><code>embedding</code> as a Python <code>list[float]</code></li> <li><code>top_k</code> limit</li> <li>Returns:</li> <li><code>list[ChunkMatch]</code> with stable identifiers and a numeric <code>score</code></li> <li>Must be deterministic for the same inputs (modulo concurrent indexing)</li> </ul>"},{"location":"features/pgvector/#performance-tuning-tips","title":"Performance tuning tips","text":""},{"location":"features/pgvector/#index-choice-parameters","title":"Index choice &amp; parameters","text":"<ul> <li>Prefer HNSW for interactive search latency and strong recall.</li> <li>Prefer IVFFlat when:</li> <li>you have very large corpora</li> <li>you can tune <code>lists</code>/<code>probes</code></li> <li>you can rebuild indexes periodically</li> </ul> <p>Tune with real queries</p> <p>Use a representative query set and measure: - p50/p95 latency - recall@k (against a brute-force baseline on a sample) - Postgres CPU and memory</p>"},{"location":"features/pgvector/#batch-upserts-and-concurrency","title":"Batch upserts and concurrency","text":"<ul> <li>Increase <code>indexing_batch_size</code> to improve throughput, but watch:</li> <li>memory usage (embedding arrays + chunk payloads)</li> <li>transaction size and lock contention</li> <li>Increase <code>indexing_workers</code> until Postgres becomes the bottleneck; then stop.</li> </ul>"},{"location":"features/pgvector/#keep-vectors-close-to-the-query-path","title":"Keep vectors \u201cclose\u201d to the query path","text":"<ul> <li>If you store per-repo tables (<code>table_name = \"code_chunks_{repo}\"</code>), you reduce index size per table and keep scans/index traversals smaller.</li> <li>If you store everything in one table, ensure you have:</li> <li>a selective <code>repo_id</code> filter</li> <li>appropriate composite indexing strategy for metadata filters (when used)</li> </ul>"},{"location":"features/pgvector/#thresholding-and-top-k","title":"Thresholding and top-k","text":"<ul> <li><code>top_k</code> is the first-order driver of query cost.</li> <li><code>similarity_threshold</code> (in <code>VectorSearchConfig</code>) can reduce downstream load (reranking, hydration), but it does not reduce the cost of the initial ANN query unless you also add SQL-side filtering (which is usually not effective with ANN ordering).</li> </ul> <p>Don\u2019t over-filter early</p> <p>In tri-brid fusion, vector results are only one leg. Overly aggressive thresholds can reduce recall and harm fusion quality, especially when sparse/graph legs are also contributing.</p>"},{"location":"features/pgvector/#related-pages","title":"Related pages","text":"<ul> <li>Sparse Search (FTS/BM25)</li> <li>Graph Search (Neo4j)</li> <li>Fusion (RRF / Weighted)</li> <li>Configuration (Pydantic models)</li> </ul>"},{"location":"features/tribrid-search/","title":"Tri-Brid Search","text":""},{"location":"features/tribrid-search/#tri-brid-search-architecture","title":"Tri-Brid Search Architecture","text":"<p>TriBridRAG is a tri-brid retrieval engine: we run three independent search legs\u2014vector, sparse, and graph\u2014and then fuse their candidate sets into a single ranked list. This architecture is designed to maximize recall across different \u201csignals\u201d (semantic similarity, lexical match, and structural relationships) while keeping the system tunable and observable.</p> <p>Pydantic is the law</p> <p>All retrieval and fusion behavior is governed by Pydantic models in <code>server/models/tribrid_config_model.py</code>. Any configuration that affects tri-brid search should be represented there first. TypeScript types are generated from Pydantic\u2014never hand-written.</p>"},{"location":"features/tribrid-search/#1-overview-the-three-search-legs","title":"1) Overview: the three search legs","text":"<p>TriBridRAG runs these legs in parallel (conceptually; implementation may choose concurrency):</p> Leg Backend Best at Typical failure mode Vector search PostgreSQL + pgvector Semantic similarity, paraphrases, \u201cwhat does this do?\u201d queries Misses exact identifiers/strings; can be noisy for short queries Sparse search PostgreSQL FTS/BM25 Exact terms, symbols, filenames, error messages Misses paraphrases; brittle to synonyms Graph search Neo4j Relationship-aware retrieval (calls/imports/contains), entity neighborhoods, communities Needs good entity extraction; can drift if graph is stale <p>At the code level, each leg is encapsulated as a retriever:</p> <ul> <li><code>server/retrieval/vector.py</code> \u2192 <code>VectorRetriever</code></li> <li><code>server/retrieval/sparse.py</code> \u2192 <code>SparseRetriever</code></li> <li><code>server/retrieval/graph.py</code> \u2192 <code>GraphRetriever</code></li> </ul> <p>Fusion happens in:</p> <ul> <li><code>server/retrieval/fusion.py</code> \u2192 <code>TriBridFusion</code> (fusion utilities implemented; <code>search()</code> orchestrator is currently <code>NotImplementedError</code>)</li> </ul>"},{"location":"features/tribrid-search/#2-data-flow-mermaid","title":"2) Data flow (Mermaid)","text":"<pre><code>flowchart LR\n    UserQuery[\"User Query\"] --&gt; Fusion[\"TriBridFusion\n(Fuse 3 legs)\"]\n\n    Fusion --&gt; Vector[\"Vector Search\n(pgvector in Postgres)\"]\n    Fusion --&gt; Sparse[\"Sparse Search\n(PostgreSQL FTS/BM25)\"]\n    Fusion --&gt; Graph[\"Graph Search\n(Neo4j traversal)\"]\n\n    Vector --&gt; VRes[\"ChunkMatch[]\n(scored)\"]\n    Sparse --&gt; SRes[\"ChunkMatch[]\n(scored)\"]\n    Graph --&gt; GRes[\"ChunkMatch[]\n(scored)\"]\n\n    VRes --&gt; Merge[\"Fusion Method\n(RRF or Weighted)\"]\n    SRes --&gt; Merge\n    GRes --&gt; Merge\n\n    Merge --&gt; Ranked[\"Ranked ChunkMatch[]\n(final)\"]</code></pre>"},{"location":"features/tribrid-search/#3-how-each-search-type-works","title":"3) How each search type works","text":""},{"location":"features/tribrid-search/#31-vector-search-pgvector-in-postgresql","title":"3.1 Vector search (pgvector in PostgreSQL)","text":"<p>Implementation anchor: <code>server/retrieval/vector.py</code></p> <pre><code>from server.db.postgres import PostgresClient\nfrom server.indexing.embedder import Embedder\nfrom server.models.config import VectorSearchConfig\nfrom server.models.retrieval import ChunkMatch\n\n\nclass VectorRetriever:\n    def __init__(self, postgres: PostgresClient, embedder: Embedder):\n        self.postgres = postgres\n        self.embedder = embedder\n\n    async def search(self, repo_id: str, query: str, config: VectorSearchConfig) -&gt; list[ChunkMatch]:\n        embedding = await self.embedder.embed(query)\n        results = await self.postgres.vector_search(repo_id, embedding, config.top_k)\n        if config.similarity_threshold &gt; 0:\n            results = [r for r in results if r.score &gt;= config.similarity_threshold]\n        return results\n</code></pre> <ol> <li>We embed the query text into a dense vector using the configured embedding provider/model.</li> <li>We query PostgreSQL via <code>PostgresClient.vector_search(...)</code> to retrieve the top-K nearest chunks.</li> <li>We optionally filter by a similarity threshold.</li> </ol>"},{"location":"features/tribrid-search/#indexing-and-ann-hnsw","title":"Indexing and ANN (HNSW)","text":"<p>In pgvector, approximate nearest neighbor search is typically implemented with HNSW indexes. Conceptually:</p> <ul> <li>We store one embedding per chunk (or per chunk_summary, depending on indexing strategy).</li> <li>We create an HNSW index on the vector column.</li> <li>Queries use <code>ORDER BY embedding &lt;-&gt; :query_embedding LIMIT :k</code> (operator depends on metric).</li> </ul> <p>Common similarity metrics in pgvector:</p> <code>L2 distance</code> Good default for many embedding models; uses Euclidean distance. <code>Inner product</code> Often used when embeddings are normalized or trained for dot-product similarity. <code>Cosine distance</code> Common for text embeddings; typically implemented by normalizing vectors or using cosine operators. <p>When vector search shines</p> <p>Use vector search when users ask conceptual questions (\u201chow does auth work?\u201d), when they paraphrase, or when they don\u2019t know the exact symbol names.</p> <p>Short queries can be noisy</p> <p>Very short queries (e.g., <code>cache</code>) can produce semantically broad matches. In those cases, sparse search and graph search often provide better precision.</p>"},{"location":"features/tribrid-search/#32-sparse-search-postgresql-fts-bm25","title":"3.2 Sparse search (PostgreSQL FTS / BM25)","text":"<p>Implementation anchor: <code>server/retrieval/sparse.py</code></p> <pre><code>from server.db.postgres import PostgresClient\nfrom server.models.config import SparseSearchConfig\nfrom server.models.retrieval import ChunkMatch\n\n\nclass SparseRetriever:\n    def __init__(self, postgres: PostgresClient):\n        self.postgres = postgres\n\n    async def search(self, repo_id: str, query: str, config: SparseSearchConfig) -&gt; list[ChunkMatch]:\n        return await self.postgres.sparse_search(repo_id, query, config.top_k)\n</code></pre> <p>Sparse retrieval is lexical: it ranks documents/chunks based on term matching. In practice, we use PostgreSQL full-text search primitives and a BM25-style scoring model.</p>"},{"location":"features/tribrid-search/#bm25-intuition-why-it-works-for-code","title":"BM25 intuition (why it works for code)","text":"<p>BM25 is a bag-of-words ranking function that combines:</p> <code>TF (term frequency)</code> More occurrences of a term in a chunk increases score, but with diminishing returns. <code>IDF (inverse document frequency)</code> Rare terms (e.g., a specific function name) are more discriminative and get higher weight. <code>Length normalization</code> Longer chunks are penalized so they don\u2019t win just because they contain more words. <p>In <code>server/models/tribrid_config_model.py</code>, the retrieval section exposes BM25 tuning:</p> <code>bm25_k1</code> Term frequency saturation (higher \u2192 TF matters more). <code>bm25_b</code> Length normalization (0 \u2192 no length penalty, 1 \u2192 full penalty). For code, moderate values are often better. <p>When sparse search shines</p> <p>Use sparse search for exact identifiers, error messages, config keys, filenames, and API routes\u2014anything where the user likely knows the literal text.</p>"},{"location":"features/tribrid-search/#33-graph-search-neo4j","title":"3.3 Graph search (Neo4j)","text":"<p>Implementation anchor: <code>server/retrieval/graph.py</code></p> <pre><code>from server.db.neo4j import Neo4jClient\nfrom server.indexing.embedder import Embedder\nfrom server.models.config import GraphSearchConfig\nfrom server.models.retrieval import ChunkMatch\n\n\nclass GraphRetriever:\n    def __init__(self, neo4j: Neo4jClient, embedder: Embedder):\n        self.neo4j = neo4j\n        self.embedder = embedder\n\n    async def search(self, repo_id: str, query: str, config: GraphSearchConfig) -&gt; list[ChunkMatch]:\n        return await self.neo4j.graph_search(repo_id, query, config.max_hops, config.top_k)\n\n    async def expand_context(self, chunk_ids: list[str], max_hops: int) -&gt; list[ChunkMatch]:\n        raise NotImplementedError\n</code></pre> <p>Graph retrieval uses Neo4j to exploit entity relationships extracted from the codebase:</p> <ul> <li>Entities: functions, classes, modules, variables, imports (configurable)</li> <li>Relationships: calls, imports, inherits, contains, references (configurable)</li> </ul> <p>Graph search typically works like:</p> <ol> <li>Identify seed entities relevant to the query (by name match, indexed metadata, or precomputed mappings).</li> <li>Traverse relationships up to <code>max_hops</code>.</li> <li>Return chunks associated with visited entities, optionally boosted by:</li> <li>proximity (fewer hops)</li> <li>relationship type</li> <li>community membership</li> </ol>"},{"location":"features/tribrid-search/#communities-why-they-matter","title":"Communities (why they matter)","text":"<p>TriBridRAG supports community detection in the graph storage config:</p> <ul> <li><code>include_communities: bool</code></li> <li><code>community_algorithm: \"louvain\" | \"label_propagation\"</code></li> </ul> <p>Communities help cluster related entities (e.g., all auth-related modules). During retrieval, community membership can be used to:</p> <ul> <li>expand results beyond direct neighbors</li> <li>reduce noise by preferring entities in the same community as strong seeds</li> </ul> <p>Graph search is relationship-first</p> <p>Graph retrieval is not a replacement for semantic or lexical search. It\u2019s a structural amplifier: once we find something relevant, the graph helps us pull in the surrounding implementation context (callers, callees, base classes, imported modules, etc.).</p>"},{"location":"features/tribrid-search/#4-fusion-methods-rrf-vs-weighted-scoring","title":"4) Fusion methods: RRF vs weighted scoring","text":"<p>Implementation anchor: <code>server/retrieval/fusion.py</code></p> <p>TriBridRAG supports two fusion strategies:</p> <ul> <li>RRF (Reciprocal Rank Fusion): rank-based, robust across heterogeneous scoring scales</li> <li>Weighted scoring: score-based, more controllable when scores are calibrated/normalized</li> </ul>"},{"location":"features/tribrid-search/#41-rrf-reciprocal-rank-fusion","title":"4.1 RRF (Reciprocal Rank Fusion)","text":"<pre><code>def rrf_fusion(self, results: list[list[ChunkMatch]], k: int) -&gt; list[ChunkMatch]:\n    scores: dict[str, float] = defaultdict(float)\n    chunk_map: dict[str, ChunkMatch] = {}\n    for result_list in results:\n        for rank, chunk in enumerate(result_list):\n            scores[chunk.chunk_id] += 1.0 / (k + rank + 1)\n            chunk_map[chunk.chunk_id] = chunk\n    sorted_ids = sorted(scores, key=lambda cid: scores[cid], reverse=True)\n    return [chunk_map[cid].model_copy(update={\"score\": scores[cid]}) for cid in sorted_ids]\n</code></pre> <ol> <li>Each leg contributes a score based only on rank (not raw score).</li> <li>Higher-ranked items get more contribution; <code>k</code> smooths how steeply rank matters.</li> <li>Items appearing in multiple legs naturally rise to the top.</li> </ol> <code>rrf_k</code> Smoothing constant. Higher values reduce the dominance of top-1 positions and spread credit further down the list. <p>Default to RRF when in doubt</p> <p>RRF is usually the safest fusion method because vector, sparse, and graph scores often live on different scales.</p>"},{"location":"features/tribrid-search/#42-weighted-scoring-fusion","title":"4.2 Weighted scoring fusion","text":"<pre><code>def weighted_fusion(self, results: list[list[ChunkMatch]], weights: list[float]) -&gt; list[ChunkMatch]:\n    scores: dict[str, float] = defaultdict(float)\n    chunk_map: dict[str, ChunkMatch] = {}\n    for weight, result_list in zip(weights, results):\n        for chunk in result_list:\n            scores[chunk.chunk_id] += chunk.score * weight\n            chunk_map[chunk.chunk_id] = chunk\n    sorted_ids = sorted(scores, key=lambda cid: scores[cid], reverse=True)\n    return [chunk_map[cid].model_copy(update={\"score\": scores[cid]}) for cid in sorted_ids]\n</code></pre> <ol> <li>Each leg contributes <code>chunk.score * weight</code>.</li> <li>This assumes scores are comparable or normalized.</li> </ol> <code>normalize_scores</code> (FusionConfig) When enabled, we should normalize per-leg scores to a common range (typically <code>[0,1]</code>) before applying weights. (The config exists; ensure the orchestrator applies it consistently.) <p>Weighted fusion requires score hygiene</p> <p>If one leg produces scores in <code>[0, 100]</code> and another in <code>[0, 1]</code>, weighted fusion will be dominated by the larger-scale scorer unless we normalize.</p>"},{"location":"features/tribrid-search/#5-configuration-when-to-use-what","title":"5) Configuration: when to use what","text":"<p>Fusion behavior is controlled by <code>FusionConfig</code> in <code>server/models/tribrid_config_model.py</code>:</p> <code>method</code> <code>\"rrf\"</code> or <code>\"weighted\"</code> <code>vector_weight</code>, <code>sparse_weight</code>, <code>graph_weight</code> Used for weighted fusion; automatically normalized to sum to 1.0 by Pydantic. <code>rrf_k</code> Used for RRF. <code>normalize_scores</code> Intended for weighted fusion (and can also be useful for diagnostics)."},{"location":"features/tribrid-search/#recommended-presets","title":"Recommended presets","text":"<p>Practical starting points</p> <p>These are starting points; tune using evals and query logs.</p> Scenario Fusion method Suggested weights / params Why General code Q&amp;A RRF <code>rrf_k=60</code> Robust across score scales; rewards agreement Symbol-heavy queries (stack traces, identifiers) Weighted <code>sparse_weight\u2191</code>, <code>vector_weight\u2193</code> Lexical match is the strongest signal Architecture / \u201chow does X work\u201d Weighted or RRF <code>vector_weight\u2191</code>, keep graph moderate Semantic similarity finds entry points; graph expands neighborhood Refactor impact / \u201cwho calls this\u201d Weighted <code>graph_weight\u2191</code>, <code>max_hops</code> tuned Relationship traversal is primary signal FusionConfig parameters (definition list) <code>method</code> Fusion strategy: <code>\"rrf\"</code> or <code>\"weighted\"</code>. <code>vector_weight</code> Contribution of pgvector results in weighted fusion. <code>sparse_weight</code> Contribution of PostgreSQL FTS/BM25 results in weighted fusion. <code>graph_weight</code> Contribution of Neo4j graph results in weighted fusion. <code>rrf_k</code> RRF smoothing constant. <code>normalize_scores</code> Normalize per-leg scores before weighted fusion."},{"location":"features/tribrid-search/#6-performance-considerations","title":"6) Performance considerations","text":""},{"location":"features/tribrid-search/#61-latency-budget-and-parallelism","title":"6.1 Latency budget and parallelism","text":"<p>Tri-brid retrieval is only as fast as the slowest leg if executed sequentially. In practice:</p> <ul> <li>Vector search in Postgres (pgvector + HNSW) is typically fast and predictable.</li> <li>Sparse search is usually fast but can degrade with poor indexing/tokenization or very large corpora.</li> <li>Graph search latency depends heavily on:</li> <li>traversal depth (<code>max_hops</code>)</li> <li>graph density</li> <li>whether community metadata is precomputed</li> <li>query shape and constraints</li> </ul> <p>Graph traversal can explode</p> <p>Keep <code>max_hops</code> small (typically 1\u20132) unless you have strong pruning. Dense graphs can produce huge neighborhoods.</p>"},{"location":"features/tribrid-search/#62-candidate-set-sizing-top_k","title":"6.2 Candidate set sizing (top_k)","text":"<p>Each leg returns <code>top_k</code> candidates; fusion then ranks the union. Larger <code>top_k</code> increases recall but:</p> <ul> <li>increases CPU time in fusion</li> <li>increases downstream reranking cost (if enabled)</li> <li>increases hydration cost (if you hydrate content eagerly)</li> </ul> <p>A common pattern is:</p> <ul> <li>retrieve generously per leg (e.g., 50\u2013100)</li> <li>fuse</li> <li>optionally rerank a smaller set</li> <li>return a final <code>final_k</code></li> </ul>"},{"location":"features/tribrid-search/#63-score-normalization-and-calibration","title":"6.3 Score normalization and calibration","text":"<p>If you use weighted fusion:</p> <ul> <li>normalize scores per leg (min-max or rank-to-score)</li> <li>ensure similarity metrics are consistent (cosine vs L2 vs inner product)</li> <li>be explicit about thresholds (vector similarity thresholds can prune aggressively)</li> </ul>"},{"location":"features/tribrid-search/#64-index-health","title":"6.4 Index health","text":"<p>Vector and sparse retrieval both depend on Postgres indexes:</p> <ul> <li>pgvector HNSW index parameters affect recall/latency tradeoffs.</li> <li>FTS/BM25 depends on tokenization choices (stemming, stopwords) and maintaining tsvector columns/materialized structures.</li> </ul> <p>Graph retrieval depends on:</p> <ul> <li>entity extraction quality</li> <li>relationship correctness</li> <li>keeping Neo4j in sync with the indexed corpus</li> </ul> <p>Stale graph = misleading context</p> <p>If Neo4j is not updated alongside code indexing, graph traversal can return \u201cvalid\u201d but outdated relationships. Treat graph freshness as a first-class operational concern.</p>"},{"location":"features/tribrid-search/#related-pages","title":"Related pages","text":"<ul> <li>Configuration model (Pydantic)</li> <li>Retrieval pipeline overview</li> <li>Graph RAG (Neo4j) storage and traversal</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#installation","title":"Installation","text":"<p>This page covers installing TriBridRAG in two supported modes:</p> <ul> <li>Docker-based installation (recommended): fastest path to a working tri-brid stack (PostgreSQL + pgvector, PostgreSQL FTS/BM25-style ranking, and Neo4j graph traversal).</li> <li>Local development with <code>uv</code>: run the FastAPI backend on your host while still using Docker for databases (recommended for development), or run everything locally if you prefer.</li> </ul> <p>Tri-brid architecture reminder</p> <p>TriBridRAG fuses three retrieval legs:</p> <ol> <li>Vector search in PostgreSQL via pgvector</li> <li>Sparse search via PostgreSQL Full-Text Search (BM25-style ranking)</li> <li>Graph search via Neo4j (entities, relationships, communities)</li> </ol> <p>Fusion is performed via RRF (Reciprocal Rank Fusion) or weighted scoring. See Architecture and Retrieval &amp; Fusion.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"getting-started/installation/#required","title":"Required","text":"<ul> <li>Docker and Docker Compose</li> <li>Python 3.11+</li> <li>Node.js 18+ (for the <code>web/</code> frontend)</li> <li>uv (Python package manager)</li> </ul>"},{"location":"getting-started/installation/#recommended","title":"Recommended","text":"<ul> <li><code>curl</code> (for health checks)</li> <li>A working embedding provider API key (e.g., OpenAI or Voyage) if you plan to index and search immediately</li> </ul> Install uv (Click to expand) <p><code>uv</code> installation options vary by OS. Follow the official instructions: https://docs.astral.sh/uv/</p> <p>After installation, verify:</p> <pre><code>uv --version\npython --version\n</code></pre>"},{"location":"getting-started/installation/#option-a-docker-based-installation-recommended","title":"Option A \u2014 Docker-based installation (recommended)","text":"<p>This option runs the full stack via Docker Compose. It is the most reproducible way to run TriBridRAG.</p>"},{"location":"getting-started/installation/#1-clone-the-repository","title":"1) Clone the repository","text":"<pre><code>git clone https://github.com/your-org/tribrid-rag.git\ncd tribrid-rag\n</code></pre>"},{"location":"getting-started/installation/#2-create-your-environment-file","title":"2) Create your environment file","text":"<pre><code>cp .env.example .env\n</code></pre> <p>Edit <code>.env</code> and set the required variables (see Environment variables).</p> <p>Do not skip <code>.env</code></p> <p>The API container loads configuration from <code>.env</code>. Missing keys commonly cause indexing/search to fail (especially embedding and reranking providers).</p>"},{"location":"getting-started/installation/#3-start-infrastructure-postgresql-neo4j-observability","title":"3) Start infrastructure (PostgreSQL + Neo4j + observability)","text":"<pre><code>docker compose up -d postgres neo4j grafana prometheus\n</code></pre> <p>You can also start everything, including the API container:</p> <pre><code>docker compose up -d\n</code></pre>"},{"location":"getting-started/installation/#4-confirm-containers-are-healthy","title":"4) Confirm containers are healthy","text":"<pre><code>docker compose ps\n</code></pre> <p>You should see <code>postgres</code> and <code>neo4j</code> as healthy.</p> Expected ports (Click to expand) <ul> <li>PostgreSQL: <code>localhost:${POSTGRES_PORT:-5432}</code></li> <li>Neo4j HTTP: http://localhost:7474</li> <li>Neo4j Bolt: <code>bolt://localhost:7687</code></li> <li>API: http://localhost:${SERVER_PORT:-8000}</li> <li>Grafana: http://localhost:${GRAFANA_PORT:-3000}</li> <li>Prometheus: http://localhost:9090</li> </ul>"},{"location":"getting-started/installation/#option-b-local-development-setup-with-uv","title":"Option B \u2014 Local development setup with <code>uv</code>","text":"<p>This option runs the backend on your host machine (fast iteration), while databases run in Docker.</p>"},{"location":"getting-started/installation/#1-start-databases-via-docker-compose","title":"1) Start databases via Docker Compose","text":"<pre><code>docker compose up -d postgres neo4j\n</code></pre>"},{"location":"getting-started/installation/#2-install-python-dependencies-with-uv","title":"2) Install Python dependencies with <code>uv</code>","text":"<p>From the repo root:</p> <pre><code>uv sync\n</code></pre> <p>This installs dependencies from <code>pyproject.toml</code> (Python 3.11+ required).</p>"},{"location":"getting-started/installation/#3-run-the-api-locally","title":"3) Run the API locally","text":"<pre><code>uv run uvicorn server.main:app --reload\n</code></pre> <p>The API should be available at:</p> <ul> <li>http://localhost:8000</li> <li>OpenAPI docs: http://localhost:8000/docs</li> </ul>"},{"location":"getting-started/installation/#4-optional-install-and-run-the-frontend","title":"4) (Optional) Install and run the frontend","text":"<pre><code>cd web\nnpm install\nnpm run dev\n</code></pre> <p>Frontend dev server:</p> <ul> <li>http://localhost:5173</li> </ul> <p>Recommended dev workflow</p> <p>Run PostgreSQL + Neo4j in Docker, and run the API locally with <code>uv</code>. This keeps your iteration loop fast while preserving reproducible databases.</p>"},{"location":"getting-started/installation/#environment-variables-required","title":"Environment variables required","text":"<p>TriBridRAG reads runtime configuration from environment variables (and from the project\u2019s configuration model). At minimum, you must provide database connectivity and at least one embedding provider if you plan to index.</p> <p>Pydantic is the law</p> <p>All configuration is ultimately validated and shaped by the Pydantic model(s) in <code>tribrid_config_model.py</code>. If a value is not accepted by the model, it is not a valid configuration\u2014even if it exists in <code>.env</code>.</p>"},{"location":"getting-started/installation/#core-database-variables","title":"Core database variables","text":"<p>These are used by the API to connect to PostgreSQL and Neo4j.</p> PostgreSQL <code>POSTGRES_HOST</code> <code>POSTGRES_PORT</code> <code>POSTGRES_DB</code> <code>POSTGRES_USER</code> <code>POSTGRES_PASSWORD</code> Neo4j <code>NEO4J_URI</code> <code>NEO4J_USER</code> <code>NEO4J_PASSWORD</code> <p>Docker Compose defaults</p> <p>The provided <code>docker-compose.yml</code> sets sensible defaults:</p> <ul> <li>PostgreSQL container: <code>tribrid-postgres</code> (pgvector-enabled image)</li> <li>Neo4j container: <code>tribrid-neo4j</code> (Neo4j 5 community + APOC plugin)</li> </ul> <p>When running the API inside Docker Compose, the service uses:</p> <ul> <li><code>POSTGRES_HOST=postgres</code></li> <li><code>NEO4J_URI=bolt://neo4j:7687</code></li> </ul>"},{"location":"getting-started/installation/#ai-provider-variables-typical","title":"AI provider variables (typical)","text":"<p>You will usually need at least one of these to index and retrieve effectively:</p> OpenAI <code>OPENAI_API_KEY</code> Voyage <code>VOYAGE_API_KEY</code> Cohere (reranking) <code>COHERE_API_KEY</code> <p>Indexing requires embeddings</p> <p>If you attempt to index a repository without a working embedding provider (or a configured local embedding model), the vector leg cannot be built, and tri-brid fusion quality will degrade or indexing may fail depending on your configuration.</p>"},{"location":"getting-started/installation/#database-setup","title":"Database setup","text":"<p>TriBridRAG uses two databases:</p> <ul> <li>PostgreSQL (single instance) for:</li> <li>vector embeddings via pgvector</li> <li>sparse retrieval via PostgreSQL Full-Text Search (BM25-style ranking)</li> <li>metadata and chunk storage</li> <li>Neo4j for:</li> <li>entity nodes, relationship edges, and community structure used by graph retrieval</li> </ul>"},{"location":"getting-started/installation/#postgresql-pgvector","title":"PostgreSQL + pgvector","text":"<p>The Docker Compose configuration uses:</p> <ul> <li>Image: <code>pgvector/pgvector:pg16</code></li> <li>Port: <code>5432</code> (configurable via <code>POSTGRES_PORT</code>)</li> <li>Persistent volume: <code>postgres_data</code></li> </ul> <p>pgvector is required</p> <p>Vector search is implemented in PostgreSQL using pgvector. Ensure your PostgreSQL instance has the extension available.</p> Manual verification (Click to expand) <p>If you want to confirm pgvector is available:</p> <pre><code>docker exec -it tribrid-postgres psql -U postgres -d tribrid_rag -c \"CREATE EXTENSION IF NOT EXISTS vector;\"\n</code></pre> <p>If your database name/user differ, adjust <code>-U</code> and <code>-d</code> accordingly.</p>"},{"location":"getting-started/installation/#neo4j","title":"Neo4j","text":"<p>The Docker Compose configuration uses:</p> <ul> <li>Image: <code>neo4j:5-community</code></li> <li>Ports:</li> <li>HTTP: <code>7474</code></li> <li>Bolt: <code>7687</code></li> <li>APOC plugin enabled</li> <li>Persistent volumes: <code>neo4j_data</code>, <code>neo4j_logs</code></li> </ul> <p>Neo4j credentials must match</p> <p>The API connects using <code>NEO4J_URI</code>, <code>NEO4J_USER</code>, and <code>NEO4J_PASSWORD</code>. The Neo4j container is configured via <code>NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}</code>\u2014ensure these values are consistent.</p>"},{"location":"getting-started/installation/#verifying-the-installation","title":"Verifying the installation","text":""},{"location":"getting-started/installation/#1-verify-services-are-reachable","title":"1) Verify services are reachable","text":"API health (Docker or local API) <code>bash linenums=\"1\"   curl -f http://localhost:8000/health</code> Neo4j HTTP <code>bash linenums=\"1\"   curl -f http://localhost:7474</code> PostgreSQL readiness (Docker) <code>bash linenums=\"1\"   docker exec -it tribrid-postgres pg_isready -U postgres</code> <p>FastAPI docs</p> <p>If the API is running, confirm OpenAPI renders:</p> <ul> <li>http://localhost:8000/docs</li> </ul>"},{"location":"getting-started/installation/#2-verify-tri-brid-retrieval-prerequisites","title":"2) Verify tri-brid retrieval prerequisites","text":"<ul> <li>PostgreSQL is up and accepts connections</li> <li>pgvector extension is available</li> <li>Neo4j is up and Bolt is reachable</li> <li>Your embedding provider key is set (or local embeddings are configured)</li> </ul> <p>See also: Configuration and Indexing.</p>"},{"location":"getting-started/installation/#common-installation-issues-and-solutions","title":"Common installation issues and solutions","text":""},{"location":"getting-started/installation/#docker-containers-are-not-healthy","title":"Docker containers are not healthy","text":"<p>Symptoms - <code>docker compose ps</code> shows <code>unhealthy</code> for <code>postgres</code> or <code>neo4j</code> - API container fails to start due to dependency health checks</p> <p>Fixes - Check logs:   </p><pre><code>docker compose logs -f postgres\ndocker compose logs -f neo4j\ndocker compose logs -f api\n</code></pre> - Ensure ports are not already in use (5432, 7474, 7687, 8000, 3000, 9090). - On first boot, Neo4j may take longer; wait and re-check health.<p></p>"},{"location":"getting-started/installation/#port-conflicts-eg-5432-already-in-use","title":"Port conflicts (e.g., 5432 already in use)","text":"<p>Symptoms - Docker reports: \u201cbind: address already in use\u201d</p> <p>Fixes - Change the host port in <code>.env</code> (or your shell environment), e.g.:   - <code>POSTGRES_PORT=55432</code>   - <code>SERVER_PORT=18000</code> - Restart:   </p><pre><code>docker compose down\ndocker compose up -d\n</code></pre><p></p>"},{"location":"getting-started/installation/#api-cannot-connect-to-postgresql-or-neo4j","title":"API cannot connect to PostgreSQL or Neo4j","text":"<p>Symptoms - Connection refused / timeout errors in API logs</p> <p>Fixes - If running API locally, your DB host should typically be <code>localhost</code>, not the Docker service name:   - <code>POSTGRES_HOST=localhost</code>   - <code>NEO4J_URI=bolt://localhost:7687</code> - If running API in Docker Compose, use service names:   - <code>POSTGRES_HOST=postgres</code>   - <code>NEO4J_URI=bolt://neo4j:7687</code></p> <p>Do not mix hostnames across environments</p> <p><code>postgres</code> and <code>neo4j</code> resolve only inside the Docker network. <code>localhost</code> resolves only from your host machine.</p>"},{"location":"getting-started/installation/#missing-embedding-api-keys-provider-misconfiguration","title":"Missing embedding API keys / provider misconfiguration","text":"<p>Symptoms - Indexing fails - Search returns empty or low-quality results - Errors referencing OpenAI/Voyage/Cohere authentication</p> <p>Fixes - Ensure <code>.env</code> contains the correct key(s), e.g. <code>OPENAI_API_KEY</code> or <code>VOYAGE_API_KEY</code>. - Confirm the configuration you selected is valid per the Pydantic model. - Re-run the API after updating <code>.env</code>.</p>"},{"location":"getting-started/installation/#typescript-types-are-out-of-date","title":"TypeScript types are out of date","text":"<p>Symptoms - Frontend build/type errors after backend config/schema changes</p> <p>Fix - Regenerate TypeScript types from Pydantic (never hand-edit):   </p><pre><code>uv run python scripts/generate_types.py\n</code></pre><p></p> <p>Generated types are authoritative</p> <p>TypeScript types are generated from Pydantic models. If you find yourself \u201cfixing\u201d types manually in the frontend, you are working against the architecture.</p>"},{"location":"getting-started/installation/#next-steps","title":"Next steps","text":"<ul> <li>Proceed to Configuration to understand how <code>tribrid_config.json</code> and the Pydantic model control behavior.</li> <li>Then follow Indexing to ingest a repository and build vector/sparse/graph indices.</li> <li>For retrieval behavior and fusion details, see Retrieval &amp; Fusion.</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":""},{"location":"getting-started/quickstart/#quick-start","title":"Quick Start","text":"<p>Get TriBridRAG running locally with the full tri-brid stack:</p> <ul> <li>Vector search via pgvector in PostgreSQL</li> <li>Sparse search via PostgreSQL Full-Text Search / BM25-style ranking</li> <li>Graph search via Neo4j (entities + relationships + communities)</li> <li>Fusion via RRF (Reciprocal Rank Fusion) or weighted scoring</li> </ul> <p>Note</p> <p>Pydantic is the law. All configuration originates from <code>tribrid_config_model.py</code>, and TypeScript types are generated from Pydantic (never hand-written). See Configuration and Type Generation.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker + Docker Compose</li> <li>Python 3.10+ (3.11 recommended)</li> <li>Node.js 18+</li> <li><code>uv</code> (Python package manager)</li> </ul> Click to expand: install uv macOS / LinuxWindows (PowerShell) <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <pre><code>irm https://astral.sh/uv/install.ps1 | iex\n</code></pre>"},{"location":"getting-started/quickstart/#1-clone-and-set-up","title":"1) Clone and set up","text":"<pre><code>git clone https://github.com/your-org/tribrid-rag.git\ncd tribrid-rag\n</code></pre> <p>Copy the environment file and edit it with your provider keys (if applicable):</p> <pre><code>cp .env.example .env\n# Edit .env with your API keys and local overrides\n</code></pre> <p>Install backend dependencies:</p> <pre><code>uv sync\n</code></pre> <p>Install frontend dependencies:</p> <pre><code>cd web\nnpm install\ncd ..\n</code></pre> <p>Tip</p> <p>If you change any Pydantic models that drive the frontend, regenerate TypeScript types: </p><pre><code>uv run python scripts/generate_types.py\n</code></pre><p></p>"},{"location":"getting-started/quickstart/#2-start-infrastructure-postgres-neo4j-observability","title":"2) Start infrastructure (Postgres + Neo4j + observability)","text":"<p>Bring up the core services:</p> <pre><code>docker compose up -d postgres neo4j grafana prometheus\n</code></pre> <p>Note</p> <p>PostgreSQL runs with the pgvector image and stores both relational data and embeddings in the same database instance.</p>"},{"location":"getting-started/quickstart/#3-start-the-api-fastapi","title":"3) Start the API (FastAPI)","text":"<p>Run the backend locally:</p> <pre><code>uv run uvicorn server.main:app --reload\n</code></pre> <p>The API will be available at:</p> <ul> <li>API: <code>http://localhost:8000</code></li> <li>OpenAPI docs: <code>http://localhost:8000/docs</code></li> </ul>"},{"location":"getting-started/quickstart/#4-start-the-web-app","title":"4) Start the web app","text":"<p>In a second terminal:</p> <pre><code>cd web\nnpm run dev\n</code></pre> <p>Open:</p> <ul> <li>Web UI: <code>http://localhost:5173</code></li> </ul>"},{"location":"getting-started/quickstart/#5-verify-services-are-running-health-checks","title":"5) Verify services are running (health checks)","text":""},{"location":"getting-started/quickstart/#docker-container-status","title":"Docker container status","text":"<pre><code>docker compose ps\n</code></pre>"},{"location":"getting-started/quickstart/#api-health-endpoint","title":"API health endpoint","text":"<pre><code>curl -s http://localhost:8000/health | jq .\n</code></pre> <p>If you don\u2019t have <code>jq</code>, you can omit it:</p> <pre><code>curl -s http://localhost:8000/health\n</code></pre>"},{"location":"getting-started/quickstart/#neo4j-and-postgres-ports","title":"Neo4j and Postgres ports","text":"<ul> <li>Neo4j browser: <code>http://localhost:7474</code></li> <li>Neo4j Bolt: <code>bolt://localhost:7687</code></li> <li>Postgres: <code>localhost:5432</code></li> </ul> <p>Warning</p> <p>The <code>api</code> service in <code>docker-compose.yml</code> has its own healthcheck, but if you run the API via <code>uvicorn</code> locally (recommended for development), use <code>GET /health</code> to validate readiness.</p>"},{"location":"getting-started/quickstart/#6-first-api-call-tri-brid-search","title":"6) First API call: tri-brid <code>/search</code>","text":"<p>TriBridRAG\u2019s <code>/search</code> endpoint runs the three retrieval legs (vector + sparse + graph) and fuses results (RRF or weighted scoring depending on config).</p> <p>Danger</p> <p>Search quality depends on having indexed content. If you haven\u2019t indexed a repository yet, do that first via the web UI or the indexing endpoint. See Indexing.</p> <p>Below is a copy-paste example request.</p> <pre><code>curl -X POST \"http://localhost:8000/search\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"query\": \"Where is the configuration model defined and how are types generated?\",\n    \"top_k\": 10\n  }'\n</code></pre> Click to expand: what to expect in the response <p>You should receive a JSON payload containing fused results (and often per-leg metadata depending on configuration). If results are empty, confirm: 1) you indexed a repo, and 2) Postgres + Neo4j are healthy, and 3) your embedding provider is configured (if vector search is enabled).</p>"},{"location":"getting-started/quickstart/#next-steps","title":"Next steps","text":"<ul> <li>Architecture Overview \u2014 how the three retrieval legs fuse into one ranked result set</li> <li>Configuration \u2014 Pydantic-first configuration and runtime overrides</li> <li>Indexing \u2014 chunking, embeddings, entity extraction, and graph building</li> <li>API Reference \u2014 endpoints like <code>/index</code>, <code>/search</code>, <code>/answer</code>, and graph inspection</li> <li>Observability \u2014 Prometheus + Grafana setup and dashboards</li> </ul>"},{"location":"operations/monitoring/","title":"Monitoring","text":""},{"location":"operations/monitoring/#monitoring-observability","title":"Monitoring &amp; Observability","text":"<p>TriBridRAG exposes a standard set of observability surfaces\u2014health/readiness endpoints, Prometheus-compatible metrics, and OpenTelemetry tracing\u2014so I can run it safely in production and diagnose issues across the tri-brid retrieval pipeline (vector + sparse + graph).</p> <p>Warning</p> <p>The current source stubs (<code>server/api/health.py</code>, <code>server/observability/metrics.py</code>, <code>server/observability/tracing.py</code>) are placeholders. This page documents the intended contract and recommended implementation patterns. When you implement these modules, keep the public behavior stable so dashboards/alerts don\u2019t break.</p>"},{"location":"operations/monitoring/#architecture-overview","title":"Architecture overview","text":"<pre><code>flowchart LR\n  client[Client / Load Balancer] --&gt; api[FastAPI API]\n\n  api --&gt; health[/GET /health/]\n  api --&gt; ready[/GET /ready/]\n  api --&gt; metrics[/GET /metrics/]\n\n  api --&gt; tri[TriBridRAG Retrieval]\n  tri --&gt; pg[(PostgreSQL: pgvector + FTS/BM25)]\n  tri --&gt; neo[(Neo4j: entities + relationships + communities)]\n\n  api --&gt; otel[OpenTelemetry SDK]\n  otel --&gt; collector[OTel Collector]\n  collector --&gt; traces[(Tracing backend)]\n  collector --&gt; metricsb[(Metrics backend)]\n  collector --&gt; logs[(Log backend)]\n\n  metrics --&gt; prom[Prometheus]\n  prom --&gt; alert[Alertmanager]</code></pre> <p>Note</p> <p>Observability should cover all three search legs and the fusion step (RRF or weighted scoring). If I can\u2019t see per-leg latency and error rates, I can\u2019t tune fusion or capacity plan.</p>"},{"location":"operations/monitoring/#health-check-endpoints","title":"Health check endpoints","text":"<p>Source: <code>server/api/health.py</code> (router stubs)</p> <p>TriBridRAG exposes three endpoints:</p> <ul> <li><code>GET /health</code> \u2014 liveness</li> <li><code>GET /ready</code> \u2014 readiness (dependency checks)</li> <li><code>GET /metrics</code> \u2014 Prometheus scrape endpoint</li> </ul>"},{"location":"operations/monitoring/#get-health-liveness","title":"<code>GET /health</code> (liveness)","text":"<p>Purpose: answers \u201cis the process up and able to serve HTTP?\u201d</p> <ul> <li>Must not perform deep dependency checks.</li> <li>Should be fast and non-blocking.</li> <li>Should return <code>200</code> if the API event loop is alive.</li> </ul> <p>Response contract (recommended):</p> <ul> <li><code>status</code>: <code>\"ok\"</code> or <code>\"error\"</code></li> <li><code>service</code>: service name (e.g., <code>\"tribridrag-server\"</code>)</li> <li><code>version</code>: build/version string</li> <li><code>time</code>: ISO timestamp</li> </ul>"},{"location":"operations/monitoring/#get-ready-readiness","title":"<code>GET /ready</code> (readiness)","text":"<p>Purpose: answers \u201ccan this instance serve requests successfully right now?\u201d</p> <p>Readiness should validate connectivity to:</p> <ul> <li>PostgreSQL (vector + sparse live in the same Postgres instance)</li> <li>Neo4j (graph leg)</li> <li>Any required model/embedding provider (if applicable)</li> <li>Optional: background workers / ingestion pipelines (if they are required for serving)</li> </ul> <p>Response contract (recommended):</p> <ul> <li><code>status</code>: <code>\"ready\"</code> or <code>\"not_ready\"</code></li> <li><code>checks</code>: object keyed by dependency name with <code>ok</code>, <code>latency_ms</code>, and optional <code>error</code></li> </ul> <p>Tip</p> <p>In Kubernetes, wire <code>/health</code> to <code>livenessProbe</code> and <code>/ready</code> to <code>readinessProbe</code>. Keep <code>/health</code> shallow so transient DB issues don\u2019t cause restart loops.</p>"},{"location":"operations/monitoring/#get-metrics-prometheus","title":"<code>GET /metrics</code> (Prometheus)","text":"<p>Purpose: exposes Prometheus text format metrics.</p> <ul> <li>Must return <code>Content-Type: text/plain; version=0.0.4</code></li> <li>Must be safe to scrape frequently (e.g., every 10\u201330 seconds)</li> </ul>"},{"location":"operations/monitoring/#metrics-prometheus","title":"Metrics (Prometheus)","text":"<p>Source: <code>server/observability/metrics.py</code> (stub)</p> <p>TriBridRAG is designed to export metrics that let me answer:</p> <ol> <li>Is the API healthy (traffic, errors, latency)?</li> <li>Which retrieval leg is slow or failing (vector vs sparse vs graph)?</li> <li>Is fusion behaving as expected (RRF/weighted scoring)?</li> <li>Are dependencies saturated (Postgres/Neo4j latency, pool exhaustion)?</li> </ol>"},{"location":"operations/monitoring/#recommended-metric-families","title":"Recommended metric families","text":"<p>Below is the recommended baseline. Names are suggestions; keep them stable once published.</p>"},{"location":"operations/monitoring/#http-metrics","title":"HTTP metrics","text":"<ul> <li><code>http_requests_total{method,route,status_code}</code></li> <li><code>http_request_duration_seconds_bucket{method,route}</code> (histogram)</li> <li><code>http_inflight_requests{route}</code> (gauge)</li> </ul>"},{"location":"operations/monitoring/#retrieval-pipeline-metrics-tri-brid","title":"Retrieval pipeline metrics (tri-brid)","text":"<ul> <li> <p><code>retrieval_requests_total{mode}</code> <code>mode</code>: <code>vector|sparse|graph|tri_brid</code></p> </li> <li> <p><code>retrieval_leg_duration_seconds_bucket{leg}</code> (histogram) <code>leg</code>: <code>vector|sparse|graph|fusion|rerank</code></p> </li> <li> <p><code>retrieval_leg_errors_total{leg,error_type}</code></p> </li> <li> <p><code>fusion_candidates_total{leg}</code> (gauge or summary)   Number of candidates produced per leg before fusion.</p> </li> <li> <p><code>fusion_method_total{method}</code> <code>method</code>: <code>rrf|weighted</code></p> </li> </ul> <p>Note</p> <p>Fusion metrics are not optional in a tri-brid system. If RRF is enabled, I want to see rank distributions and candidate counts per leg to detect regressions (e.g., graph leg returning zero candidates due to a Neo4j issue).</p>"},{"location":"operations/monitoring/#dependency-metrics","title":"Dependency metrics","text":"<ul> <li><code>postgres_query_duration_seconds_bucket{query_name}</code></li> <li><code>neo4j_query_duration_seconds_bucket{query_name}</code></li> <li><code>db_pool_in_use{db}</code> / <code>db_pool_size{db}</code></li> </ul>"},{"location":"operations/monitoring/#process-metrics","title":"Process metrics","text":"<p>If you use the standard Prometheus client, you\u2019ll also get:</p> <ul> <li><code>process_cpu_seconds_total</code></li> <li><code>process_resident_memory_bytes</code></li> <li><code>python_gc_objects_collected_total</code> (Python runtime dependent)</li> </ul>"},{"location":"operations/monitoring/#instrumentation-points","title":"Instrumentation points","text":"<p>At minimum, instrument:</p> <ul> <li>API request lifecycle (middleware)</li> <li>Each retrieval leg:</li> <li>vector search (pgvector)</li> <li>sparse search (PostgreSQL FTS/BM25)</li> <li>graph search (Neo4j)</li> <li>Fusion step (RRF or weighted scoring)</li> <li>Any reranking step (if present)</li> </ul> Click to expand: Example metric instrumentation skeleton Python <pre><code>from prometheus_client import Counter, Histogram\n\nHTTP_REQUESTS = Counter(\n    \"http_requests_total\",\n    \"Total HTTP requests\",\n    [\"method\", \"route\", \"status_code\"],\n)\n\nRETRIEVAL_LEG_LATENCY = Histogram(\n    \"retrieval_leg_duration_seconds\",\n    \"Latency per retrieval leg\",\n    [\"leg\"],\n    buckets=(0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2, 5, 10),\n)\n\ndef observe_leg(leg: str):\n    return RETRIEVAL_LEG_LATENCY.labels(leg=leg).time()\n</code></pre>"},{"location":"operations/monitoring/#tracing-opentelemetry","title":"Tracing (OpenTelemetry)","text":"<p>Source: <code>server/observability/tracing.py</code> (stub)</p> <p>TriBridRAG uses OpenTelemetry to trace requests end-to-end, including:</p> <ul> <li>HTTP request span (root)</li> <li>Postgres spans (vector + sparse queries)</li> <li>Neo4j spans (graph queries)</li> <li>Fusion span (RRF/weighted scoring)</li> <li>Optional: embedding/reranking spans</li> </ul>"},{"location":"operations/monitoring/#trace-structure-recommended","title":"Trace structure (recommended)","text":"<p>A typical trace should look like:</p> <ul> <li><code>HTTP GET /query</code> (root span)</li> <li><code>vector_search</code> (child span)<ul> <li><code>postgres.query</code> (db span)</li> </ul> </li> <li><code>sparse_search</code> (child span)<ul> <li><code>postgres.query</code> (db span)</li> </ul> </li> <li><code>graph_search</code> (child span)<ul> <li><code>neo4j.query</code> (db span)</li> </ul> </li> <li><code>fusion</code> (child span)</li> <li><code>rerank</code> (optional)</li> </ul>"},{"location":"operations/monitoring/#required-span-attributes","title":"Required span attributes","text":"<p>Use consistent attributes so I can filter and aggregate:</p> <ul> <li><code>retrieval.leg</code>: <code>vector|sparse|graph|fusion|rerank</code></li> <li><code>retrieval.fusion_method</code>: <code>rrf|weighted</code></li> <li><code>retrieval.top_k</code>: integer</li> <li><code>db.system</code>: <code>postgresql</code> or <code>neo4j</code></li> <li><code>db.statement</code>: avoid full raw queries if they contain sensitive data</li> </ul> <p>Danger</p> <p>Do not attach raw user prompts, document text, or secrets to spans. If you need debugging context, attach hashed identifiers or bounded metadata (e.g., <code>doc_count</code>, <code>top_k</code>, <code>community_id</code>).</p>"},{"location":"operations/monitoring/#export-model","title":"Export model","text":"<p>Recommended deployment pattern:</p> <ul> <li>Application exports OTLP to an OpenTelemetry Collector</li> <li>Collector exports to your tracing backend (Jaeger/Tempo/etc.)</li> </ul> Click to expand: Example tracing setup skeleton Python <pre><code>from opentelemetry import trace\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n\ndef setup_tracing(service_name: str, otlp_endpoint: str) -&gt; None:\n    resource = Resource.create({\"service.name\": service_name})\n    provider = TracerProvider(resource=resource)\n    trace.set_tracer_provider(provider)\n\n    exporter = OTLPSpanExporter(endpoint=otlp_endpoint)\n    processor = BatchSpanProcessor(exporter)\n    provider.add_span_processor(processor)\n\ntracer = trace.get_tracer(\"tribridrag\")\n</code></pre>"},{"location":"operations/monitoring/#logging-configuration","title":"Logging configuration","text":"<p>TriBridRAG logging should be:</p> <ul> <li>Structured (JSON preferred) for ingestion into log backends</li> <li>Correlated with traces (include <code>trace_id</code> / <code>span_id</code>)</li> <li>Safe (no secrets, no raw document payloads unless explicitly enabled)</li> </ul>"},{"location":"operations/monitoring/#recommended-log-fields","title":"Recommended log fields","text":"<ul> <li><code>timestamp</code></li> <li><code>level</code></li> <li><code>logger</code></li> <li><code>message</code></li> <li><code>request_id</code> (if you generate one)</li> <li><code>trace_id</code>, <code>span_id</code> (from OpenTelemetry context)</li> <li><code>route</code>, <code>method</code>, <code>status_code</code></li> <li><code>retrieval.fusion_method</code></li> <li><code>retrieval.top_k</code></li> <li><code>latency_ms</code></li> </ul> <p>Tip</p> <p>If you already have OpenTelemetry, prefer log correlation via trace context propagation rather than inventing a parallel correlation ID scheme.</p>"},{"location":"operations/monitoring/#log-levels","title":"Log levels","text":"<ul> <li><code>INFO</code>: request summaries, startup/shutdown, readiness transitions</li> <li><code>WARNING</code>: degraded dependency, partial leg failures (e.g., graph leg timed out but vector+sparse succeeded)</li> <li><code>ERROR</code>: request failures, repeated dependency failures</li> <li><code>DEBUG</code>: gated behind config; never enable globally in production without sampling</li> </ul> Click to expand: Minimal Python logging skeleton (structured) <pre><code>import logging\nimport json\nfrom datetime import datetime, timezone\n\nclass JsonFormatter(logging.Formatter):\n    def format(self, record: logging.LogRecord) -&gt; str:\n        payload = {\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"level\": record.levelname,\n            \"logger\": record.name,\n            \"message\": record.getMessage(),\n        }\n        return json.dumps(payload, ensure_ascii=False)\n\nhandler = logging.StreamHandler()\nhandler.setFormatter(JsonFormatter())\n\nroot = logging.getLogger()\nroot.setLevel(logging.INFO)\nroot.addHandler(handler)\n</code></pre>"},{"location":"operations/monitoring/#alerting-setup","title":"Alerting setup","text":"<p>Alerting should be driven by symptoms that matter to users and by signals that indicate one of the tri-brid legs is failing.</p>"},{"location":"operations/monitoring/#recommended-alert-categories","title":"Recommended alert categories","text":""},{"location":"operations/monitoring/#availability-traffic","title":"Availability &amp; traffic","text":"<ul> <li>API error rate elevated (5xx)</li> <li>API latency elevated (p95/p99)</li> <li>No traffic (if unexpected)</li> </ul>"},{"location":"operations/monitoring/#dependency-health","title":"Dependency health","text":"<ul> <li>Postgres query latency elevated</li> <li>Neo4j query latency elevated</li> <li>Connection pool saturation</li> <li>Readiness failing for &gt; N minutes</li> </ul>"},{"location":"operations/monitoring/#retrieval-quality-signals-operational-proxies","title":"Retrieval quality signals (operational proxies)","text":"<ul> <li>Graph leg returning zero candidates unexpectedly</li> <li>Sparse leg returning zero candidates unexpectedly</li> <li>Fusion candidate counts drop sharply</li> <li>Reranker timeouts (if reranking exists)</li> </ul> <p>Warning</p> <p>Alerting on \u201cquality\u201d is hard. Start with operational proxies (candidate counts, leg error rates, timeouts) before attempting semantic quality alerts.</p>"},{"location":"operations/monitoring/#example-prometheus-alert-rules-illustrative","title":"Example Prometheus alert rules (illustrative)","text":"Click to expand: PrometheusRule examples <pre><code>groups:\n  - name: tribridrag-api\n    rules:\n      - alert: TriBridRAGHigh5xxRate\n        expr: |\n          sum(rate(http_requests_total{status_code=~\"5..\"}[5m]))\n          /\n          sum(rate(http_requests_total[5m])) &gt; 0.02\n        for: 10m\n        labels:\n          severity: page\n        annotations:\n          summary: \"High 5xx rate in TriBridRAG API\"\n\n      - alert: TriBridRAGHighLatencyP99\n        expr: |\n          histogram_quantile(\n            0.99,\n            sum by (le) (rate(http_request_duration_seconds_bucket[5m]))\n          ) &gt; 2\n        for: 10m\n        labels:\n          severity: page\n        annotations:\n          summary: \"High p99 latency in TriBridRAG API\"\n\n      - alert: TriBridRAGNeo4jSlowQueries\n        expr: |\n          histogram_quantile(\n            0.95,\n            sum by (le) (rate(neo4j_query_duration_seconds_bucket[5m]))\n          ) &gt; 1\n        for: 15m\n        labels:\n          severity: ticket\n        annotations:\n          summary: \"Neo4j p95 query latency elevated\"\n</code></pre>"},{"location":"operations/monitoring/#configuration-pydantic-driven","title":"Configuration (Pydantic-driven)","text":"<p>Note</p> <p>Pydantic is the law. All observability configuration should be modeled in <code>tribrid_config_model.py</code>, and any TypeScript types must be generated from that model (never hand-written). This keeps server behavior, deployment values, and UI/admin tooling aligned.</p>"},{"location":"operations/monitoring/#suggested-configuration-parameters","title":"Suggested configuration parameters","text":"<p>Use a dedicated section in the Pydantic config model for observability.</p> Observability settings (suggested) <code>service_name</code> Service name used for logs/traces/metrics labels. <code>metrics_enabled</code> Enable Prometheus metrics endpoint. <code>metrics_path</code> Path for metrics scraping (default <code>/metrics</code>). <code>tracing_enabled</code> Enable OpenTelemetry tracing. <code>otlp_endpoint</code> OTLP exporter endpoint (HTTP or gRPC depending on exporter). <code>trace_sample_ratio</code> Head sampling ratio (0.0\u20131.0). <code>log_format</code> <code>json</code> or <code>text</code>. <code>log_level</code> <code>DEBUG|INFO|WARNING|ERROR</code>. <p>Tip</p> <p>Keep the <code>/metrics</code> endpoint enabled by default in non-public networks. If you expose it publicly, protect it (network policy / auth proxy) to avoid leaking operational details.</p>"},{"location":"operations/monitoring/#related-pages","title":"Related pages","text":"<ul> <li>API</li> <li>Configuration</li> <li>Retrieval pipeline (TriBrid)</li> <li>Graph RAG (Neo4j)</li> </ul>"},{"location":"operations/troubleshooting/","title":"Troubleshooting","text":""},{"location":"operations/troubleshooting/#troubleshooting","title":"Troubleshooting","text":"<p>This page collects the most common TriBridRAG failure modes and the fastest ways to diagnose and fix them. TriBridRAG is tri-brid by design: vector search (pgvector in PostgreSQL), sparse search (PostgreSQL FTS/BM25-style ranking), and graph search (Neo4j). When something \u201cdoesn\u2019t work,\u201d the first step is identifying which leg (or fusion) is failing.</p> <p>Note</p> <p>Before deep debugging, confirm the backend is up and responding: - FastAPI docs: http://localhost:8000/docs - Health endpoint (if exposed): see <code>server/api/health</code></p>"},{"location":"operations/troubleshooting/#quick-triage-checklist","title":"Quick triage checklist","text":"Click to expand <ol> <li>Infrastructure up?</li> <li><code>docker compose ps</code> shows <code>postgres</code> and <code>neo4j</code> as healthy/running.</li> <li>Backend up?</li> <li><code>uv run uvicorn server.main:app --reload</code> is running and logs show no startup errors.</li> <li>Config valid?</li> <li>TriBridRAG config is validated by Pydantic. If config parsing fails, fix config first.</li> <li>Index exists?</li> <li>You must index a repository before search returns meaningful results.</li> <li>Which leg fails?</li> <li>Vector: embeddings + pgvector</li> <li>Sparse: PostgreSQL FTS</li> <li>Graph: Neo4j entities/relationships</li> <li>Fusion: RRF or weighted scoring</li> </ol> <p>Warning</p> <p>Pydantic is the law. If configuration is invalid, TriBridRAG should fail fast. Do not \u201cpatch around\u201d config errors in code or the UI\u2014fix the configuration model inputs instead. See Configuration and Type Generation.</p>"},{"location":"operations/troubleshooting/#1-connection-errors-postgresql-neo4j","title":"1) Connection errors (PostgreSQL / Neo4j)","text":""},{"location":"operations/troubleshooting/#symptoms","title":"Symptoms","text":"<ul> <li>Backend fails at startup or on first request.</li> <li>Errors like:</li> <li><code>connection refused</code></li> <li><code>timeout</code></li> <li><code>authentication failed</code></li> <li><code>could not translate host name</code></li> <li>Search endpoints return 500s; indexing fails immediately.</li> </ul>"},{"location":"operations/troubleshooting/#diagnosis-steps","title":"Diagnosis steps","text":"<ol> <li> <p>Confirm containers are running:    </p><pre><code>docker compose ps\n</code></pre><p></p> </li> <li> <p>Inspect logs for the failing service:    </p><pre><code>docker compose logs -f postgres\n</code></pre> <pre><code>docker compose logs -f neo4j\n</code></pre><p></p> </li> <li> <p>Verify the backend has the correct environment variables (from <code>.env</code>):    </p><pre><code>cat .env\n</code></pre><p></p> </li> <li> <p>Confirm the backend exposes routes (router wiring is in <code>server/main.py</code>):</p> </li> <li><code>/search</code> (tri-brid retrieval)</li> <li><code>/index</code> (indexing)</li> <li><code>/graph/...</code> (graph inspection)</li> <li><code>/config</code> (configuration management)</li> </ol> <p>If the server is up, check:    - http://localhost:8000/docs</p>"},{"location":"operations/troubleshooting/#solutions","title":"Solutions","text":"<ul> <li>PostgreSQL connection refused</li> <li>Ensure <code>postgres</code> is started:     <pre><code>docker compose up -d postgres\n</code></pre></li> <li> <p>Ensure the host/port in <code>.env</code> matches your compose mapping.</p> </li> <li> <p>Neo4j connection/auth issues</p> </li> <li>Ensure <code>neo4j</code> is started:     <pre><code>docker compose up -d neo4j\n</code></pre></li> <li>Confirm Neo4j credentials in <code>.env</code> match the container configuration.</li> <li> <p>If you changed the password, restart Neo4j and update <code>.env</code>.</p> </li> <li> <p>Network name resolution issues</p> </li> <li>If running backend outside Docker, use <code>localhost</code> + published ports.</li> <li>If running backend inside Docker, use the compose service names (<code>postgres</code>, <code>neo4j</code>) as hosts.</li> </ul> <p>Tip</p> <p>If you can hit the FastAPI docs but database calls fail, the issue is almost always <code>.env</code> connectivity or container health\u2014not routing.</p>"},{"location":"operations/troubleshooting/#2-embedding-failures","title":"2) Embedding failures","text":"<p>Embedding failures break the vector leg (pgvector) and often prevent indexing entirely.</p>"},{"location":"operations/troubleshooting/#symptoms_1","title":"Symptoms","text":"<ul> <li>Indexing fails during embedding generation.</li> <li>Search returns only sparse/graph results (or very low quality).</li> <li>Errors like:</li> <li><code>401 Unauthorized</code> / <code>403 Forbidden</code> (API key)</li> <li><code>429 Too Many Requests</code> (rate limiting)</li> <li><code>model not found</code></li> <li>timeouts calling embedding provider</li> </ul>"},{"location":"operations/troubleshooting/#diagnosis-steps_1","title":"Diagnosis steps","text":"<ol> <li>Confirm your embedding provider configuration is valid.</li> <li>TriBridRAG configuration is validated by Pydantic (source of truth: <code>tribrid_config_model.py</code>).</li> <li> <p>If config fails validation, fix config first.</p> </li> <li> <p>Confirm required API keys exist in <code>.env</code>:    </p><pre><code>grep -E \"OPENAI|VOYAGE|JINA|COHERE\" .env\n</code></pre><p></p> </li> <li> <p>Re-run indexing and watch logs:    </p><pre><code>uv run uvicorn server.main:app --reload\n</code></pre>    In another terminal, trigger indexing via the UI or <code>/index</code>.<p></p> </li> <li> <p>If using a local embedding model, confirm it is installed and loadable in your environment.</p> </li> </ol>"},{"location":"operations/troubleshooting/#solutions_1","title":"Solutions","text":"<ul> <li>Auth errors (401/403)</li> <li>Fix the API key in <code>.env</code>.</li> <li> <p>Ensure the backend process was restarted after editing <code>.env</code>.</p> </li> <li> <p>Rate limiting (429)</p> </li> <li>Reduce indexing concurrency (if configurable).</li> <li>Index smaller repos first to validate the pipeline.</li> <li> <p>Retry after cooldown.</p> </li> <li> <p>Model mismatch</p> </li> <li>Ensure the configured embedding model name is supported by the provider.</li> <li>Keep embedding dimensionality consistent with the pgvector column used for storage (dimension mismatches typically surface during insert).</li> </ul> <p>Warning</p> <p>If embeddings fail, the vector leg will be empty. Fusion (RRF/weighted) can still return sparse/graph hits, but overall quality will degrade significantly.</p>"},{"location":"operations/troubleshooting/#3-search-returning-no-results","title":"3) Search returning no results","text":"<p>\u201cNo results\u201d can mean: nothing indexed, one leg empty, filters too strict, or fusion thresholds eliminating candidates.</p>"},{"location":"operations/troubleshooting/#symptoms_2","title":"Symptoms","text":"<ul> <li><code>/search</code> returns an empty list.</li> <li>UI shows no hits even for obvious queries.</li> <li>Graph endpoints return empty entity lists.</li> </ul>"},{"location":"operations/troubleshooting/#diagnosis-steps_2","title":"Diagnosis steps","text":"<ol> <li>Confirm you indexed a repository successfully.</li> <li>Use the UI or <code>/repos</code> + <code>/index</code> flow.</li> <li> <p>Check backend logs for indexing completion.</p> </li> <li> <p>Determine which leg is empty:</p> </li> <li> <p>Vector leg: embedding + pgvector storage</p> </li> <li>Sparse leg: PostgreSQL FTS index populated</li> <li> <p>Graph leg: Neo4j entities/relationships created</p> </li> <li> <p>Check graph population:</p> </li> <li> <p>Use <code>/graph/{repo_id}/entities</code> (listed in README) to confirm entities exist.</p> </li> <li> <p>Check configuration for overly restrictive filters:</p> </li> <li>Repo ID mismatch</li> <li>File path filters</li> <li>Language filters</li> <li>Minimum score thresholds</li> <li>Top-k too small</li> </ol>"},{"location":"operations/troubleshooting/#solutions_2","title":"Solutions","text":"<ul> <li>Nothing indexed</li> <li>Re-run indexing and ensure it completes without errors.</li> <li> <p>Verify Postgres and Neo4j are reachable during indexing.</p> </li> <li> <p>Only one leg populated</p> </li> <li>If vector is empty: fix embeddings (see Embedding failures).</li> <li>If sparse is empty: ensure chunk text is being stored and FTS is enabled in Postgres schema/migrations.</li> <li> <p>If graph is empty: ensure graph-building is enabled and Neo4j is reachable.</p> </li> <li> <p>Fusion eliminates results</p> </li> <li>If using RRF, ensure each leg returns enough candidates (RRF needs ranks to fuse).</li> <li>If using weighted scoring, ensure weights and thresholds aren\u2019t effectively zeroing out all candidates.</li> </ul> <p>Tip</p> <p>When debugging \u201cno results,\u201d temporarily increase per-leg <code>top_k</code> and reduce minimum score thresholds to confirm retrieval is working end-to-end, then tighten back up.</p>"},{"location":"operations/troubleshooting/#4-performance-issues-slow-indexing-slow-search","title":"4) Performance issues (slow indexing / slow search)","text":"<p>Performance problems can come from any of the three legs, plus reranking.</p>"},{"location":"operations/troubleshooting/#symptoms_3","title":"Symptoms","text":"<ul> <li>Indexing takes \u201cforever.\u201d</li> <li><code>/search</code> latency is high.</li> <li>CPU pegged, DB slow queries, or Neo4j traversal is slow.</li> <li>Reranking dominates request time.</li> </ul>"},{"location":"operations/troubleshooting/#diagnosis-steps_3","title":"Diagnosis steps","text":"<ol> <li>Identify which stage is slow:</li> <li>Indexing: chunking \u2192 embedding \u2192 Postgres inserts \u2192 Neo4j writes</li> <li> <p>Search: vector query \u2192 FTS query \u2192 graph traversal \u2192 fusion \u2192 optional rerank</p> </li> <li> <p>Check container resource usage:    </p><pre><code>docker stats\n</code></pre><p></p> </li> <li> <p>Inspect Postgres slow queries (if enabled) and Neo4j query logs.</p> </li> <li> <p>Temporarily disable reranking to isolate retrieval latency:</p> </li> <li>Set reranker mode to <code>none</code> in config (via <code>/config</code> or UI).</li> </ol>"},{"location":"operations/troubleshooting/#solutions_3","title":"Solutions","text":"<ul> <li>Vector search slow</li> <li>Ensure pgvector indexes are created (IVFFlat/HNSW depending on your schema).</li> <li>Reduce <code>top_k</code> for vector retrieval.</li> <li> <p>Avoid overly large embedding dimensions if configurable.</p> </li> <li> <p>Sparse search slow</p> </li> <li>Ensure FTS indexes exist and are used.</li> <li> <p>Reduce query complexity and <code>top_k</code>.</p> </li> <li> <p>Graph traversal slow</p> </li> <li>Reduce traversal depth / breadth (community expansion can explode).</li> <li> <p>Ensure Neo4j has appropriate indexes/constraints for entity lookup.</p> </li> <li> <p>Reranker slow</p> </li> <li>Use <code>none</code> or a faster local model for development.</li> <li>Reduce number of candidates passed into reranking (smaller fused <code>top_k</code>).</li> </ul> <p>Note</p> <p>TriBridRAG fuses three legs. If you set each leg to <code>top_k=50</code> and then rerank 150 candidates, latency will reflect the slowest leg plus reranking overhead.</p>"},{"location":"operations/troubleshooting/#5-memory-problems-oom-crashes","title":"5) Memory problems (OOM / crashes)","text":"<p>Memory issues are most common during indexing (large repos) and reranking (large candidate sets).</p>"},{"location":"operations/troubleshooting/#symptoms_4","title":"Symptoms","text":"<ul> <li>Backend process killed (OOM).</li> <li>Docker containers restart unexpectedly.</li> <li>Python errors like <code>MemoryError</code>.</li> <li>Neo4j or Postgres container exits under load.</li> </ul>"},{"location":"operations/troubleshooting/#diagnosis-steps_4","title":"Diagnosis steps","text":"<ol> <li> <p>Check system and container memory:    </p><pre><code>docker stats\n</code></pre><p></p> </li> <li> <p>Check backend logs around the crash time.</p> </li> <li> <p>Identify whether the spike happens during:</p> </li> <li>Chunking (too many files loaded at once)</li> <li>Embedding batching (batch too large)</li> <li>Reranking (too many candidates)</li> </ol>"},{"location":"operations/troubleshooting/#solutions_4","title":"Solutions","text":"<ul> <li>Reduce indexing batch sizes and concurrency (if configurable).</li> <li>Index fewer repositories at once.</li> <li>Reduce chunk size / number of chunks produced (if configurable).</li> <li>Reduce per-leg <code>top_k</code> and fused candidate count before reranking.</li> <li>Increase Docker memory limits (Docker Desktop) if you\u2019re running locally.</li> </ul> <p>Warning</p> <p>If you increase <code>top_k</code> aggressively across all three legs and enable reranking, memory usage can grow quickly. Keep candidate counts bounded.</p>"},{"location":"operations/troubleshooting/#6-docker-issues","title":"6) Docker issues","text":"<p>Docker problems typically manifest as \u201ceverything is configured correctly, but nothing can connect.\u201d</p>"},{"location":"operations/troubleshooting/#symptoms_5","title":"Symptoms","text":"<ul> <li><code>docker compose up</code> fails.</li> <li>Containers start but immediately exit.</li> <li>Ports already in use.</li> <li>Volumes/migrations appear \u201cstuck\u201d across restarts.</li> </ul>"},{"location":"operations/troubleshooting/#diagnosis-steps_5","title":"Diagnosis steps","text":"<ol> <li> <p>Check compose status:    </p><pre><code>docker compose ps\n</code></pre><p></p> </li> <li> <p>Inspect logs:    </p><pre><code>docker compose logs -f\n</code></pre><p></p> </li> <li> <p>Check for port conflicts:    </p><pre><code>lsof -i :5432\n</code></pre> <pre><code>lsof -i :7687\n</code></pre><p></p> </li> <li> <p>If state seems corrupted, inspect volumes:    </p><pre><code>docker volume ls\n</code></pre><p></p> </li> </ol>"},{"location":"operations/troubleshooting/#solutions_5","title":"Solutions","text":"<ul> <li>Port conflicts</li> <li> <p>Stop the conflicting service or change the published port in <code>docker-compose.yml</code>.</p> </li> <li> <p>Bad persisted state</p> </li> <li> <p>If you can safely reset local data:     </p><pre><code>docker compose down -v\ndocker compose up -d postgres neo4j grafana\n</code></pre><p></p> </li> <li> <p>Container exits immediately</p> </li> <li>Read the container logs; most often it\u2019s invalid env vars, permissions, or insufficient memory.</li> </ul> <p>Danger</p> <p><code>docker compose down -v</code> deletes volumes (your local Postgres/Neo4j data). Only do this if you\u2019re sure you can re-index.</p>"},{"location":"operations/troubleshooting/#related-pages","title":"Related pages","text":"<ul> <li>Configuration</li> <li>Indexing</li> <li>Search &amp; Fusion (RRF / weighted scoring)</li> <li>Graph RAG (Neo4j entities &amp; relationships)</li> <li>Type Generation (Pydantic \u2192 TypeScript)</li> </ul>"},{"location":"operations/troubleshooting/#reference-what-healthy-looks-like","title":"Reference: what \u201chealthy\u201d looks like","text":"<pre><code>flowchart LR\n  Q[Query] --&gt; V[Vector search&lt;br/&gt;Postgres + pgvector]\n  Q --&gt; S[Sparse search&lt;br/&gt;Postgres FTS (BM25-style)]\n  Q --&gt; G[Graph search&lt;br/&gt;Neo4j traversal]\n\n  V --&gt; F[Fusion&lt;br/&gt;RRF or weighted scoring]\n  S --&gt; F\n  G --&gt; F\n\n  F --&gt; R[Rerank (optional)&lt;br/&gt;cross-encoder]\n  R --&gt; A[Answer / Results]</code></pre> Click to expand <p>If you can: - index a repo without errors, - list entities via <code>/graph/{repo_id}/entities</code>, - and <code>/search</code> returns results with reranking disabled,</p> <p>then the core tri-brid pipeline is functioning. From there, tune fusion weights/RRF parameters and reranking for quality and latency.</p>"}]}