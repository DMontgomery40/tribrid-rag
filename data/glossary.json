{
  "version": "1.1.0",
  "generated_from": "web/src/modules/_archived/tooltips.js",
  "terms": [
    {
      "term": "Advanced Parameters",
      "key": "ADVANCED_RAG_TUNING",
      "definition": "Expert controls for fusion weighting, score bonuses, and iteration behavior. These significantly affect retrieval quality and performance. Change only if you understand trade-offs.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Expert",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Alert Webhook Timeout",
      "key": "ALERT_WEBHOOK_TIMEOUT",
      "definition": "Maximum seconds to wait for alert webhook response (Slack, Discord, etc.). Prevents slow webhooks from blocking the main process. Typical: 5-10 seconds.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Webhook Timeouts",
          "href": "https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Timeout"
        },
        {
          "text": "Slack Webhooks",
          "href": "https://api.slack.com/messaging/webhooks"
        }
      ],
      "badges": []
    },
    {
      "term": "Anthropic API Key",
      "key": "ANTHROPIC_API_KEY",
      "definition": "API key for Anthropic models (Claude family: claude-3-5-sonnet, claude-3-opus, claude-instant). Required when using Claude models for generation. Get your key from Anthropic Console under Account Settings > API Keys. Claude models excel at code understanding, long context (200K tokens), and following complex instructions. Costs vary by model tier.",
      "category": "generation",
      "related": [],
      "links": [
        {
          "text": "Get API Key",
          "href": "https://console.anthropic.com/settings/keys"
        },
        {
          "text": "Claude Models",
          "href": "https://docs.anthropic.com/en/docs/about-claude/models"
        },
        {
          "text": "API Quickstart",
          "href": "https://docs.anthropic.com/en/api/getting-started"
        },
        {
          "text": "Pricing",
          "href": "https://www.anthropic.com/pricing"
        }
      ],
      "badges": []
    },
    {
      "term": "AST Overlap Lines",
      "key": "AST_OVERLAP_LINES",
      "definition": "Number of overlapping lines between consecutive AST-based code chunks. Overlap ensures context continuity across chunk boundaries, preventing loss of meaning when functions or classes are split. Higher overlap (5-15 lines) improves retrieval quality by providing more context but increases index size and duplicate content. Lower overlap (0-5 lines) reduces redundancy but risks fragmenting logical units.\n\nSweet spot: 3-5 lines for balanced context preservation. Use 5-10 lines for codebases with large functions or complex nested structures where context matters heavily. Use 0-2 lines for memory-constrained environments or when chunk boundaries align well with natural code structure (e.g., clean function boundaries). AST-aware chunking (cAST method) respects syntax boundaries, so overlap supplements structural chunking.\n\nExample: With 5-line overlap, if chunk 1 ends at line 100, chunk 2 starts at line 96, creating a 5-line bridge. This helps when a query matches content near chunk boundaries - the overlapping region appears in both chunks, improving recall. The cAST paper (EMNLP 2025) shows overlap significantly improves code retrieval accuracy.\n\n• Range: 0-15 lines (typical)\n• Minimal: 0-2 lines (tight memory, clean boundaries)\n• Balanced: 3-5 lines (recommended for most codebases)\n• High context: 5-10 lines (complex nested code)\n• Very high: 10-15 lines (maximum context, high redundancy)\n• Trade-off: More overlap = better recall, larger index",
      "category": "chunking",
      "related": [],
      "links": [
        {
          "text": "cAST Chunking Paper (EMNLP 2025)",
          "href": "https://arxiv.org/abs/2506.15655"
        },
        {
          "text": "AST Chunking Toolkit",
          "href": "https://github.com/yilinjz/astchunk"
        },
        {
          "text": "Context Window in RAG",
          "href": "https://arxiv.org/abs/2312.10997"
        }
      ],
      "badges": [
        {
          "text": "Advanced chunking",
          "class": "info"
        },
        {
          "text": "Requires reindex",
          "class": "reindex"
        }
      ]
    },
    {
      "term": "Auto-Start Colima",
      "key": "AUTO_COLIMA",
      "definition": "Automatically start Colima Docker runtime if not running (macOS only, 1=yes, 0=no). Convenient for local development, ensures Docker containers start without manual intervention.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Colima",
          "href": "https://github.com/abiosoft/colima"
        }
      ],
      "badges": []
    },
    {
      "term": "Baseline Path",
      "key": "BASELINE_PATH",
      "definition": "Directory where evaluation loop saves baseline results for regression testing and A/B comparison. Each eval run's metrics (Hit@K, MRR, latency) are stored here with timestamps. Use this to ensure retrieval quality doesn't regress after configuration changes, reindexing, or model upgrades. Compare current run against baseline to detect improvements or degradations.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Regression Prevention",
          "href": "https://en.wikipedia.org/wiki/Software_regression"
        }
      ],
      "badges": []
    },
    {
      "term": "BM25 b (Length Normalization)",
      "key": "BM25_B",
      "definition": "BM25 length-normalization parameter. Controls how strongly sparse (keyword) scoring penalizes long chunks compared to short chunks.\n\nb = 0.0 means no length penalty (a long chunk can score highly simply because it contains more terms). b = 1.0 means full length normalization (long chunks are penalized relative to the average chunk length). For code corpora, moderate values typically work best because chunk lengths are already partially normalized by chunking.\n\nTune b when sparse results feel “too long” or “too short”: if long boilerplate chunks dominate, increase b; if large files should remain competitive, decrease b.\n\n• Range: 0.0–1.0\n• Code sweet spot: 0.3–0.5 (recommended)\n• Lower b: favors longer chunks (higher recall, more noise)\n• Higher b: favors shorter chunks (higher precision, may miss context)\n• Interacts with: BM25_K1 and chunking (CHUNK_SIZE / CHUNK_OVERLAP)",
      "category": "retrieval",
      "related": [
        "BM25 k1 (Term Saturation)",
        "BM25 Tokenizer",
        "BM25 Weight (Hybrid Fusion)",
        "Chunk Size"
      ],
      "links": [
        {
          "text": "Okapi BM25",
          "href": "https://en.wikipedia.org/wiki/Okapi_BM25"
        }
      ],
      "badges": [
        {
          "text": "Advanced RAG tuning",
          "class": "info"
        },
        {
          "text": "Affects keyword search",
          "class": "info"
        }
      ]
    },
    {
      "term": "BM25 k1 (Term Saturation)",
      "key": "BM25_K1",
      "definition": "BM25 term-frequency saturation parameter. Controls how much repeated occurrences of a query term within the same chunk increase the sparse score.\n\nLow k1 makes BM25 behave closer to “binary” matching (term present vs. absent). High k1 keeps rewarding repeats, which can overweight boilerplate or very repetitive identifiers.\n\nFor code search, moderate k1 (around 1.0–1.5) usually works well: it rewards chunks that are clearly about the query term without letting repetition dominate.\n\n• Range: 0.5–3.0\n• Typical: 1.2 (default)\n• Lower k1: repeats matter less (more binary)\n• Higher k1: repeats matter more (can favor verbose/repetitive chunks)\n• Interacts with: BM25_B (length normalization)",
      "category": "retrieval",
      "related": [
        "BM25 b (Length Normalization)",
        "BM25 Tokenizer",
        "BM25 Weight (Hybrid Fusion)"
      ],
      "links": [
        {
          "text": "Okapi BM25",
          "href": "https://en.wikipedia.org/wiki/Okapi_BM25"
        },
        {
          "text": "Term Frequency",
          "href": "https://en.wikipedia.org/wiki/Term_frequency"
        }
      ],
      "badges": [
        {
          "text": "Advanced RAG tuning",
          "class": "info"
        },
        {
          "text": "Affects keyword search",
          "class": "info"
        }
      ]
    },
    {
      "term": "BM25 Stemmer Language",
      "key": "BM25_STEMMER_LANG",
      "definition": "Language for stemming/normalization in BM25 sparse indexing. Common values: \"en\" (English - default), \"multilingual\" (multiple languages), \"none\" (disable stemming). Stemming reduces words to root forms (e.g., \"running\" -> \"run\") to improve keyword matching. English stemming works well for code comments, docs, and variable names. Use \"none\" for non-English repos or when exact keyword matching is critical (e.g., API names, error codes).\n\nRecommended: \"en\" for English codebases, \"multilingual\" for international teams, \"none\" for strict keyword matching.",
      "category": "retrieval",
      "related": [],
      "links": [
        {
          "text": "BM25 Algorithm",
          "href": "https://en.wikipedia.org/wiki/Okapi_BM25"
        },
        {
          "text": "Stemming Explained",
          "href": "https://en.wikipedia.org/wiki/Stemming"
        },
        {
          "text": "BM25S Library",
          "href": "https://github.com/xhluca/bm25s#supported-stemmers"
        }
      ],
      "badges": [
        {
          "text": "Affects keyword search",
          "class": "info"
        }
      ]
    },
    {
      "term": "BM25 Stopwords Language",
      "key": "BM25_STOPWORDS_LANG",
      "definition": "Language code used to select the stopword list for sparse (BM25/FTS) tokenization. Stopwords are extremely common words that are ignored to reduce noise (e.g., “the”, “and”, “of”).\n\nFor code-heavy corpora, stopword filtering is a trade-off: it can clean up comments/README text, but it can also remove short tokens that sometimes matter. If your corpus contains multiple human languages, pick the dominant language, or use a minimal/no-stopword configuration if you care more about exact identifier matching.\n\nChanging stopwords changes the sparse vocabulary, so it only takes effect after rebuilding the sparse/FTS index.\n\n• Examples: en, es, fr (language codes)\n• Effect: controls which common words are ignored during sparse search\n• Symptom too aggressive: useful terms disappear from vocab preview\n• Symptom too permissive: huge vocab + noisy matches\n• Requires reindex: yes",
      "category": "retrieval",
      "related": [
        "BM25 Tokenizer",
        "BM25 Stemmer Language",
        "BM25 Vocabulary Preview"
      ],
      "links": [
        {
          "text": "Stop Words",
          "href": "https://en.wikipedia.org/wiki/Stop_word"
        },
        {
          "text": "PostgreSQL Full Text Search Configurations",
          "href": "https://www.postgresql.org/docs/current/textsearch-configurations.html"
        }
      ],
      "badges": [
        {
          "text": "Advanced indexing",
          "class": "info"
        },
        {
          "text": "Requires reindex",
          "class": "reindex"
        }
      ]
    },
    {
      "term": "BM25 Tokenizer",
      "key": "BM25_TOKENIZER",
      "definition": "Tokenization strategy for BM25 sparse index. Controls how code text is split into searchable terms. Options: \"stemmer\" (Porter stemming, normalizes word forms like \"running\" → \"run\"), \"whitespace\" (split on spaces only, preserves exact forms), \"standard\" (lowercase + split on punctuation). For code search, preserving exact forms is usually better than stemming.\n\nSweet spot: \"whitespace\" or \"standard\" for code search. Stemming helps with natural language (README files, comments) but can hurt code search by conflating different identifiers. For example, stemming might merge \"user\" and \"users\" (good for prose) but also \"handler\" and \"handle\" (bad for code). Most code-focused RAG systems avoid stemming.\n\n\"whitespace\": Splits on whitespace only, preserves case and punctuation. Good for camelCase and snake_case. Example: \"getUserData\" → [\"getUserData\"].\n\n\"standard\": Lowercase + split on punctuation. Better for cross-case matching. Example: \"getUserData\" → [\"getuserdata\"] (matches \"getuserdata\", \"getUserData\", \"GETUSERDATA\").\n\n\"stemmer\": Applies Porter stemmer. Best for natural language, risky for code. Example: \"getUserData\" → stems individual tokens.\n\n• whitespace: Preserve exact forms, case-sensitive, best for strict code search\n• standard: Lowercase + punctuation split, case-insensitive, balanced (recommended)\n• stemmer: Normalize word forms, best for natural language, risky for code\n• Effect: Changes how BM25 matches query terms to code\n• Requires reindex: Changes take effect after rebuilding BM25 index",
      "category": "retrieval",
      "related": [],
      "links": [
        {
          "text": "BM25 Algorithm",
          "href": "https://en.wikipedia.org/wiki/Okapi_BM25"
        },
        {
          "text": "Porter Stemmer",
          "href": "https://en.wikipedia.org/wiki/Stemming#Porter_stemmer"
        },
        {
          "text": "Tokenization",
          "href": "https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization"
        },
        {
          "text": "BM25S Tokenizers",
          "href": "https://github.com/xhluca/bm25s#tokenization"
        }
      ],
      "badges": [
        {
          "text": "Advanced indexing",
          "class": "info"
        },
        {
          "text": "Requires reindex",
          "class": "reindex"
        }
      ]
    },
    {
      "term": "Resolved Tokenizer",
      "key": "BM25_TOKENIZER_RESOLVED",
      "definition": "The actual tokenization settings that will be applied during indexing. Shows the combined effect of tokenizer type, stemmer language, and stopwords language. This is the effective configuration after all settings are resolved.",
      "category": "retrieval",
      "related": [],
      "links": [
        {
          "text": "BM25 Algorithm",
          "href": "https://en.wikipedia.org/wiki/Okapi_BM25"
        },
        {
          "text": "Tokenization",
          "href": "https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization"
        }
      ],
      "badges": [
        {
          "text": "Read-only",
          "class": "info"
        }
      ]
    },
    {
      "term": "BM25 Vocabulary Preview",
      "key": "BM25_VOCAB_PREVIEW",
      "definition": "Inspect tokenized vocabulary from BM25 sparse index. Shows term frequencies for debugging. Use cases: verify code identifiers preserved, check stemmer behavior, identify noise terms, debug zero-result queries. Vocabulary reflects tokenizer: whitespace (exact, best for code), stemmer (normalized, best for prose), standard (balanced). Large vocabularies (>100K) indicate insufficient stopword filtering.",
      "category": "retrieval",
      "related": [],
      "links": [
        {
          "text": "BM25S: Eager Sparse Scoring (arXiv 2024)",
          "href": "https://arxiv.org/abs/2407.03618"
        },
        {
          "text": "BMX: Entropy-weighted BM25 Extension",
          "href": "https://arxiv.org/abs/2408.06643"
        },
        {
          "text": "Tokenization Foundations (ICLR 2025)",
          "href": "https://arxiv.org/abs/2407.11606"
        },
        {
          "text": "BM25 Algorithm",
          "href": "https://en.wikipedia.org/wiki/Okapi_BM25"
        },
        {
          "text": "Text Tokenization",
          "href": "https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization"
        }
      ],
      "badges": [
        {
          "text": "DEBUGGING",
          "class": "info"
        },
        {
          "text": "REINDEX TO UPDATE",
          "class": "warn"
        }
      ]
    },
    {
      "term": "BM25 Weight (Hybrid Fusion)",
      "key": "BM25_WEIGHT",
      "definition": "Weight assigned to BM25 (sparse lexical) scores during hybrid search fusion. BM25 excels at exact keyword matches - variable names, function names, error codes, technical terms. Higher weights (0.5-0.7) prioritize keyword precision, favoring exact matches over semantic similarity. Lower weights (0.2-0.4) defer to dense embeddings, better for conceptual queries. The fusion formula is: final_score = (BM25_WEIGHT × bm25_score) + (VECTOR_WEIGHT × dense_score).\n\nSweet spot: 0.4-0.5 for balanced hybrid retrieval. Use 0.5-0.6 when users search with specific identifiers (e.g., \"getUserById function\" or \"AuthenticationError exception\"). Use 0.3-0.4 for natural language queries (e.g., \"how does authentication work?\"). The two weights should sum to approximately 1.0 for normalized scoring, though this isn't strictly enforced.\n\nSymptom of too high: Semantic matches are buried under keyword matches. Symptom of too low: Exact identifier matches rank poorly despite containing query terms. Production systems often A/B test 0.4 vs 0.5 to optimize for their user query patterns. Code search typically needs higher BM25 weight than document search.\n\n• Range: 0.2-0.7 (typical)\n• Keyword-heavy: 0.5-0.6 (function names, error codes)\n• Balanced: 0.4-0.5 (recommended for mixed queries)\n• Semantic-heavy: 0.3-0.4 (conceptual questions)\n• Should sum with VECTOR_WEIGHT to ~1.0\n• Affects: Hybrid fusion ranking, keyword vs semantic balance",
      "category": "retrieval",
      "related": [],
      "links": [
        {
          "text": "BM25 Algorithm",
          "href": "https://en.wikipedia.org/wiki/Okapi_BM25"
        },
        {
          "text": "Hybrid Search Overview",
          "href": "https://qdrant.tech/articles/hybrid-search/"
        },
        {
          "text": "Fusion Strategies in RAG",
          "href": "https://arxiv.org/abs/2402.14734"
        },
        {
          "text": "Sparse vs Dense Retrieval",
          "href": "https://www.pinecone.io/learn/hybrid-search-intro/"
        }
      ],
      "badges": [
        {
          "text": "Advanced RAG tuning",
          "class": "info"
        },
        {
          "text": "Pairs with VECTOR_WEIGHT",
          "class": "info"
        }
      ]
    },
    {
      "term": "Cards Max",
      "key": "CARDS_MAX",
      "definition": "Maximum number of summary cards to load and consider during retrieval for score boosting. Cards are high-level summaries of code modules/features. Lower values (10-20) are faster but may miss relevant modules. Higher values (30-50) provide better coverage but increase memory and latency. Set to 0 to disable cards entirely. Recommended: 20-30 for balanced performance.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Affects memory",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Card Semantic Bonus",
      "key": "CARD_BONUS",
      "definition": "Score bonus when a result matches code \"Cards\" (semantic summaries from enrichment). Improves intent‑based retrieval (e.g., \"where is auth handled?\"). Requires ENRICH_CODE_CHUNKS.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Improves intent",
          "class": "info"
        }
      ]
    },
    {
      "term": "Card Search Enabled",
      "key": "CARD_SEARCH_ENABLED",
      "definition": "Enable card-based boosting during retrieval to surface relevant code modules and features. When enabled, the system loads summary cards (high-level descriptions of modules/classes/features) and boosts results that match card content. This improves retrieval for conceptual queries like \"where is payment processing?\" at the cost of slightly increased memory and query latency. Requires ENRICH_CODE_CHUNKS=1 and cards to be built during indexing.\n\nRecommended: 1 (enabled) for production with enrichment, 0 (disabled) for development or when enrichment is off.",
      "category": "retrieval",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Better conceptual search",
          "class": "info"
        },
        {
          "text": "Requires enrichment",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Auto‑Scroll to New Messages",
      "key": "CHAT_AUTO_SCROLL",
      "definition": "Automatically scrolls the conversation to the newest message. Disable when reviewing earlier context while messages stream.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "ARIA Live Regions (UX)",
          "href": "https://developer.mozilla.org/en-US/docs/Web/Accessibility/ARIA/ARIA_Live_Regions"
        }
      ],
      "badges": []
    },
    {
      "term": "Retrieval Confidence",
      "key": "CHAT_CONFIDENCE",
      "definition": "Show a normalized confidence score (0–1) alongside answers to help judge reliability. Scores reflect retrieval confidence, not model certainty.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Precision vs Recall",
          "href": "https://en.wikipedia.org/wiki/Precision_and_recall"
        }
      ],
      "badges": []
    },
    {
      "term": "Answer Confidence Threshold",
      "key": "CHAT_CONFIDENCE_THRESHOLD",
      "definition": "Minimum retrieval confidence to return an answer without fallback. Lower values return more answers (risking guesses); higher values are conservative.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Precision vs Recall",
          "href": "https://en.wikipedia.org/wiki/Precision_and_recall"
        }
      ],
      "badges": []
    },
    {
      "term": "Default Chat Model",
      "key": "CHAT_DEFAULT_MODEL",
      "definition": "Default LLM model used for chat when not overridden per-request. Common options: gpt-4o-mini (fast/cheap), gpt-4o (balanced), claude-3-5-sonnet (high quality), or local Ollama models. Per-request model overrides take precedence.",
      "category": "generation",
      "related": [],
      "links": [
        {
          "text": "OpenAI Models",
          "href": "https://platform.openai.com/docs/models"
        },
        {
          "text": "Anthropic Models",
          "href": "https://docs.anthropic.com/en/docs/about-claude/models"
        }
      ],
      "badges": []
    },
    {
      "term": "Chat History Storage",
      "key": "CHAT_HISTORY",
      "definition": "Controls how chat history is saved and loaded. History persists in browser localStorage only — no server storage for privacy.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "localStorage",
          "href": "https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage"
        }
      ],
      "badges": [
        {
          "text": "Browser storage",
          "class": "info"
        }
      ]
    },
    {
      "term": "Save Chat Messages",
      "key": "CHAT_HISTORY_ENABLED",
      "definition": "When enabled, messages are persisted to browser localStorage and restored on reload. Disable for ephemeral sessions or shared devices.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "localStorage",
          "href": "https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage"
        }
      ],
      "badges": []
    },
    {
      "term": "History Limit",
      "key": "CHAT_HISTORY_LIMIT",
      "definition": "Maximum number of messages to retain in local history. Older messages are pruned when the limit is reached. Typical range: 50–1000.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Usability: History & Recall",
          "href": "https://www.nngroup.com/articles/search-logs/"
        }
      ],
      "badges": []
    },
    {
      "term": "Load History on Startup",
      "key": "CHAT_HISTORY_LOAD_ON_START",
      "definition": "Automatically loads and displays previous conversations when opening the Chat tab. Disable to start with a clean slate every session.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "localStorage",
          "href": "https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage"
        }
      ],
      "badges": []
    },
    {
      "term": "Max Response Tokens (Chat)",
      "key": "CHAT_MAX_TOKENS",
      "definition": "Upper bound on generated tokens for chat answers. ~4 chars ≈ 1 token. Higher values cost more and may slow responses.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Tokenization Basics",
          "href": "https://huggingface.co/docs/transformers/main_classes/tokenizer"
        }
      ],
      "badges": []
    },
    {
      "term": "Chat Configuration",
      "key": "CHAT_SETTINGS",
      "definition": "Settings that control model, answer length, rewrite strategy, and retrieval size for the chat interface. These affect latency, cost, and answer quality.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Affects quality",
          "class": "info"
        },
        {
          "text": "Affects latency",
          "class": "info"
        }
      ]
    },
    {
      "term": "Inline File References",
      "key": "CHAT_SHOW_CITATIONS",
      "definition": "Display source file paths and line numbers inline with the answer. Citations become clickable links to code locations.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Chat Streaming",
      "key": "CHAT_STREAMING_ENABLED",
      "definition": "Enable streaming responses for chat interfaces. When on, tokens appear incrementally (like typing). Better UX but requires SSE support. Disable for simple request-response APIs.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Server-Sent Events",
          "href": "https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events"
        },
        {
          "text": "Streaming API",
          "href": "https://platform.openai.com/docs/api-reference/streaming"
        }
      ],
      "badges": []
    },
    {
      "term": "Include Thinking in Stream",
      "key": "CHAT_STREAM_INCLUDE_THINKING",
      "definition": "When enabled and using a thinking/reasoning model (like Anthropic Claude with extended thinking or OpenAI o-series), the model's reasoning process will be streamed to the UI before the final answer. This provides transparency into how the model arrived at its conclusion but increases response length. Disable if you only want final answers without reasoning traces.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Anthropic Extended Thinking",
          "href": "https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking"
        },
        {
          "text": "OpenAI Reasoning Models",
          "href": "https://platform.openai.com/docs/guides/reasoning"
        }
      ],
      "badges": [
        {
          "text": "Advanced",
          "class": "info"
        }
      ]
    },
    {
      "term": "Stream Timeout (seconds)",
      "key": "CHAT_STREAM_TIMEOUT",
      "definition": "Maximum time in seconds to wait for a streaming chat response to complete. If the stream doesn't finish within this time, the connection will be closed. Increase for complex queries that require longer generation times. Default: 120 seconds (2 minutes). Range: 30-600 seconds.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "HTTP Timeouts",
          "href": "https://developer.mozilla.org/en-US/docs/Web/API/fetch#options"
        }
      ],
      "badges": [
        {
          "text": "Affects reliability",
          "class": "info"
        }
      ]
    },
    {
      "term": "Code Block Highlighting",
      "key": "CHAT_SYNTAX_HIGHLIGHT",
      "definition": "Apply syntax highlighting to code blocks in responses. Improves readability in multi‑language projects. May increase render time on very long messages.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Prism.js",
          "href": "https://prismjs.com/"
        }
      ],
      "badges": [
        {
          "text": "UX",
          "class": "info"
        }
      ]
    },
    {
      "term": "Custom System Prompt",
      "key": "CHAT_SYSTEM_PROMPT",
      "definition": "Override the default expert system prompt for Chat. Use to adjust tone, safety constraints, or provide domain instructions. Leave empty to use the built‑in TriBridRAG expert prompt.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Prompt Engineering (Guide)",
          "href": "https://platform.openai.com/docs/guides/prompt-engineering"
        }
      ],
      "badges": []
    },
    {
      "term": "Response Creativity (Chat)",
      "key": "CHAT_TEMPERATURE",
      "definition": "Controls randomness for chat answers. For code Q&A, prefer 0.0–0.3; for ideation, increase to 0.5–0.9.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Sampling Controls",
          "href": "https://platform.openai.com/docs/guides/text-generation"
        }
      ],
      "badges": []
    },
    {
      "term": "Thinking Budget Tokens",
      "key": "CHAT_THINKING_BUDGET_TOKENS",
      "definition": "Maximum number of tokens allocated for the model's internal reasoning/thinking process when using thinking-enabled models like Anthropic Claude with extended thinking. Higher budgets allow deeper reasoning but increase latency and cost. Only applies when using models that support extended thinking. Default: 10,000 tokens. Range: 1,000-100,000.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Anthropic Thinking Budget",
          "href": "https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#budget-tokens"
        }
      ],
      "badges": [
        {
          "text": "Cost",
          "class": "warning"
        }
      ]
    },
    {
      "term": "Chunking Strategy",
      "key": "CHUNKING_STRATEGY",
      "definition": "Primary strategy for splitting code into chunks during indexing. Options: \"ast\" (AST-aware, syntax-respecting, recommended for code), \"greedy\" (line-based splitting, simpler), \"hybrid\" (AST with greedy fallback). AST chunking uses the cAST method (EMNLP 2025) to respect function/class boundaries, preserving semantic units. Greedy chunking splits at line breaks to hit target size, ignoring syntax. Hybrid uses AST primarily with greedy fallback for unparseable files.\n\n\"ast\" (recommended for code): Parses syntax tree and chunks at natural boundaries (functions, classes, methods). Produces semantically coherent chunks. Best for code retrieval. Requires parseable syntax - fails gracefully on malformed code.\n\n\"greedy\": Simple line-based splitting at target character count. Fast, always works, but may split mid-function or mid-class, fragmenting semantic units. Use for non-code (markdown, text) or when AST parsing is too slow.\n\n\"hybrid\": Tries AST first, falls back to greedy on parse errors. Balanced approach - gets AST benefits for well-formed code, handles edge cases gracefully. Recommended for mixed codebases (code + docs + config).\n\n• ast: Syntax-aware, best retrieval quality, code-only, requires parseable syntax (recommended for code)\n• greedy: Fast, always works, ignores syntax, lower quality chunks, good for non-code\n• hybrid: AST + greedy fallback, balanced, handles all files (recommended for mixed repos)\n• Effect: Fundamental impact on chunk quality, retrieval precision, index structure\n• Requires reindex: Changes take effect after full rebuild",
      "category": "chunking",
      "related": [],
      "links": [
        {
          "text": "cAST Chunking Paper (EMNLP 2025)",
          "href": "https://arxiv.org/abs/2506.15655"
        },
        {
          "text": "AST Chunking Toolkit",
          "href": "https://github.com/yilinjz/astchunk"
        },
        {
          "text": "RAG Chunking Best Practices",
          "href": "https://weaviate.io/blog/chunking-strategies-for-rag"
        }
      ],
      "badges": [
        {
          "text": "Core indexing choice",
          "class": "warn"
        },
        {
          "text": "Requires reindex",
          "class": "reindex"
        }
      ]
    },
    {
      "term": "Chunk Overlap",
      "key": "CHUNK_OVERLAP",
      "definition": "Number of characters overlapped between adjacent chunks. Overlap reduces boundary effects and improves recall at the cost of a larger index and slower indexing.",
      "category": "chunking",
      "related": [],
      "links": [
        {
          "text": "LangChain: Text Splitters",
          "href": "https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/"
        }
      ],
      "badges": []
    },
    {
      "term": "Chunk Size",
      "key": "CHUNK_SIZE",
      "definition": "Target size (in characters) for each indexed chunk. For AST chunking this acts as a guardrail when nodes are large. Larger chunks preserve more context but reduce recall; smaller chunks improve recall but may fragment semantics.",
      "category": "chunking",
      "related": [],
      "links": [
        {
          "text": "LangChain: Text Splitters",
          "href": "https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/"
        },
        {
          "text": "Okapi BM25 (context windows)",
          "href": "https://en.wikipedia.org/wiki/Okapi_BM25"
        }
      ],
      "badges": [
        {
          "text": "Affects recall/precision",
          "class": "info"
        }
      ]
    },
    {
      "term": "Chunk Summaries Enrich Default",
      "key": "CHUNK_SUMMARIES_ENRICH_DEFAULT",
      "definition": "Enable chunk summary enrichment by default when building summaries. When enabled, summaries include enriched metadata (detailed purpose, technical details, domain concepts) using LLM analysis. When disabled, summaries use lightweight extraction only. Enrichment improves quality but increases indexing time and cost.\n\nSweet spot: enabled for production systems. Enriched summaries provide better retrieval quality and more detailed metadata. Disable only if indexing speed or cost is a concern, or if lightweight summaries are sufficient.\n\n• Enabled: Full enrichment with LLM analysis (recommended)\n• Disabled: Lightweight extraction only (faster, lower cost)\n• Effect: Controls whether summaries are enriched with detailed metadata\n• Symptom if disabled: Less detailed summaries, potentially lower retrieval quality",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Code Enrichment",
          "href": "https://cookbook.openai.com/examples/summarizing_long_documents"
        },
        {
          "text": "Chunk Summarization",
          "href": "https://vectify.ai/blog/LargeDocumentSummarization"
        }
      ],
      "badges": [
        {
          "text": "Enrichment",
          "class": "info"
        }
      ]
    },
    {
      "term": "Exclude Directories",
      "key": "CHUNK_SUMMARIES_EXCLUDE_DIRS",
      "definition": "List of directory paths to skip when building chunk summaries. Directories matching these paths (exact or prefix) are excluded from summarization. Useful for excluding test files, documentation, generated code, or third-party dependencies. Default includes: docs, tests, node_modules, venv, dist, etc.\n\nSweet spot: Default list for most projects. Add project-specific directories (e.g., \"legacy\", \"deprecated\") to improve summary quality. Remove defaults only if you want to include tests or docs in summaries.\n\n• Format: Comma-separated or newline-separated directory paths\n• Default: docs, tests, node_modules, venv, dist, etc.\n• Effect: Filters out directories from summarization\n• Symptom if too permissive: Low-quality summaries from test files or dependencies",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Code Filtering",
          "href": "https://en.wikipedia.org/wiki/Code_filtering"
        },
        {
          "text": "File Exclusion Patterns",
          "href": "https://en.wikipedia.org/wiki/Glob_(programming)"
        }
      ],
      "badges": [
        {
          "text": "Filtering",
          "class": "info"
        }
      ]
    },
    {
      "term": "Exclude Keywords",
      "key": "CHUNK_SUMMARIES_EXCLUDE_KEYWORDS",
      "definition": "List of keywords that, when present in code, cause the chunk to be skipped during summarization. Useful for excluding deprecated code, TODO comments, legacy implementations, or experimental features. Case-insensitive matching.\n\nSweet spot: Add project-specific keywords (e.g., \"deprecated\", \"legacy\", \"TODO\", \"FIXME\") to improve summary quality. Leave empty if you want all code summarized regardless of keywords.\n\n• Format: Comma-separated or newline-separated keywords\n• Examples: deprecated, legacy, TODO, FIXME, experimental\n• Effect: Filters out chunks containing keywords from summarization\n• Symptom if too permissive: Low-quality summaries from deprecated or incomplete code",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Code Filtering",
          "href": "https://en.wikipedia.org/wiki/Code_filtering"
        },
        {
          "text": "Keyword Extraction",
          "href": "https://en.wikipedia.org/wiki/Keyword_extraction"
        }
      ],
      "badges": [
        {
          "text": "Filtering",
          "class": "info"
        }
      ]
    },
    {
      "term": "Exclude Patterns",
      "key": "CHUNK_SUMMARIES_EXCLUDE_PATTERNS",
      "definition": "List of glob patterns (e.g., \"*.min.js\", \"*.lock\", \"**/*.test.ts\") to skip when building chunk summaries. Files matching these patterns are excluded from summarization. Useful for excluding generated files, lock files, minified code, or test files. More flexible than directory exclusion.\n\nSweet spot: Add patterns for generated or minified files (e.g., \"*.min.js\", \"*.bundle.js\"). Include test file patterns if you don't want tests summarized (e.g., \"**/*.test.ts\", \"**/*.spec.js\"). Leave empty if you want all files summarized.\n\n• Format: Comma-separated or newline-separated glob patterns\n• Examples: *.min.js, *.lock, **/*.test.ts, dist/**\n• Effect: Filters out files matching patterns from summarization\n• Symptom if too permissive: Low-quality summaries from generated or test files",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Glob Patterns",
          "href": "https://en.wikipedia.org/wiki/Glob_(programming)"
        },
        {
          "text": "File Filtering",
          "href": "https://en.wikipedia.org/wiki/File_system"
        }
      ],
      "badges": [
        {
          "text": "Filtering",
          "class": "info"
        }
      ]
    },
    {
      "term": "Max Chunk Summaries",
      "key": "CHUNK_SUMMARIES_MAX",
      "definition": "Maximum number of chunk summaries to generate per corpus. Chunk summaries provide structured metadata (purpose, symbols, keywords) for each code chunk, improving retrieval quality. Higher values (200-500) provide more comprehensive coverage but increase indexing time and storage. Lower values (50-100) are faster but may miss important chunks.\n\nSweet spot: 100 for balanced coverage. Use 50-75 for large codebases where indexing speed matters. Use 200-300 when comprehensive coverage is critical. Use 500+ only for small, critical codebases.\n\n• Range: 10-1000 (typical: 50-300)\n• Fast indexing: 50-75 (lower coverage)\n• Balanced: 100 (recommended)\n• Comprehensive: 200-300 (higher coverage)\n• Effect: Higher = more summaries, better coverage, longer indexing\n• Symptom too low: Important chunks missing summaries\n• Symptom too high: Slow indexing, storage overhead",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Chunk Summarization",
          "href": "https://vectify.ai/blog/LargeDocumentSummarization"
        },
        {
          "text": "Code Analysis",
          "href": "https://llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"
        },
        {
          "text": "Document Summarization",
          "href": "https://cookbook.openai.com/examples/summarizing_long_documents"
        }
      ],
      "badges": [
        {
          "text": "Indexing",
          "class": "info"
        }
      ]
    },
    {
      "term": "Chunk Summary Bonus",
      "key": "CHUNK_SUMMARY_BONUS",
      "definition": "Additive score bonus applied to results that come from chunk_summary-based retrieval. Chunk summaries are short, structured descriptions of code chunks (purpose, key symbols, keywords) and can match conceptual queries better than raw code.\n\nThis bonus helps chunk_summary hits compete with dense and sparse matches in fusion/reranking. Increase it if summary-based hits are good but rank too low; decrease it if they crowd out precise code hits.\n\n• Range: 0.0–1.0 (typical: 0.03–0.15)\n• Default: 0.08\n• Higher: stronger intent routing via summaries (risk: generic matches)\n• Lower: rely more on raw chunk text (risk: miss conceptual queries)\n• Interacts with: CHUNK_SUMMARY_SEARCH_ENABLED",
      "category": "retrieval",
      "related": [
        "Chunk Summary Search",
        "Max Chunk Summaries",
        "Chunk Summaries Enrich Default"
      ],
      "links": [
        {
          "text": "Document Summarization",
          "href": "https://en.wikipedia.org/wiki/Automatic_summarization"
        }
      ],
      "badges": [
        {
          "text": "Advanced RAG tuning",
          "class": "info"
        }
      ]
    },
    {
      "term": "Chunk Summary Search",
      "key": "CHUNK_SUMMARY_SEARCH_ENABLED",
      "definition": "Enable an additional retrieval pass that searches over each chunk’s chunk_summary (LLM-generated metadata such as purpose, key symbols, and keywords) instead of only raw chunk text. This can improve recall for conceptual questions where the exact identifier isn’t in the query.\n\nIf a corpus hasn’t generated chunk summaries yet, enabling this won’t add signal until you (re)index with chunk summaries enabled.\n\n• Disabled: only raw code/doc text participates in retrieval\n• Enabled: chunk summaries can produce candidate hits (often better for “what does this do?” queries)\n• Cost/latency: can add an extra retrieval step (usually small compared to LLM calls)\n• Interacts with: CHUNK_SUMMARY_BONUS and chunk summary indexing limits",
      "category": "retrieval",
      "related": [
        "Chunk Summary Bonus",
        "Max Chunk Summaries",
        "Chunk Summaries Enrich Default"
      ],
      "links": [
        {
          "text": "Automatic Summarization",
          "href": "https://en.wikipedia.org/wiki/Automatic_summarization"
        }
      ],
      "badges": [
        {
          "text": "Improves intent",
          "class": "info"
        }
      ]
    },
    {
      "term": "Code Cards",
      "key": "CODE_CARDS",
      "definition": "High‑level semantic summaries of code chunks, built during enrichment. Cards enable intent‑based retrieval and better filtering for conceptual queries.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Improves intent",
          "class": "info"
        }
      ]
    },
    {
      "term": "Cohere API Key",
      "key": "COHERE_API_KEY",
      "definition": "API key for Cohere reranking when RERANK_BACKEND=cohere.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Cohere Dashboard: API Keys",
          "href": "https://dashboard.cohere.com/api-keys"
        }
      ],
      "badges": []
    },
    {
      "term": "Cohere Rerank Calls (calls/min)",
      "key": "COHERE_RERANK_CALLS",
      "definition": "Alert when Cohere reranking API is called this many times per minute. Reranking is expensive ($1-2 per 1M tokens) and high call rates can quickly increase costs. Normal usage: 5-10 calls/min. If this spikes to 50+, check for loops or unnecessary reranking. Consider caching rerank results or using local reranker instead.",
      "category": "reranking",
      "related": [],
      "links": [
        {
          "text": "Cohere Pricing",
          "href": "https://cohere.com/pricing"
        }
      ],
      "badges": [
        {
          "text": "Cost Control",
          "class": "warn"
        },
        {
          "text": "API Usage",
          "class": "info"
        }
      ]
    },
    {
      "term": "Cohere Rerank Model",
      "key": "COHERE_RERANK_MODEL",
      "definition": "Cohere rerank model name (e.g., rerank-3.5). Check the provider docs for the latest list and pricing.",
      "category": "reranking",
      "related": [],
      "links": [
        {
          "text": "Cohere Docs: Models",
          "href": "https://docs.cohere.com/docs/models"
        }
      ],
      "badges": []
    },
    {
      "term": "Colima Profile",
      "key": "COLIMA_PROFILE",
      "definition": "Colima profile name to use when AUTO_COLIMA is enabled. Profiles allow different Docker VM configurations (CPU, memory, disk). Default profile used if empty.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Colima Profiles",
          "href": "https://github.com/abiosoft/colima#profile"
        }
      ],
      "badges": []
    },
    {
      "term": "Collection Name",
      "key": "COLLECTION_NAME",
      "definition": "Optional override for the Qdrant collection name where vectors are stored. Defaults to code_chunks_{REPO}. Set this if you maintain multiple profiles, A/B test embedding models, or run parallel indexing. Must be lowercase alphanumeric + underscore. Examples: code_chunks_v2, vectors_staging, embeddings_prod",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Qdrant Collections Intro",
          "href": "https://qdrant.tech/documentation/concepts/collections/"
        },
        {
          "text": "Create Collections",
          "href": "https://qdrant.tech/documentation/concepts/collections/#create-collection"
        },
        {
          "text": "Database Collections",
          "href": "https://en.wikipedia.org/wiki/Database_collection"
        }
      ],
      "badges": []
    },
    {
      "term": "Collection Suffix",
      "key": "COLLECTION_SUFFIX",
      "definition": "Optional string appended to the default collection name (code_chunks_{REPO}) for A/B testing different indexing strategies. For example, suffix \"_v2\" creates \"code_chunks_myrepo_v2\". Useful when comparing embedding models, chunking strategies, or reranking approaches without overwriting your production index. Leave empty for default collection.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Qdrant Collections",
          "href": "https://qdrant.tech/documentation/concepts/collections/"
        },
        {
          "text": "Collection Management",
          "href": "https://qdrant.tech/documentation/concepts/collections/#create-collection"
        },
        {
          "text": "Collection Naming",
          "href": "https://qdrant.tech/documentation/concepts/collections/#collection-name"
        }
      ],
      "badges": [
        {
          "text": "Experimental",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Confidence Any",
      "key": "CONF_ANY",
      "definition": "Fallback threshold - proceed with retrieval if ANY single result exceeds this score, even if top-1 or avg-5 thresholds aren't met. This prevents the system from giving up when there's at least one decent match. Lower values (0.30-0.40) are more permissive, returning results even with weak confidence. Higher values (0.45-0.50) maintain quality standards. Recommended: 0.35-0.45 as a safety net.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Fallback Strategies",
          "href": "https://en.wikipedia.org/wiki/Fault_tolerance"
        },
        {
          "text": "Decision Boundaries",
          "href": "https://en.wikipedia.org/wiki/Decision_boundary"
        }
      ],
      "badges": [
        {
          "text": "Safety net",
          "class": "info"
        }
      ]
    },
    {
      "term": "Confidence Avg-5",
      "key": "CONF_AVG5",
      "definition": "Average confidence score of the top-5 results, used as a gate for query rewriting iterations. If avg(top-5) is below this threshold, the system may rewrite the query and try again. Lower values (0.50-0.53) reduce retries, accepting more borderline results. Higher values (0.56-0.60) force more rewrites for higher quality. Recommended: 0.52-0.58 for balanced behavior.\n\nSweet spot: 0.52-0.55 for production systems. Use 0.55-0.58 when quality is paramount and you have budget for extra LLM calls (query rewriting). Use 0.50-0.52 for cost-sensitive scenarios or when initial retrieval is already high-quality. This threshold examines the top-5 results as a group - even if top-1 is strong, weak supporting results might trigger a rewrite.\n\nAVG5 complements TOP1: TOP1 checks the best result, AVG5 checks overall result quality. A query might pass TOP1 (strong top result) but fail AVG5 (weak supporting results), triggering refinement. Conversely, borderline TOP1 with strong AVG5 might proceed. Tune both thresholds together for optimal precision/recall trade-offs.\n\n• Range: 0.48-0.60 (typical)\n• Cost-sensitive: 0.50-0.52 (fewer retries)\n• Balanced: 0.52-0.55 (recommended)\n• Quality-focused: 0.55-0.58 (more retries)\n• Effect: Higher = more query rewrites, better quality, higher cost\n• Interacts with: CONF_TOP1 (top result threshold), MQ_REWRITES (rewrite budget)",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Iterative Refinement",
          "href": "https://en.wikipedia.org/wiki/Iterative_refinement"
        },
        {
          "text": "Query Reformulation",
          "href": "https://en.wikipedia.org/wiki/Query_reformulation"
        },
        {
          "text": "Multi-Query RAG",
          "href": "https://arxiv.org/abs/2305.14283"
        }
      ],
      "badges": [
        {
          "text": "Advanced RAG tuning",
          "class": "info"
        },
        {
          "text": "Controls retries",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Fallback Confidence Threshold",
      "key": "CONF_FALLBACK",
      "definition": "When initial retrieval confidence falls below this threshold, triggers a fallback with expanded query rewrites. Lower = more aggressive fallback. Typical: 0.5–0.7.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Confidence Top-1",
      "key": "CONF_TOP1",
      "definition": "Minimum confidence score (0.0-1.0) required to accept the top-1 result without further processing. If the best result scores above this threshold, it's returned immediately. Lower values (0.55-0.60) produce more answers but risk lower quality. Higher values (0.65-0.70) ensure precision but may trigger unnecessary query rewrites. Recommended: 0.60-0.65 for balanced precision/recall.\n\nSweet spot: 0.60-0.65 for production systems. Use 0.65-0.70 when precision is critical and false positives are costly (e.g., production debugging, compliance queries). Use 0.55-0.60 for exploratory search where recall matters more. This threshold gates whether the system accepts the top result or attempts query rewriting for better candidates.\n\nConfidence is computed from hybrid fusion scores, reranking scores, and score boosting. A score of 0.65 means high confidence that the result is relevant. Below the threshold, the system may rewrite the query (if MQ_REWRITES > 1) and try again. Tune this alongside CONF_AVG5 and CONF_ANY for optimal answer rate vs quality.\n\n• Range: 0.55-0.75 (typical)\n• Exploratory: 0.55-0.60 (favor recall)\n• Balanced: 0.60-0.65 (recommended)\n• Precision-critical: 0.65-0.70 (favor precision)\n• Effect: Lower = more answers, higher risk; Higher = fewer answers, higher quality\n• Triggers: Query rewriting when below threshold",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Confidence Thresholds",
          "href": "https://en.wikipedia.org/wiki/Confidence_interval"
        },
        {
          "text": "Precision-Recall Tradeoff",
          "href": "https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall"
        },
        {
          "text": "Decision Boundaries",
          "href": "https://en.wikipedia.org/wiki/Decision_boundary"
        }
      ],
      "badges": [
        {
          "text": "Advanced RAG tuning",
          "class": "info"
        },
        {
          "text": "Affects answer rate",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Index Readiness",
      "key": "DASHBOARD_INDEX_PANEL",
      "definition": "Live embedding config, indexing cost, and storage requirements pulled directly from /api/index/status. Updates automatically every 30 seconds and mirrors the legacy GUI layout exactly.",
      "category": "infrastructure",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Auto-refresh",
          "class": "info"
        }
      ]
    },
    {
      "term": "Data Directory",
      "key": "DATA_DIR",
      "definition": "Base directory for application data storage (logs, tracking, temp files). Defaults to ./data. Change if you need data stored elsewhere or shared across deployments.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Directory Structure",
          "href": "https://en.wikipedia.org/wiki/Directory_structure"
        }
      ],
      "badges": []
    },
    {
      "term": "Dev Local Uvicorn",
      "key": "DEV_LOCAL_UVICORN",
      "definition": "Run Uvicorn ASGI server in direct Python mode instead of Docker for faster development iteration (1=yes, 0=no). Enables hot-reload and easier debugging. Production should use 0 (Docker).",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Uvicorn",
          "href": "https://www.uvicorn.org/"
        }
      ],
      "badges": []
    },
    {
      "term": "Clear Python bytecode caches",
      "key": "DEV_STACK_CLEAR_PYTHON_BYTECODE",
      "definition": "Clears <span class=\"tt-strong\">Python bytecode caches</span> inside this repo and then triggers a backend reload.<br><br><span class=\"tt-strong\">What it deletes</span> (repo-owned only):<br>- <span class=\"mono\">__pycache__/</span> directories<br>- <span class=\"mono\">*.pyc</span> files<br><br><span class=\"tt-strong\">Where</span>: <span class=\"mono\">server/</span>, <span class=\"mono\">tests/</span>, <span class=\"mono\">scripts/</span><br><br><span class=\"tt-strong\">What it does NOT delete</span>: your <span class=\"mono\">.venv</span> / uv / pip caches, Docker images/volumes, Postgres/Neo4j data, or model files under <span class=\"mono\">models/</span>.<br><br><span class=\"tt-strong\">Expected consequences</span>: backend reload (brief interruption) and a slightly slower first request after reload due to re-import/compile.<br><br>Use this when you suspect stale bytecode after refactors or aggressive file watching — not as routine maintenance.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Python bytecode docs",
          "href": "https://docs.python.org/3/library/importlib.html#bytecode-cache"
        }
      ],
      "badges": [
        {
          "text": "Safe",
          "class": "ok"
        }
      ]
    },
    {
      "term": "All Containers",
      "key": "DOCKER_ALL_CONTAINERS",
      "definition": "Complete list of Docker containers on this host, including running, stopped, and paused containers. Use this view to manage container lifecycle (start, stop, restart, pause, remove) and view logs for debugging.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Container States",
          "href": "https://docs.docker.com/engine/reference/commandline/ps/"
        },
        {
          "text": "Container Management",
          "href": "https://docs.docker.com/config/containers/"
        }
      ],
      "badges": [
        {
          "text": "Container management",
          "class": "info"
        }
      ]
    },
    {
      "term": "Container Action Timeout",
      "key": "DOCKER_CONTAINER_ACTION_TIMEOUT",
      "definition": "Maximum seconds to wait for container start/stop/restart operations. Containers with complex startup sequences or cleanup hooks may need higher values. If container actions timeout, increase this. Range: 5-120 seconds.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Container Lifecycle",
          "href": "https://docs.docker.com/config/containers/start-containers-automatically/"
        },
        {
          "text": "Stop Containers",
          "href": "https://docs.docker.com/engine/reference/commandline/stop/"
        }
      ],
      "badges": [
        {
          "text": "Container operations",
          "class": "info"
        }
      ]
    },
    {
      "term": "Container List Timeout",
      "key": "DOCKER_CONTAINER_LIST_TIMEOUT",
      "definition": "Maximum seconds to wait when listing all Docker containers. Increase if you have many containers (100+) or slow Docker API response. Range: 1-60 seconds.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Docker ps command",
          "href": "https://docs.docker.com/engine/reference/commandline/ps/"
        },
        {
          "text": "Container Management",
          "href": "https://docs.docker.com/config/containers/"
        }
      ],
      "badges": [
        {
          "text": "Performance",
          "class": "info"
        }
      ]
    },
    {
      "term": "Infrastructure Services",
      "key": "DOCKER_INFRASTRUCTURE_SERVICES",
      "definition": "TriBridRAG infrastructure containers that power the RAG engine. Includes Postgres (storage + pgvector), Neo4j (graph), Grafana (monitoring), Loki (log aggregation), Prometheus (metrics), and Alertmanager (notifications). Start all services with \"Start All\" or manage individually.",
      "category": "infrastructure",
      "related": [
        "PostgreSQL pgvector URL",
        "Neo4j Connection URI"
      ],
      "links": [],
      "badges": [
        {
          "text": "Core services",
          "class": "info"
        }
      ]
    },
    {
      "term": "Infrastructure Down Timeout",
      "key": "DOCKER_INFRA_DOWN_TIMEOUT",
      "definition": "Maximum seconds to wait when stopping TriBridRAG infrastructure services. Containers with data persistence may need time to flush to disk. If infra down fails, increase this value. Range: 10-120 seconds.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Docker Compose Down",
          "href": "https://docs.docker.com/compose/reference/down/"
        },
        {
          "text": "Graceful Shutdown",
          "href": "https://docs.docker.com/engine/reference/commandline/stop/#extended-description"
        }
      ],
      "badges": [
        {
          "text": "Infrastructure shutdown",
          "class": "info"
        }
      ]
    },
    {
      "term": "Infrastructure Up Timeout",
      "key": "DOCKER_INFRA_UP_TIMEOUT",
      "definition": "Maximum seconds to wait when starting TriBridRAG infrastructure services (Postgres, Neo4j, Grafana, Loki, etc.) via docker-compose. First-time startup may pull images and take longer. If infra up fails with timeout, increase this value. Range: 30-300 seconds.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Docker Compose",
          "href": "https://docs.docker.com/compose/"
        }
      ],
      "badges": [
        {
          "text": "Infrastructure startup",
          "class": "info"
        },
        {
          "text": "May pull images",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Log Lines to Tail",
      "key": "DOCKER_LOGS_TAIL",
      "definition": "Number of log lines to display when viewing container logs. Higher values show more history but may slow down log retrieval. Use 50-100 for quick checks, 500-1000 for debugging. Range: 10-1000 lines.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Docker Logs",
          "href": "https://docs.docker.com/engine/reference/commandline/logs/"
        },
        {
          "text": "Log Management",
          "href": "https://docs.docker.com/config/containers/logging/"
        }
      ],
      "badges": [
        {
          "text": "Log visibility",
          "class": "info"
        }
      ]
    },
    {
      "term": "Include Log Timestamps",
      "key": "DOCKER_LOGS_TIMESTAMPS",
      "definition": "Whether to include timestamps in Docker log output. Timestamps help correlate events across containers but add visual noise. Set to 1 to show timestamps, 0 to hide them.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Docker Logs Timestamps",
          "href": "https://docs.docker.com/engine/reference/commandline/logs/#options"
        },
        {
          "text": "Log Analysis",
          "href": "https://grafana.com/docs/loki/latest/"
        }
      ],
      "badges": [
        {
          "text": "Log format",
          "class": "info"
        }
      ]
    },
    {
      "term": "Docker Settings",
      "key": "DOCKER_SETTINGS",
      "definition": "Configuration settings for Docker timeouts and log behavior. These settings control how long TriBridRAG waits for Docker operations and how logs are displayed. Adjust these if you experience timeouts or need more/less log output.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Docker Configuration",
          "href": "https://docs.docker.com/config/"
        }
      ],
      "badges": [
        {
          "text": "Configurable",
          "class": "info"
        }
      ]
    },
    {
      "term": "Docker Status",
      "key": "DOCKER_STATUS",
      "definition": "Real-time status of the Docker daemon connection. Shows whether TriBridRAG can communicate with Docker to manage containers. If status is unhealthy, ensure Docker Desktop is running or the Docker daemon is accessible.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Docker Daemon",
          "href": "https://docs.docker.com/config/daemon/"
        },
        {
          "text": "Troubleshooting",
          "href": "https://docs.docker.com/config/daemon/troubleshoot/"
        }
      ],
      "badges": [
        {
          "text": "Health check",
          "class": "info"
        }
      ]
    },
    {
      "term": "Docker Status Timeout",
      "key": "DOCKER_STATUS_TIMEOUT",
      "definition": "Maximum seconds to wait when checking Docker daemon status. Increase if your Docker host is slow to respond or under heavy load. If health checks timeout frequently, raise this value. Range: 1-30 seconds.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Docker Health Checks",
          "href": "https://docs.docker.com/engine/reference/commandline/inspect/"
        },
        {
          "text": "Docker Daemon",
          "href": "https://docs.docker.com/config/daemon/"
        }
      ],
      "badges": [
        {
          "text": "Performance",
          "class": "info"
        }
      ]
    },
    {
      "term": "Documentation Directory",
      "key": "DOCS_DIR",
      "definition": "Path to the documentation directory containing markdown files, API references, and user guides. This directory is served at /docs/* by the FastAPI static file handler, making documentation accessible through the web interface. Used by the built-in documentation viewer and help system. Default is ./docs. Change this if you have moved your documentation to a custom location or are using a shared docs directory across multiple projects.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Static File Serving",
          "href": "https://fastapi.tiangolo.com/tutorial/static-files/"
        }
      ],
      "badges": [
        {
          "text": "Optional",
          "class": "info"
        }
      ]
    },
    {
      "term": "Editor Bind Address",
      "key": "EDITOR_BIND",
      "definition": "Network interface for editor service to bind to. Use 127.0.0.1 for localhost-only access (secure), 0.0.0.0 for network access (enable remote editing).",
      "category": "ui",
      "related": [],
      "links": [
        {
          "text": "Network Binding",
          "href": "https://en.wikipedia.org/wiki/Network_socket"
        }
      ],
      "badges": []
    },
    {
      "term": "Editor Embed Mode",
      "key": "EDITOR_EMBED_ENABLED",
      "definition": "Enable embedded editor iframe in GUI (1=yes, 0=no). When enabled, editor opens inline. When disabled, opens in new tab/window. Embedding requires CORS configuration.",
      "category": "embedding",
      "related": [],
      "links": [
        {
          "text": "iframe Embedding",
          "href": "https://developer.mozilla.org/en-US/docs/Web/HTML/Element/iframe"
        }
      ],
      "badges": []
    },
    {
      "term": "Editor Enabled",
      "key": "EDITOR_ENABLED",
      "definition": "Enable embedded code editor integration in GUI (1=yes, 0=no). Allows viewing and editing code snippets from retrieval results directly in browser.",
      "category": "ui",
      "related": [],
      "links": [
        {
          "text": "Code Editor Integration",
          "href": "https://en.wikipedia.org/wiki/Source-code_editor"
        }
      ],
      "badges": []
    },
    {
      "term": "Editor Port",
      "key": "EDITOR_PORT",
      "definition": "TCP port for code editor service. Default: varies by editor. Must not conflict with other services (PORT, MCP_HTTP_PORT, PROMETHEUS_PORT).",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Port Configuration",
          "href": "https://en.wikipedia.org/wiki/Port_(computer_networking)"
        }
      ],
      "badges": []
    },
    {
      "term": "Embedding Batch Size",
      "key": "EMBEDDING_BATCH_SIZE",
      "definition": "Number of text chunks to embed in a single API call or local batch during indexing. Higher values (50-200) speed up indexing by reducing API round trips but may hit rate limits or memory constraints. Lower values (10-30) are safer but slower. For OpenAI/Voyage APIs, batching significantly reduces total indexing time. For local models, larger batches improve GPU utilization but require more VRAM. If indexing fails with rate limit or OOM errors, reduce this value.\n\nRecommended: 100-150 for API providers, 16-32 for local models (GPU), 4-8 for CPU-only.",
      "category": "embedding",
      "related": [],
      "links": [
        {
          "text": "OpenAI Batch Embedding",
          "href": "https://platform.openai.com/docs/guides/embeddings/use-cases"
        },
        {
          "text": "Rate Limits",
          "href": "https://platform.openai.com/docs/guides/rate-limits"
        },
        {
          "text": "GPU Memory Management",
          "href": "https://huggingface.co/docs/transformers/en/perf_train_gpu_one"
        }
      ],
      "badges": [
        {
          "text": "Performance tuning",
          "class": "info"
        },
        {
          "text": "Watch rate limits",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Embedding Cache",
      "key": "EMBEDDING_CACHE_ENABLED",
      "definition": "Cache embedding API results to disk to avoid re-computing vectors for identical text. Reduces API costs and speeds up reindexing. Disable only for debugging or when embeddings change frequently.",
      "category": "embedding",
      "related": [],
      "links": [
        {
          "text": "Caching Strategies",
          "href": "https://en.wikipedia.org/wiki/Cache_(computing)"
        },
        {
          "text": "Embedding Best Practices",
          "href": "https://platform.openai.com/docs/guides/embeddings/use-cases"
        }
      ],
      "badges": []
    },
    {
      "term": "Embedding Dimension",
      "key": "EMBEDDING_DIM",
      "definition": "Vector dimensionality for MXBAI/local embedding models. Common sizes: 384 (fast, lower quality), 768 (balanced, recommended), 1024 (best quality, slower). Larger dimensions capture more semantic nuance but increase Qdrant storage requirements and query latency. Must match your embedding model's output size. Changing this requires full reindexing - vectors of different dimensions are incompatible.",
      "category": "embedding",
      "related": [],
      "links": [
        {
          "text": "Vector Embeddings",
          "href": "https://en.wikipedia.org/wiki/Word_embedding"
        },
        {
          "text": "Dimensionality Tradeoffs",
          "href": "https://www.sbert.net/docs/pretrained_models.html#model-overview"
        },
        {
          "text": "Qdrant Vector Config",
          "href": "https://qdrant.tech/documentation/concepts/collections/#create-a-collection"
        }
      ],
      "badges": [
        {
          "text": "Requires reindex",
          "class": "reindex"
        },
        {
          "text": "Affects storage",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Embedding Configuration Valid",
      "key": "EMBEDDING_MATCH",
      "definition": "Your current embedding configuration matches what was used to create the index. Search results will be accurate and relevant. The vectors in your index are compatible with queries generated using your current embedding model.",
      "category": "embedding",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Valid",
          "class": "info"
        }
      ]
    },
    {
      "term": "Embedding Max Tokens",
      "key": "EMBEDDING_MAX_TOKENS",
      "definition": "Maximum token length for text chunks sent to embedding models during indexing. Text exceeding this length is truncated by the tokenizer. Most embedding models support 512-8192 tokens. Longer limits preserve more context per chunk but increase embedding cost and processing time. Shorter limits are faster and cheaper but may lose semantic context for large functions/classes. Balance based on your average code chunk size and model capabilities.\n\nRecommended: 512 for most code (functions/methods), 1024 for documentation-heavy repos, 256 for ultra-fast indexing.",
      "category": "embedding",
      "related": [],
      "links": [
        {
          "text": "Tokenization Basics",
          "href": "https://huggingface.co/docs/transformers/main/en/tokenizer_summary"
        },
        {
          "text": "OpenAI Token Limits",
          "href": "https://platform.openai.com/docs/guides/embeddings/embedding-models"
        },
        {
          "text": "Voyage Limits",
          "href": "https://docs.voyageai.com/docs/embeddings#input-text"
        }
      ],
      "badges": [
        {
          "text": "Affects cost",
          "class": "warn"
        },
        {
          "text": "Context preservation",
          "class": "info"
        }
      ]
    },
    {
      "term": "Embedding Type Mismatch",
      "key": "EMBEDDING_MISMATCH",
      "definition": "Your current embedding configuration differs from what was used to create your index. This is a CRITICAL issue that will cause search to return completely irrelevant results. Embeddings are mathematical representations of text in high-dimensional vector space - when you use different embedding models, these vectors exist in incompatible spaces and cannot be meaningfully compared. Think of it like trying to search a French dictionary using Spanish words - the dimensions and meaning of the numbers don't align. You must either: (1) Re-index your code with the current embedding type, or (2) Change your embedding configuration back to match what the index was built with.",
      "category": "embedding",
      "related": [],
      "links": [
        {
          "text": "What are Embeddings?",
          "href": "https://platform.openai.com/docs/guides/embeddings"
        },
        {
          "text": "Vector Space Explained",
          "href": "https://en.wikipedia.org/wiki/Vector_space"
        },
        {
          "text": "Semantic Search",
          "href": "https://www.pinecone.io/learn/semantic-search/"
        },
        {
          "text": "Embedding Model Comparison",
          "href": "https://huggingface.co/spaces/mteb/leaderboard"
        }
      ],
      "badges": [
        {
          "text": "Critical",
          "class": "err"
        },
        {
          "text": "Requires Action",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Embedding Model (OpenAI)",
      "key": "EMBEDDING_MODEL",
      "definition": "OpenAI embedding model name when EMBEDDING_TYPE=openai. Current options: \"text-embedding-3-small\" (512-3072 dims, $0.02/1M tokens, fast), \"text-embedding-3-large\" (256-3072 dims, $0.13/1M tokens, highest quality), \"text-embedding-ada-002\" (legacy, 1536 dims, $0.10/1M tokens). Larger models improve semantic search quality but cost more and require more storage. Changing this requires full reindexing as embeddings are incompatible across models.\n\nRecommended: text-embedding-3-small for most use cases, text-embedding-3-large for production systems demanding highest quality.",
      "category": "embedding",
      "related": [],
      "links": [
        {
          "text": "OpenAI Embeddings Guide",
          "href": "https://platform.openai.com/docs/guides/embeddings"
        },
        {
          "text": "Embedding Models",
          "href": "https://platform.openai.com/docs/models/embeddings"
        },
        {
          "text": "Pricing Calculator",
          "href": "https://openai.com/api/pricing/"
        }
      ],
      "badges": [
        {
          "text": "Requires reindex",
          "class": "reindex"
        },
        {
          "text": "Costs API calls",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Local Embedding Model",
      "key": "EMBEDDING_MODEL_LOCAL",
      "definition": "HuggingFace model name or local path when EMBEDDING_TYPE=local or mxbai. Popular options: \"mixedbread-ai/mxbai-embed-large-v1\" (1024 dims, excellent quality), \"BAAI/bge-small-en-v1.5\" (384 dims, fast), \"sentence-transformers/all-MiniLM-L6-v2\" (384 dims, lightweight). Local embeddings are free but slower than API-based options. Model is downloaded on first use and cached locally. Choose larger models (768-1024 dims) for quality or smaller (384 dims) for speed.\n\nRecommended: mxbai-embed-large-v1 for best free quality, all-MiniLM-L6-v2 for resource-constrained environments.",
      "category": "embedding",
      "related": [],
      "links": [
        {
          "text": "Sentence Transformers Models",
          "href": "https://www.sbert.net/docs/sentence_transformer/pretrained_models.html"
        },
        {
          "text": "HuggingFace Model Hub",
          "href": "https://huggingface.co/models?pipeline_tag=feature-extraction&sort=downloads"
        },
        {
          "text": "MTEB Leaderboard",
          "href": "https://huggingface.co/spaces/mteb/leaderboard"
        }
      ],
      "badges": [
        {
          "text": "Free (no API)",
          "class": "info"
        },
        {
          "text": "Requires download",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Embedding Max Retries",
      "key": "EMBEDDING_RETRY_MAX",
      "definition": "Retry attempts for failed embedding API calls during indexing. Higher values ensure indexing completes despite transient errors but slow down failure recovery. Typical: 2-3 retries.",
      "category": "embedding",
      "related": [],
      "links": [
        {
          "text": "Error Handling",
          "href": "https://platform.openai.com/docs/guides/error-codes"
        },
        {
          "text": "Retry Patterns",
          "href": "https://en.wikipedia.org/wiki/Retry_pattern"
        }
      ],
      "badges": []
    },
    {
      "term": "Embedding Timeout",
      "key": "EMBEDDING_TIMEOUT",
      "definition": "Maximum seconds to wait for embedding API response. Similar to GEN_TIMEOUT but for embedding calls during indexing. Increase for large batches or slow networks. Typical: 30-60 seconds.",
      "category": "embedding",
      "related": [],
      "links": [
        {
          "text": "API Timeouts",
          "href": "https://platform.openai.com/docs/guides/rate-limits"
        },
        {
          "text": "Embedding API",
          "href": "https://platform.openai.com/docs/api-reference/embeddings"
        }
      ],
      "badges": []
    },
    {
      "term": "Embedding Provider",
      "key": "EMBEDDING_TYPE",
      "definition": "Selects the embedding provider for dense vector search. Also determines the token counter used during code chunking, which affects chunk boundaries and splitting behavior.\n\n• openai — strong quality, paid (cl100k tokenizer)\n• voyage — strong retrieval, paid (voyage tokenizer)\n• mxbai — OSS via SentenceTransformers\n• local — any HuggingFace SentenceTransformer model\n• gemini — Google Gemini embeddings\n\nNote: Changing this setting affects both retrieval quality AND how code is split into chunks during indexing. A reindex is required after changing.",
      "category": "embedding",
      "related": [],
      "links": [
        {
          "text": "OpenAI Embeddings",
          "href": "https://platform.openai.com/docs/guides/embeddings"
        },
        {
          "text": "Voyage AI Embeddings",
          "href": "https://docs.voyageai.com/docs/embeddings"
        },
        {
          "text": "Google Gemini Embeddings",
          "href": "https://ai.google.dev/gemini-api/docs/embeddings"
        },
        {
          "text": "SentenceTransformers Docs",
          "href": "https://www.sbert.net/"
        }
      ],
      "badges": [
        {
          "text": "Requires reindex",
          "class": "reindex"
        },
        {
          "text": "Affects chunking",
          "class": "info"
        }
      ]
    },
    {
      "term": "Endpoint Call Frequency (calls/min)",
      "key": "ENDPOINT_CALL_FREQUENCY",
      "definition": "Alert when a single API endpoint receives this many calls per minute. Detects infinite loops, polling gone wrong, or DDoS-like patterns. For example, if /api/search is called 100 times/min for 2+ minutes, something is likely wrong. Typical values: 10-30 calls/min for normal usage, 100+ for high-traffic production.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Anomaly Detection",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Sustained Frequency Duration (minutes)",
      "key": "ENDPOINT_SUSTAINED_DURATION",
      "definition": "How long the high call frequency must be sustained before triggering an alert. Prevents false positives from legitimate bursts. For example, if frequency threshold is 20 calls/min and duration is 2 minutes, the endpoint must receive 20+ calls/min for 2 consecutive minutes to alert. Typical values: 2-5 minutes for quick detection, 10+ for noise reduction.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Alert Design Patterns",
          "href": "https://grafana.com/docs/grafana/latest/alerting/fundamentals/"
        }
      ],
      "badges": [
        {
          "text": "Anomaly Detection",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Enrichment Backend",
      "key": "ENRICH_BACKEND",
      "definition": "Backend service for generating code summaries and enrichment metadata during indexing. Options: \"openai\" (GPT models, highest quality), \"ollama\" (local models, free), \"mlx\" (Apple Silicon optimized). Enrichment adds per-chunk summaries and keywords used by features like cards and improved reranking. Disable to speed up indexing or reduce costs.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "MLX on Apple Silicon",
          "href": "https://github.com/ml-explore/mlx"
        },
        {
          "text": "Ollama Local Models",
          "href": "https://ollama.com/library"
        }
      ],
      "badges": [
        {
          "text": "Optional feature",
          "class": "info"
        },
        {
          "text": "Increases index time",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Enrich Code Chunks",
      "key": "ENRICH_CODE_CHUNKS",
      "definition": "Enable per-chunk code summarization during indexing. When on, each code chunk gets an AI-generated summary and keywords stored alongside the code. Powers the Cards feature (high-level code summaries) and improves reranking by providing semantic context. Increases indexing time and cost (API calls) but significantly improves retrieval quality for conceptual queries like \"where is auth handled?\"",
      "category": "chunking",
      "related": [],
      "links": [
        {
          "text": "Code Summarization",
          "href": "https://en.wikipedia.org/wiki/Automatic_summarization"
        }
      ],
      "badges": [
        {
          "text": "Better retrieval",
          "class": "info"
        },
        {
          "text": "Slower indexing",
          "class": "warn"
        },
        {
          "text": "Costs API calls",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Disable Enrichment",
      "key": "ENRICH_DISABLED",
      "definition": "Completely disable code enrichment (summaries, keywords, cards) during indexing (1=disable, 0=enable). When disabled, indexing is much faster and cheaper (no LLM API calls) but you lose card search, enriched metadata, and semantic boosting. Use this for quick re-indexing during development, CI/CD pipelines, or when working with non-code content. Re-enable for production to get full retrieval quality benefits.\n\nRecommended: 0 (enrichment ON) for production, 1 (enrichment OFF) for fast iteration and testing.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Much faster indexing",
          "class": "info"
        },
        {
          "text": "Loses card search",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Enrichment Model",
      "key": "ENRICH_MODEL",
      "definition": "Specific model name for code enrichment when ENRICH_BACKEND is set. For OpenAI: \"gpt-4o-mini\" (recommended, cheap), \"gpt-4o\" (higher quality, costly). For Ollama: specify via ENRICH_MODEL_OLLAMA instead. Smaller models (gpt-4o-mini, qwen2.5-coder:7b) balance cost and quality for summaries. Enrichment happens during indexing, not at query time.",
      "category": "generation",
      "related": [],
      "links": [
        {
          "text": "OpenAI Models",
          "href": "https://platform.openai.com/docs/models"
        },
        {
          "text": "Cost Comparison",
          "href": "https://openai.com/api/pricing/"
        }
      ],
      "badges": [
        {
          "text": "Affects index cost",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Enrichment Model (Ollama)",
      "key": "ENRICH_MODEL_OLLAMA",
      "definition": "Ollama model name for code enrichment when ENRICH_BACKEND=ollama. Recommended: \"qwen2.5-coder:7b\" (fast, code-focused), \"deepseek-coder:6.7b\" (excellent code understanding), \"codellama:13b\" (high quality, slower). Model must be pulled via \"ollama pull <model>\" before use. Local enrichment is free but slower than cloud APIs.",
      "category": "generation",
      "related": [],
      "links": [
        {
          "text": "Ollama Models",
          "href": "https://ollama.com/library"
        },
        {
          "text": "Pull Models",
          "href": "https://github.com/ollama/ollama#quickstart"
        },
        {
          "text": "Code-Focused Models",
          "href": "https://ollama.com/search?c=tools"
        }
      ],
      "badges": [
        {
          "text": "Free (local)",
          "class": "info"
        },
        {
          "text": "Requires model download",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Entity Types",
      "key": "ENTITY_TYPES",
      "definition": "Types of code entities to extract and store in the graph: function, class, module, variable, import. Each entity becomes a node in Neo4j with relationships to other entities it references or contains.",
      "category": "retrieval",
      "related": [
        "Relationship Types",
        "Max Hops",
        "Neo4j Connection URI"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Error Rate Threshold (%)",
      "key": "ERROR_RATE_THRESHOLD",
      "definition": "Percentage threshold for triggering error rate alerts. When the error rate across all requests exceeds this percentage over a 5-minute window, Grafana will trigger an alert. Typical values: 5% for production (strict), 10-15% for development. Set lower for critical systems, higher for experimental features.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Grafana Alerting",
          "href": "https://grafana.com/docs/grafana/latest/alerting/"
        },
        {
          "text": "SLOs and Error Budgets",
          "href": "https://sre.google/sre-book/service-level-objectives/"
        }
      ],
      "badges": [
        {
          "text": "Performance",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Eval Analysis",
      "key": "EVAL_ANALYSIS_SUBTAB",
      "definition": "View and compare RAG evaluation runs. Analyze retrieval accuracy metrics, see question-by-question results, compare configuration changes between runs, and get AI-powered insights on performance regressions and recommendations.",
      "category": "evaluation",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Deep-dive analysis",
          "class": "info"
        }
      ]
    },
    {
      "term": "Compare With (BEFORE)",
      "key": "EVAL_COMPARE_RUN",
      "definition": "Optionally select a previous evaluation run to compare against. This enables the configuration diff view showing exactly what parameters changed between runs, and highlights regressions (questions that got worse) vs improvements. The AI Analysis will use both runs to provide root cause analysis and recommendations.",
      "category": "evaluation",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Optional",
          "class": "info"
        },
        {
          "text": "Enables AI Analysis",
          "class": "success"
        }
      ]
    },
    {
      "term": "Eval Final‑K",
      "key": "EVAL_FINAL_K",
      "definition": "Number of top results to consider when evaluating Hit@K metrics. If set to 10, eval checks if the expected answer appears in the top 10 results. Lower values (5) test precision, higher values (20) test recall. Should match your production FINAL_K setting for realistic evaluation. Common: 5 (strict), 10 (balanced), 20 (lenient).",
      "category": "evaluation",
      "related": [],
      "links": [
        {
          "text": "Hit@K Metric",
          "href": "https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_K"
        }
      ],
      "badges": []
    },
    {
      "term": "Evaluation Logs Terminal",
      "key": "EVAL_LOGS_TERMINAL",
      "definition": "Open the sliding terminal to stream raw evaluation output (question-by-question) and verify the exact settings used for the last run.",
      "category": "evaluation",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Live output",
          "class": "info"
        }
      ]
    },
    {
      "term": "Eval Multi‑Query",
      "key": "EVAL_MULTI",
      "definition": "Enable multi-query expansion during evaluation runs (1=yes, 0=no). When enabled, each golden question is rewritten multiple times (per MQ_REWRITES setting) to test recall under query variation. Turning this on makes eval results match production behavior if you use multi-query in prod, but increases eval runtime. Use 1 to measure realistic performance, 0 for faster eval iterations.",
      "category": "evaluation",
      "related": [],
      "links": [
        {
          "text": "Multi-Query RAG",
          "href": "https://arxiv.org/abs/2305.14283"
        }
      ],
      "badges": [
        {
          "text": "Affects eval time",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Primary Run (AFTER)",
      "key": "EVAL_PRIMARY_RUN",
      "definition": "Select the evaluation run to analyze. This is typically the most recent run you want to inspect. When comparing, this is the \"AFTER\" run showing your latest configuration changes. The accuracy metrics and question results will be displayed from this run.",
      "category": "evaluation",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Required",
          "class": "info"
        }
      ]
    },
    {
      "term": "Sample Size (Quick vs Full)",
      "key": "EVAL_SAMPLE_SIZE",
      "definition": "Limit evaluation to a subset of golden questions for faster iteration and testing. Quick (10): ~1 minute, good for sanity checks. Medium (25): ~2-3 minutes, better coverage. Large (50): ~5 minutes, more representative. Full (all): Run complete eval suite for milestone validation and regression detection. Leave empty or select \"Full\" to evaluate all questions. Sample evals are perfect for rapid iteration; use full evals before production changes or major updates.",
      "category": "evaluation",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Quick testing",
          "class": "info"
        },
        {
          "text": "Sample recommended for CI",
          "class": "info"
        }
      ]
    },
    {
      "term": "Exclude Directories",
      "key": "EXCLUDE_PATHS",
      "definition": "Comma‑separated directories to exclude when building semantic Code Cards or indexing. Examples: node_modules, vendor, dist.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Fallback Confidence",
      "key": "FALLBACK_CONFIDENCE",
      "definition": "Confidence threshold that decides when to escalate to fallback retrieval strategies (e.g., rewrite the query, broaden candidate pools, or lean on alternative sources) instead of trusting the initial result set.\n\nThink of it as “how bad is too bad”: if the system’s confidence in the current retrieval is below this, it tries something else; if it’s above, it proceeds without extra work.\n\nHigher values trigger fallbacks more often (usually better quality, higher latency/cost). Lower values accept more first-pass results (faster, riskier). Tune alongside CONF_TOP1 and CONF_AVG5 so you don’t over-trigger rewrites.\n\n• Range: 0.0–1.0\n• Typical: 0.50–0.60\n• Default: 0.55\n• Higher: more retries/fallbacks (slower, higher precision)\n• Lower: fewer retries (faster, may answer with weaker evidence)\n• Interacts with: Confidence Top-1, Confidence Avg-5, Multi‑Query Rewrites",
      "category": "retrieval",
      "related": [
        "Confidence Top-1",
        "Confidence Avg-5",
        "Multi‑Query Rewrites"
      ],
      "links": [
        {
          "text": "Query Reformulation",
          "href": "https://en.wikipedia.org/wiki/Query_reformulation"
        },
        {
          "text": "Precision and Recall",
          "href": "https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall"
        }
      ],
      "badges": [
        {
          "text": "Advanced RAG tuning",
          "class": "info"
        },
        {
          "text": "Controls retries",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Filename Exact Match Multiplier",
      "key": "FILENAME_BOOST_EXACT",
      "definition": "Score multiplier applied when the filename matches the query exactly (e.g., auth.py). Increase to prioritize file‑specific queries.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Path Component Partial Match Multiplier",
      "key": "FILENAME_BOOST_PARTIAL",
      "definition": "Score multiplier for matches in any path component (dir name or filename prefix). Useful for queries like \"auth\" that should find src/auth/... files.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Files Root Override",
      "key": "FILES_ROOT",
      "definition": "Override the root directory for the /files HTTP mount point. This setting controls where the FastAPI static file server looks for files when serving requests to /files/*. By default, TriBridRAG uses the repository root. Set this when you need to serve files from a different location, such as a mounted volume in Docker, a shared NFS mount, or a custom data directory. Example: /mnt/shared/tribrid-files",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Static Files (FastAPI)",
          "href": "https://fastapi.tiangolo.com/tutorial/static-files/"
        },
        {
          "text": "Docker Volumes",
          "href": "https://docs.docker.com/storage/volumes/#use-a-volume-with-docker-compose"
        }
      ],
      "badges": [
        {
          "text": "Optional",
          "class": "info"
        },
        {
          "text": "Advanced",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Final Top‑K",
      "key": "FINAL_K",
      "definition": "Number of top results to return after hybrid fusion, reranking, and scoring boosts. This is what you get back from search. Higher values (15-30) provide more context but may include noise. Lower values (5-10) are faster and more precise. Default: 10. Recommended: 10 for chat, 20-30 for browsing/exploration.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Precision vs Recall",
          "href": "https://en.wikipedia.org/wiki/Precision_and_recall"
        },
        {
          "text": "Top-K Selection",
          "href": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Top-K_retrieval"
        }
      ],
      "badges": [
        {
          "text": "Core Setting",
          "class": "info"
        }
      ]
    },
    {
      "term": "Frequency Penalty",
      "key": "FREQUENCY_PENALTY",
      "definition": "Penalizes tokens that appear frequently in the generated text so far. Higher values reduce repetition and boilerplate; lower values allow more reuse of prior tokens. Typical ranges: 0.0-0.5 for code answers to avoid runaway repetition; 0.5-1.0 for verbose prose or summarization; >1.0 is rarely needed and can destabilize outputs. Applies per-token during sampling. Combine with TOP_P/TEMPERATURE carefully: high frequency penalty + high temperature can over-constrain and reduce coherence.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "OpenAI Sampling",
          "href": "https://platform.openai.com/docs/guides/text-generation"
        },
        {
          "text": "Frequency Penalty API",
          "href": "https://platform.openai.com/docs/api-reference/chat/create#chat-create-frequency_penalty"
        },
        {
          "text": "Decoding Strategies",
          "href": "https://huggingface.co/blog/how-to-generate"
        }
      ],
      "badges": [
        {
          "text": "Anti-repetition",
          "class": "info"
        },
        {
          "text": "Tune carefully",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Freshness Bonus",
      "key": "FRESHNESS_BONUS",
      "definition": "Score boost applied to recently modified files during reranking, prioritizing newer code over stale code. Based on file modification time (mtime). Files modified in the last N days receive the full bonus, with linear decay over time. Useful for prioritizing recent work, active features, and current implementation patterns. Typical range: 0.0 (disabled) to 0.10 (strong recency bias).\n\nSweet spot: 0.03-0.06 for subtle freshness preference. Use 0.06-0.10 for strong recency bias when your codebase changes rapidly and recent code is more likely relevant. Use 0.0 to disable entirely for stable codebases where age doesn't correlate with relevance. The bonus decays linearly from full value (files modified <7 days ago) to zero (files modified >90 days ago).\n\nExample: With 0.05 bonus, a file modified yesterday gets +0.05, a file modified 30 days ago gets +0.025, a file modified 90+ days ago gets 0. Freshness helps when users ask \"how do we currently handle X?\" - emphasizes recent implementations over legacy code. Trade-off: May deprioritize well-tested stable code in favor of recent changes.\n\n• Range: 0.0-0.10 (typical)\n• Disabled: 0.0 (age-agnostic ranking)\n• Subtle: 0.03-0.05\n• Balanced: 0.05-0.06 (recommended for active repos)\n• Strong recency: 0.08-0.10\n• Decay window: Full bonus at 0-7 days, linear decay to 90 days\n• Trade-off: Recent code vs battle-tested stable code",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Freshness in Ranking",
          "href": "https://en.wikipedia.org/wiki/Freshness_(search_engine)"
        },
        {
          "text": "Temporal Relevance",
          "href": "https://en.wikipedia.org/wiki/Temporal_information_retrieval"
        },
        {
          "text": "Recency Bias",
          "href": "https://en.wikipedia.org/wiki/Recency_bias"
        }
      ],
      "badges": [
        {
          "text": "Advanced RAG tuning",
          "class": "info"
        },
        {
          "text": "Time-based ranking",
          "class": "info"
        }
      ]
    },
    {
      "term": "Graph Weight",
      "key": "FUSION_GRAPH_WEIGHT",
      "definition": "Weight assigned to graph (Neo4j) search results in weighted fusion mode. Higher values (0.4-0.6) favor structural relationships, lower values (0.2-0.3) reduce graph influence. Weights must sum to ~1.0 with vector and sparse weights. Recommended: 0.3 for balanced tri-brid retrieval.\n\nSweet spot: 0.3 for balanced systems. Use 0.4-0.5 when graph relationships are critical (e.g., finding code that calls or imports specific functions). Use 0.2 when vector and sparse search are more important.\n\n• Range: 0.0-1.0 (must sum with vector + sparse ≈ 1.0)\n• Vector/sparse-focused: 0.2 (lower graph weight)\n• Balanced: 0.3 (recommended)\n• Graph-focused: 0.4-0.5 (higher graph weight)\n• Effect: Higher = more weight to graph search results\n• Symptom too high: Graph matches dominate, other modalities buried\n• Symptom too low: Graph relationships undervalued",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Neo4j GraphRAG",
          "href": "https://neo4j.com/docs/neo4j-graphrag-python/current/user_guide_rag.html"
        },
        {
          "text": "Graph Traversal",
          "href": "https://neo4j.com/blog/developer/graph-traversal-graphrag-python-package"
        },
        {
          "text": "Weighted Fusion",
          "href": "https://en.wikipedia.org/wiki/Data_fusion"
        }
      ],
      "badges": [
        {
          "text": "Weighted Mode",
          "class": "info"
        }
      ]
    },
    {
      "term": "Fusion Method",
      "key": "FUSION_METHOD",
      "definition": "Method for combining results from vector, sparse, and graph search: \"rrf\" (Reciprocal Rank Fusion) or \"weighted\" (score-based weighted sum). RRF combines ranking positions without score normalization, making it robust to different score scales. Weighted fusion requires normalized scores and allows fine-grained control over modality weights.\n\nSweet spot: \"rrf\" for most use cases. RRF is simpler, more robust, and doesn't require score normalization. Use \"weighted\" when you need precise control over modality weights or when score distributions are well-calibrated.\n\n• RRF: Position-based fusion, robust to score scales, simpler\n• Weighted: Score-based fusion, requires normalization, more control\n• Effect: Determines how tri-brid results are combined\n• Symptom wrong method: Suboptimal result ranking",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Reciprocal Rank Fusion",
          "href": "https://cormack.uwaterloo.ca/cormacksigir09-rrf.pdf"
        },
        {
          "text": "RRF Paper",
          "href": "https://research.google/pubs/reciprocal-rank-fusion-outperforms-condorcet-and-individual-rank-learning-methods/"
        },
        {
          "text": "Data Fusion",
          "href": "https://en.wikipedia.org/wiki/Data_fusion"
        }
      ],
      "badges": [
        {
          "text": "Core Setting",
          "class": "info"
        }
      ]
    },
    {
      "term": "Normalize Scores",
      "key": "FUSION_NORMALIZE_SCORES",
      "definition": "Normalize scores from vector, sparse, and graph search to [0,1] range before fusion. This ensures scores from different modalities are comparable when using weighted fusion. When disabled, raw scores are used directly (may cause one modality to dominate). Recommended: enabled for weighted fusion, not needed for RRF.\n\nSweet spot: enabled for weighted fusion mode. Normalization prevents one modality from dominating due to different score scales. For RRF mode, normalization is unnecessary since RRF uses ranking positions, not scores.\n\n• Enabled: Scores normalized to [0,1], comparable across modalities\n• Disabled: Raw scores used, may cause modality imbalance\n• Effect: Controls score normalization before weighted fusion\n• Symptom if disabled: One modality may dominate due to score scale differences",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Score Normalization",
          "href": "https://link.springer.com/chapter/10.1007/11880592_57"
        },
        {
          "text": "Normalization Methods",
          "href": "https://codecademy.com/article/min-max-zscore-normalization"
        },
        {
          "text": "Data Fusion",
          "href": "https://en.wikipedia.org/wiki/Data_fusion"
        }
      ],
      "badges": [
        {
          "text": "Weighted Mode",
          "class": "info"
        }
      ]
    },
    {
      "term": "RRF k Parameter",
      "key": "FUSION_RRF_K",
      "definition": "Smoothing constant for Reciprocal Rank Fusion. Higher values (80-120) give more weight to top-ranked results, lower values (40-60) distribute weight more evenly across ranks. The original RRF paper used k=60, which is near-optimal for most cases. Recommended: 60 for standard RRF behavior.\n\nSweet spot: 60 for production systems (matches original RRF paper). Use 40-50 when you want more uniform weight distribution (less emphasis on top ranks). Use 80-100 when top-ranked results are highly reliable and should dominate.\n\n• Range: 1-200 (typical: 40-100)\n• Uniform weights: 40-50 (less top-rank emphasis)\n• Standard RRF: 60 (recommended, original paper)\n• Top-rank focused: 80-100 (more emphasis on top results)\n• Effect: Higher = more weight to top-ranked results\n• Symptom too low: Top results undervalued\n• Symptom too high: Lower-ranked results ignored",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "RRF Original Paper",
          "href": "https://cormack.uwaterloo.ca/cormacksigir09-rrf.pdf"
        },
        {
          "text": "RRF Research",
          "href": "https://research.google/pubs/reciprocal-rank-fusion-outperforms-condorcet-and-individual-rank-learning-methods/"
        },
        {
          "text": "Reciprocal Rank Fusion",
          "href": "https://en.wikipedia.org/wiki/Reciprocal_rank_fusion"
        }
      ],
      "badges": [
        {
          "text": "RRF Mode",
          "class": "info"
        }
      ]
    },
    {
      "term": "Sparse Weight",
      "key": "FUSION_SPARSE_WEIGHT",
      "definition": "Weight assigned to sparse (BM25) search results in weighted fusion mode. Higher values (0.4-0.6) favor keyword matches, lower values (0.2-0.3) reduce keyword influence. Weights must sum to ~1.0 with vector and graph weights. Recommended: 0.3 for balanced tri-brid retrieval.\n\nSweet spot: 0.3 for balanced systems. Use 0.4-0.5 when exact keyword matching is critical (e.g., finding specific function names or error codes). Use 0.2 when semantic and graph search are more important.\n\n• Range: 0.0-1.0 (must sum with vector + graph ≈ 1.0)\n• Semantic-focused: 0.2 (lower keyword weight)\n• Balanced: 0.3 (recommended)\n• Keyword-focused: 0.4-0.5 (higher keyword weight)\n• Effect: Higher = more weight to sparse search results\n• Symptom too high: Keyword matches dominate, semantic matches buried\n• Symptom too low: Keyword matches undervalued",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "BM25 Algorithm",
          "href": "https://en.wikipedia.org/wiki/Okapi_BM25"
        },
        {
          "text": "Hybrid Search",
          "href": "https://www.elastic.co/search-labs/blog/improving-information-retrieval-elastic-stack-hybrid"
        },
        {
          "text": "Weighted Fusion",
          "href": "https://en.wikipedia.org/wiki/Data_fusion"
        }
      ],
      "badges": [
        {
          "text": "Weighted Mode",
          "class": "info"
        }
      ]
    },
    {
      "term": "Vector Weight",
      "key": "FUSION_VECTOR_WEIGHT",
      "definition": "Weight assigned to vector (pgvector) search results in weighted fusion mode. Higher values (0.5-0.7) favor semantic matches, lower values (0.2-0.4) reduce semantic influence. Weights must sum to ~1.0 with sparse and graph weights. Recommended: 0.4 for balanced tri-brid retrieval.\n\nSweet spot: 0.4 for balanced systems. Use 0.5-0.6 when semantic matching is critical (e.g., finding conceptually similar code). Use 0.2-0.3 when keyword matching is more important than semantics.\n\n• Range: 0.0-1.0 (must sum with sparse + graph ≈ 1.0)\n• Keyword-focused: 0.2-0.3 (lower semantic weight)\n• Balanced: 0.4 (recommended)\n• Semantic-focused: 0.5-0.6 (higher semantic weight)\n• Effect: Higher = more weight to vector search results\n• Symptom too high: Semantic matches dominate, keyword matches buried\n• Symptom too low: Semantic matches undervalued",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Hybrid Search",
          "href": "https://www.pinecone.io/learn/hybrid-search-intro/"
        },
        {
          "text": "Weighted Fusion",
          "href": "https://en.wikipedia.org/wiki/Data_fusion"
        },
        {
          "text": "Fusion Strategies",
          "href": "https://arxiv.org/abs/2402.14734"
        }
      ],
      "badges": [
        {
          "text": "Weighted Mode",
          "class": "info"
        }
      ]
    },
    {
      "term": "Max Tokens",
      "key": "GEN_MAX_TOKENS",
      "definition": "Maximum number of tokens the LLM can generate in a single response. Higher values allow longer answers but increase cost and latency. Typical: 512-1024 for concise answers, 2048-4096 for detailed explanations.",
      "category": "generation",
      "related": [],
      "links": [
        {
          "text": "OpenAI Token Limits",
          "href": "https://platform.openai.com/docs/guides/text-generation"
        },
        {
          "text": "Token Counting",
          "href": "https://platform.openai.com/tokenizer"
        }
      ],
      "badges": []
    },
    {
      "term": "Generation Model",
      "key": "GEN_MODEL",
      "definition": "Answer model. Local: qwen3-coder:14b via Ollama. Cloud: gpt-4o-mini, etc. Larger models cost more and can be slower; smaller ones are faster/cheaper.",
      "category": "generation",
      "related": [],
      "links": [
        {
          "text": "OpenAI Models",
          "href": "https://platform.openai.com/docs/models"
        },
        {
          "text": "Ollama API (GitHub)",
          "href": "https://github.com/ollama/ollama/blob/main/docs/api.md"
        }
      ],
      "badges": [
        {
          "text": "Affects latency",
          "class": "info"
        }
      ]
    },
    {
      "term": "Chat-Specific Model",
      "key": "GEN_MODEL_CHAT",
      "definition": "Override model for Chat interface only. Leave empty to use GEN_MODEL. Useful for using a different model (e.g., faster/cheaper) specifically for interactive chat.",
      "category": "generation",
      "related": [],
      "links": [
        {
          "text": "OpenAI Models",
          "href": "https://platform.openai.com/docs/models"
        },
        {
          "text": "Ollama API",
          "href": "https://github.com/ollama/ollama/blob/main/docs/api.md"
        }
      ],
      "badges": []
    },
    {
      "term": "CLI Channel Model",
      "key": "GEN_MODEL_CLI",
      "definition": "Override GEN_MODEL for CLI chat sessions only. Allows using different models for terminal vs web interface - e.g., faster models for CLI iteration, higher quality for production GUI. Useful for developer workflows where CLI is for quick testing and HTTP is for end users. If not set, uses GEN_MODEL.",
      "category": "generation",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Channel-specific",
          "class": "info"
        }
      ]
    },
    {
      "term": "HTTP Channel Model",
      "key": "GEN_MODEL_HTTP",
      "definition": "Override GEN_MODEL specifically for HTTP API requests (GUI, external API calls). Useful for serving different models to different channels - e.g., use gpt-4o for production HTTP but qwen-coder locally. If not set, falls back to GEN_MODEL. Example use case: cheaper models for public API, expensive models for internal tools.",
      "category": "generation",
      "related": [],
      "links": [
        {
          "text": "Model Selection",
          "href": "https://platform.openai.com/docs/models"
        }
      ],
      "badges": [
        {
          "text": "Channel-specific",
          "class": "info"
        }
      ]
    },
    {
      "term": "MCP Channel Model",
      "key": "GEN_MODEL_MCP",
      "definition": "Override GEN_MODEL for MCP tool invocations only. Use a lighter/cheaper model for MCP tools since tool calls are typically simpler than complex reasoning. Example: gpt-4o-mini for MCP, gpt-4o for main chat. Reduces costs when tools are called frequently (search, file operations, etc.). If not set, uses GEN_MODEL.",
      "category": "generation",
      "related": [],
      "links": [
        {
          "text": "Model Pricing",
          "href": "https://openai.com/api/pricing/"
        }
      ],
      "badges": [
        {
          "text": "Cost savings",
          "class": "info"
        },
        {
          "text": "Channel-specific",
          "class": "info"
        }
      ]
    },
    {
      "term": "Generation Max Retries",
      "key": "GEN_RETRY_MAX",
      "definition": "Number of retry attempts for failed LLM API calls due to rate limits, network errors, or transient failures. Higher values improve reliability but increase latency on failures. Typical: 2-3 retries.",
      "category": "generation",
      "related": [],
      "links": [
        {
          "text": "Retry Strategies",
          "href": "https://platform.openai.com/docs/guides/error-codes"
        },
        {
          "text": "Exponential Backoff",
          "href": "https://en.wikipedia.org/wiki/Exponential_backoff"
        }
      ],
      "badges": []
    },
    {
      "term": "Default Response Creativity",
      "key": "GEN_TEMPERATURE",
      "definition": "Global default temperature for generation. 0.0 = deterministic; small values (0.04-0.2) add slight variation in prose. Use per-model tuning for creative tasks vs. code answers.",
      "category": "generation",
      "related": [],
      "links": [
        {
          "text": "Sampling Controls",
          "href": "https://platform.openai.com/docs/guides/text-generation"
        },
        {
          "text": "Nucleus/Top‑p",
          "href": "https://en.wikipedia.org/wiki/Nucleus_sampling"
        }
      ],
      "badges": []
    },
    {
      "term": "Generation Timeout",
      "key": "GEN_TIMEOUT",
      "definition": "Maximum seconds to wait for LLM response before timing out. Prevents hanging on slow models or network issues. Increase for large models or slow connections. Typical: 30-120 seconds.",
      "category": "generation",
      "related": [],
      "links": [
        {
          "text": "Timeout Best Practices",
          "href": "https://platform.openai.com/docs/guides/rate-limits"
        },
        {
          "text": "HTTP Timeouts",
          "href": "https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Timeout"
        }
      ],
      "badges": []
    },
    {
      "term": "Top-P (Nucleus Sampling)",
      "key": "GEN_TOP_P",
      "definition": "Controls randomness via nucleus sampling (0.0-1.0). Lower values (0.1-0.5) make output more focused and deterministic. Higher values (0.9-1.0) increase creativity and diversity. Recommended: 0.9 for general use.",
      "category": "generation",
      "related": [],
      "links": [
        {
          "text": "Nucleus Sampling",
          "href": "https://platform.openai.com/docs/guides/text-generation/parameter-details"
        },
        {
          "text": "Top-P Explanation",
          "href": "https://en.wikipedia.org/wiki/Top-p_sampling"
        }
      ],
      "badges": []
    },
    {
      "term": "Golden Questions Path",
      "key": "GOLDEN_PATH",
      "definition": "Filesystem path to your golden questions JSON file (default: golden.json). Golden questions are curated query-answer pairs used to evaluate retrieval quality through automated testing. Format: [{\"query\": \"how does auth work?\", \"expected_file\": \"src/auth.py\"}]. Used by eval loop to measure metrics like Hit@K, MRR, and precision. Create golden questions from real user queries for best results.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Evaluation Metrics",
          "href": "https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)"
        }
      ],
      "badges": []
    },
    {
      "term": "Google API Key",
      "key": "GOOGLE_API_KEY",
      "definition": "API key for Google Gemini models and embedding endpoints (gemini-1.5-pro, gemini-1.5-flash, text-embedding-004). Required when using Google AI services. Create key at Google AI Studio. Gemini 1.5 Pro offers 2M token context window and multimodal capabilities. Flash variant is faster and cheaper. Great for code analysis with long context.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Get API Key",
          "href": "https://ai.google.dev/gemini-api/docs/api-key"
        },
        {
          "text": "Gemini Models",
          "href": "https://ai.google.dev/gemini-api/docs/models/gemini"
        },
        {
          "text": "API Quickstart",
          "href": "https://ai.google.dev/gemini-api/docs/quickstart"
        },
        {
          "text": "Pricing",
          "href": "https://ai.google.dev/pricing"
        }
      ],
      "badges": []
    },
    {
      "term": "Grafana Auth Mode",
      "key": "GRAFANA_AUTH_MODE",
      "definition": "Authentication method for Grafana. Options: \"token\" (API token), \"basic\" (username/password), \"none\" (public dashboards).",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Grafana Auth",
          "href": "https://grafana.com/docs/grafana/latest/setup-grafana/configure-security/"
        }
      ],
      "badges": []
    },
    {
      "term": "Grafana Auth Token",
      "key": "GRAFANA_AUTH_TOKEN",
      "definition": "API token or service account token for Grafana authentication. Generate in Grafana under Configuration > API Keys or Service Accounts.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Grafana API Keys",
          "href": "https://grafana.com/docs/grafana/latest/administration/api-keys/"
        }
      ],
      "badges": []
    },
    {
      "term": "Grafana Base URL",
      "key": "GRAFANA_BASE_URL",
      "definition": "Base URL for Grafana dashboard server (e.g., http://localhost:3000). Used for embedded dashboard iframes in GUI and direct links to monitoring dashboards.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Grafana Setup",
          "href": "https://grafana.com/docs/grafana/latest/setup-grafana/"
        }
      ],
      "badges": []
    },
    {
      "term": "Grafana Dashboard UID",
      "key": "GRAFANA_DASHBOARD_UID",
      "definition": "Unique identifier for default Grafana dashboard to display in GUI. Find UID in dashboard settings or URL (e.g., /d/abc123/dashboard-name -> UID is abc123).",
      "category": "ui",
      "related": [],
      "links": [
        {
          "text": "Dashboard UIDs",
          "href": "https://grafana.com/docs/grafana/latest/dashboards/"
        }
      ],
      "badges": []
    },
    {
      "term": "Chunk Entity Expansion Enabled",
      "key": "GRAPH_CHUNK_ENTITY_EXPANSION_ENABLED",
      "definition": "When graph mode is \"chunk\", expand from seed chunks via entity graph (IN_CHUNK links) to find related chunks. This combines chunk-based retrieval with entity relationships for better recall. When enabled, chunks connected to the same entities are included. Recommended: enabled for codebases with rich entity relationships.\n\nSweet spot: enabled for production systems. Entity expansion improves recall by finding chunks that share entities (functions, classes, modules) even if they're not directly connected. Disable if entity relationships are sparse or causing noise.\n\n• Enabled: Entity-based expansion, better recall, may introduce noise\n• Disabled: Chunk-only expansion, more focused results\n• Effect: Controls whether entity relationships influence chunk retrieval\n• Symptom if disabled: Related chunks sharing entities may be missed",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Neo4j Graph Traversal",
          "href": "https://neo4j.com/docs/neo4j-graphrag-python/current/user_guide_rag.html"
        },
        {
          "text": "Entity Relationships",
          "href": "https://neo4j.com/blog/developer/graph-traversal-graphrag-python-package"
        },
        {
          "text": "Graph Expansion",
          "href": "https://en.wikipedia.org/wiki/Graph_traversal"
        }
      ],
      "badges": [
        {
          "text": "Chunk Mode",
          "class": "info"
        }
      ]
    },
    {
      "term": "Chunk Entity Expansion Weight",
      "key": "GRAPH_CHUNK_ENTITY_EXPANSION_WEIGHT",
      "definition": "Blend weight for entity-expansion scores relative to seed chunk scores when entity expansion is enabled. Higher values (0.7-1.0) favor entity-expanded chunks, lower values (0.3-0.6) favor seed chunks. Recommended: 0.8 for balanced blending.\n\nSweet spot: 0.8 for production systems. Use 0.6-0.7 when seed chunks are more reliable. Use 0.9-1.0 when entity relationships are highly trustworthy. Use 0.3-0.5 when entity expansion is experimental or noisy.\n\n• Range: 0.0-1.0 (typical: 0.5-0.9)\n• Seed-focused: 0.5-0.6 (favor seed chunks)\n• Balanced: 0.7-0.8 (recommended)\n• Entity-focused: 0.9-1.0 (favor entity expansion)\n• Effect: Higher = more weight to entity-expanded chunks\n• Symptom too low: Entity expansion underutilized\n• Symptom too high: Seed chunks undervalued",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Score Blending",
          "href": "https://neo4j.com/docs/neo4j-graphrag-python/current/user_guide_rag.html"
        },
        {
          "text": "Graph Traversal",
          "href": "https://neo4j.com/blog/developer/graph-traversal-graphrag-python-package"
        },
        {
          "text": "Weighted Fusion",
          "href": "https://en.wikipedia.org/wiki/Data_fusion"
        }
      ],
      "badges": [
        {
          "text": "Advanced",
          "class": "info"
        }
      ]
    },
    {
      "term": "Chunk Neighbor Window",
      "key": "GRAPH_CHUNK_NEIGHBOR_WINDOW",
      "definition": "When graph mode is \"chunk\", include up to N adjacent chunks (NEXT_CHUNK relationships) around each seed hit. Higher values (2-5) include more context but may introduce noise. Lower values (0-1) are more focused. Recommended: 1 for balanced context.\n\nSweet spot: 1 for production systems. Use 0 for minimal context (seed chunks only). Use 2-3 when you need more surrounding context (e.g., finding complete function implementations). Use 4-5 only for exploratory queries where completeness matters.\n\n• Range: 0-10 (typical: 0-3)\n• Minimal: 0 (seed chunks only)\n• Balanced: 1 (recommended)\n• Extended context: 2-3 (more surrounding chunks)\n• Effect: Higher = more adjacent chunks included, better context, may introduce noise\n• Symptom too low: Insufficient context around seed hits\n• Symptom too high: Too much noise from distant chunks",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Neo4j Graph Traversal",
          "href": "https://neo4j.com/docs/neo4j-graphrag-python/current/user_guide_rag.html"
        },
        {
          "text": "Graph Chunk Relationships",
          "href": "https://neo4j.com/blog/developer/graph-traversal-graphrag-python-package"
        },
        {
          "text": "Context Window",
          "href": "https://en.wikipedia.org/wiki/Context_window"
        }
      ],
      "badges": [
        {
          "text": "Chunk Mode",
          "class": "info"
        }
      ]
    },
    {
      "term": "Chunk Seed Overfetch Multiplier",
      "key": "GRAPH_CHUNK_SEED_OVERFETCH",
      "definition": "When graph mode is \"chunk\" and Neo4j uses a shared database, overfetch seed hits before filtering by corpus_id. This compensates for shared database queries where corpus filtering happens after retrieval. Higher values (10-20) ensure sufficient results after filtering but increase query cost. Lower values (5-10) are more efficient but may return fewer results.\n\nSweet spot: 10 for shared database mode. Use 5-8 for per-corpus databases (no filtering needed). Use 15-20 when corpus filtering is very selective (small corpus in large database).\n\n• Range: 1-50 (typical: 5-20)\n• Per-corpus DB: 5-8 (minimal overfetch)\n• Shared DB: 10 (recommended)\n• Selective filtering: 15-20 (high overfetch)\n• Effect: Higher = more seed candidates, better recall after filtering, higher cost\n• Symptom too low: Insufficient results after corpus filtering\n• Symptom too high: Unnecessary query overhead",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Neo4j Multi-Database",
          "href": "https://assets.neo4j.com/Official-Materials/Multi+DB+Considerations.pdf"
        },
        {
          "text": "Database Isolation",
          "href": "https://neo4j.com/docs/operations-manual/current/scalability/concepts/"
        },
        {
          "text": "Query Optimization",
          "href": "https://neo4j.com/docs/cypher-manual/current/query-tuning/"
        }
      ],
      "badges": [
        {
          "text": "Performance",
          "class": "info"
        },
        {
          "text": "Shared DB",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Include Communities",
      "key": "GRAPH_INCLUDE_COMMUNITIES",
      "definition": "Enable community-based expansion in graph search. When enabled, the system uses community detection algorithms (e.g., Louvain) to identify clusters of related nodes and expands search to include entire communities. This improves recall for related concepts but may introduce noise. Recommended: enabled for entity mode, optional for chunk mode.\n\nSweet spot: enabled for entity mode, disabled for chunk mode. Community expansion works best with entity-based graphs where structural clusters are meaningful. For chunk mode, neighbor expansion is usually sufficient.\n\n• Enabled: Community-based expansion, better recall, may introduce noise\n• Disabled: Direct neighbor expansion only, more focused results\n• Effect: Controls whether community detection influences traversal\n• Symptom if disabled: Related concepts in same community may be missed",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Louvain Algorithm",
          "href": "https://neo4j.com/docs/graph-data-science/current/algorithms/louvain"
        },
        {
          "text": "Community Detection",
          "href": "https://neo4j.com/docs/graph-data-science/current/algorithms/community/"
        },
        {
          "text": "Community Detection Algorithms",
          "href": "https://en.wikipedia.org/wiki/Community_structure"
        }
      ],
      "badges": [
        {
          "text": "Advanced",
          "class": "info"
        }
      ]
    },
    {
      "term": "Graph Max Hops",
      "key": "GRAPH_MAX_HOPS",
      "definition": "Maximum number of graph traversal hops from seed nodes. Each hop expands the search to connected nodes (chunks, entities, relationships). Higher values (3-5) find more distant relationships but increase query latency and may introduce noise. Lower values (1-2) are faster and more focused. Recommended: 2 for balanced performance.\n\nSweet spot: 2 for production systems. Use 1 for fast, focused traversal (immediate neighbors only). Use 3-4 when you need to find distant relationships or explore deep code structures. Use 5 only for exploratory queries where completeness matters more than speed.\n\n• Range: 1-5 (typical: 1-3)\n• Focused: 1 (immediate neighbors only)\n• Balanced: 2 (recommended)\n• Deep exploration: 3-4 (distant relationships)\n• Effect: Higher = more relationships explored, better recall, higher latency\n• Symptom too low: Relevant connections missed\n• Symptom too high: Slower queries, noise introduced",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Neo4j Graph Traversal",
          "href": "https://neo4j.com/docs/neo4j-graphrag-python/current/user_guide_rag.html"
        },
        {
          "text": "Graph Traversal Depth",
          "href": "https://neo4j.com/blog/developer/graph-traversal-graphrag-python-package"
        },
        {
          "text": "Graph Algorithms",
          "href": "https://en.wikipedia.org/wiki/Graph_traversal"
        }
      ],
      "badges": [
        {
          "text": "Performance",
          "class": "info"
        }
      ]
    },
    {
      "term": "Graph Search Enabled",
      "key": "GRAPH_SEARCH_ENABLED",
      "definition": "Enable or disable graph-based search using Neo4j. When enabled, queries traverse the knowledge graph to find related chunks through entity relationships, code structure (AST), and community detection. When disabled, only vector and sparse search are used. Recommended: enabled for codebases with rich structure and relationships.\n\nSweet spot: enabled for production systems with graph indexing. Graph search excels at finding related code through structural relationships (imports, calls, inheritance). Disable if Neo4j is unavailable, graph indexing is incomplete, or you want pure vector/sparse retrieval.\n\n• Enabled: Full tri-brid retrieval (vector + sparse + graph)\n• Disabled: Dual-mode retrieval (vector + sparse only)\n• Effect: Controls whether graph traversal contributes to results\n• Symptom if disabled: Structural relationships and entity connections may be missed",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Neo4j GraphRAG",
          "href": "https://neo4j.com/blog/what-is-graphrag/"
        },
        {
          "text": "GraphRAG User Guide",
          "href": "https://neo4j.com/docs/neo4j-graphrag-python/current/user_guide_rag.html"
        },
        {
          "text": "Graph Traversal",
          "href": "https://neo4j.com/blog/developer/graph-traversal-graphrag-python-package"
        }
      ],
      "badges": [
        {
          "text": "Core Setting",
          "class": "info"
        }
      ]
    },
    {
      "term": "Graph Search Mode",
      "key": "GRAPH_SEARCH_MODE",
      "definition": "Graph retrieval strategy: \"chunk\" uses lexical chunk nodes with Neo4j vector index for semantic chunk search, \"entity\" uses the legacy code-entity graph. Chunk mode (recommended) combines vector similarity with graph traversal for better relevance. Entity mode uses structural relationships between code entities (functions, classes, modules).\n\nSweet spot: \"chunk\" for most use cases. Chunk mode provides better integration with vector search and more relevant results. Use \"entity\" only for legacy compatibility or when entity-based traversal is specifically needed.\n\n• Chunk mode: Modern approach, integrates with vector search, better relevance\n• Entity mode: Legacy approach, entity-based traversal, structural relationships\n• Effect: Determines how graph traversal finds related chunks\n• Symptom wrong mode: Suboptimal results, missing relevant connections",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Neo4j GraphRAG",
          "href": "https://neo4j.com/docs/neo4j-graphrag-python/current/user_guide_rag.html"
        },
        {
          "text": "Graph Retrieval Modes",
          "href": "https://neo4j.com/blog/developer/graphrag-field-guide-rag-patterns"
        },
        {
          "text": "Graph Traversal",
          "href": "https://neo4j.com/blog/developer/graph-traversal-graphrag-python-package"
        }
      ],
      "badges": [
        {
          "text": "Configuration",
          "class": "info"
        }
      ]
    },
    {
      "term": "Graph Search Top-K",
      "key": "GRAPH_SEARCH_TOP_K",
      "definition": "Number of candidate results to retrieve from Neo4j graph search before fusion. Higher values (40-100) improve recall for graph-based relationships but increase query latency. Lower values (15-30) are faster but may miss relevant connections. Must be >= FINAL_K. Recommended: 30 for balanced performance.\n\nSweet spot: 30 for production systems. Use 40-50 when graph relationships are critical (e.g., finding code that calls or imports specific functions). Use 15-20 for cost-sensitive scenarios or when graph indexing is sparse.\n\n• Range: 5-100 (typical: 20-50)\n• Balanced: 30 (recommended)\n• High recall: 40-50 (relationship-heavy queries)\n• Cost-sensitive: 15-20 (faster, lower cost)\n• Effect: Higher = more graph candidates, better recall, higher latency\n• Symptom too low: Relevant graph connections missed\n• Symptom too high: Slower queries, diminishing returns",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Neo4j GraphRAG",
          "href": "https://neo4j.com/docs/neo4j-graphrag-python/current/user_guide_rag.html"
        },
        {
          "text": "Graph Traversal",
          "href": "https://neo4j.com/blog/developer/graph-traversal-graphrag-python-package"
        },
        {
          "text": "Top-K Retrieval",
          "href": "https://en.wikipedia.org/wiki/Nearest_neighbor_search#k-nearest_neighbors"
        }
      ],
      "badges": [
        {
          "text": "Affects latency",
          "class": "info"
        },
        {
          "text": "Graph relationships",
          "class": "info"
        }
      ]
    },
    {
      "term": "Graph Weight",
      "key": "GRAPH_WEIGHT",
      "definition": "Weight assigned to graph traversal results in tri-brid fusion. Higher values prioritize code relationships like call graphs and imports. Range: 0.0-1.0. Default: 0.3. Helps find related code across files.",
      "category": "retrieval",
      "related": [
        "Tri-Brid Fusion",
        "Vector Weight",
        "Sparse Weight",
        "Max Hops"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Greedy Fallback Target (Chars)",
      "key": "GREEDY_FALLBACK_TARGET",
      "definition": "Target chunk size (in characters) for greedy fallback chunking when AST-based chunking fails or encounters oversized logical units. Greedy chunking splits text at line boundaries to hit this approximate size. Used as a safety mechanism when: (1) file syntax is unparseable, (2) a single function/class exceeds MAX_CHUNK_SIZE, (3) non-code files (markdown, text) are indexed.\n\nSweet spot: 500-800 characters for fallback chunks. This roughly corresponds to 100-150 tokens, providing reasonable context when AST chunking isn't possible. Use 800-1200 for larger fallback chunks (more context but less precise boundaries). Use 300-500 for smaller fallback chunks (tighter boundaries, less context). Greedy chunking is less semantic than AST chunking - it splits at line breaks regardless of code structure.\n\nExample: If a 3000-char function exceeds MAX_CHUNK_SIZE and can't be split structurally, greedy fallback divides it into ~4 chunks of ~750 chars each (based on GREEDY_FALLBACK_TARGET=800). This preserves some of the function in each chunk. Greedy fallback is rare in well-formed code but essential for robustness.\n\n• Range: 300-1500 characters (typical)\n• Small: 300-500 chars (tight boundaries, less context)\n• Balanced: 500-800 chars (recommended, ~100-150 tokens)\n• Large: 800-1200 chars (more context per fallback chunk)\n• Very large: 1200-1500 chars (maximum context, rare use)\n• When used: Syntax errors, oversized units, non-code files",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Chunking Robustness",
          "href": "https://github.com/yilinjz/astchunk#fallback-modes"
        },
        {
          "text": "Greedy Chunking",
          "href": "https://en.wikipedia.org/wiki/Chunking_(psychology)"
        }
      ],
      "badges": [
        {
          "text": "Fallback mechanism",
          "class": "info"
        },
        {
          "text": "Requires reindex",
          "class": "reindex"
        }
      ]
    },
    {
      "term": "UI Public Directory",
      "key": "GUI_DIR",
      "definition": "Directory for shared UI assets (for example: models.json) used by /api/models and the frontend. Defaults to ./web/public. Point this to a writable volume if you keep catalogs in sync at runtime; the React app reads from the same source.",
      "category": "ui",
      "related": [],
      "links": [
        {
          "text": "Static Files (FastAPI)",
          "href": "https://fastapi.tiangolo.com/tutorial/static-files/"
        }
      ],
      "badges": [
        {
          "text": "Recommended",
          "class": "info"
        }
      ]
    },
    {
      "term": "Server Host",
      "key": "HOST",
      "definition": "Network interface for the HTTP server to bind to when running serve_rag. Use 0.0.0.0 for all interfaces (accessible from network), 127.0.0.1 for localhost only (secure, dev mode).",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Network Interfaces",
          "href": "https://en.wikipedia.org/wiki/Network_interface"
        },
        {
          "text": "Localhost vs 0.0.0.0",
          "href": "https://stackoverflow.com/questions/20778771/what-is-the-difference-between-0-0-0-0-127-0-0-1-and-localhost"
        }
      ],
      "badges": []
    },
    {
      "term": "Hydration Max Chars",
      "key": "HYDRATION_MAX_CHARS",
      "definition": "Maximum characters to load per chunk when hydrating results with code content. Prevents huge chunks from bloating responses and consuming excessive memory. 0 = no limit (may cause memory issues with large files). Recommended: 2000 for general use, 1000 for memory-constrained environments, 5000 for detailed code review. Chunks larger than this limit are truncated.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Text Truncation",
          "href": "https://en.wikipedia.org/wiki/Truncation"
        }
      ],
      "badges": [
        {
          "text": "Performance",
          "class": "info"
        }
      ]
    },
    {
      "term": "Hydration Mode",
      "key": "HYDRATION_MODE",
      "definition": "Controls when full code is loaded from chunks.jsonl. \"Lazy\" (recommended) loads code after retrieval, providing full context with minimal memory overhead. \"None\" returns only metadata (file path, line numbers) - fastest but no code content. Use \"none\" for testing retrieval quality or when you only need file locations, not actual code.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Lazy Loading",
          "href": "https://en.wikipedia.org/wiki/Lazy_loading"
        }
      ],
      "badges": [
        {
          "text": "Lazy Recommended",
          "class": "info"
        }
      ]
    },
    {
      "term": "Community Detection",
      "key": "INCLUDE_COMMUNITIES",
      "definition": "Enable community-based expansion in graph search. Communities are clusters of closely related code entities detected using algorithms like Louvain. Helps find related functionality even without direct edges.",
      "category": "retrieval",
      "related": [
        "Max Hops",
        "Graph Weight",
        "Community Algorithm"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Code Indexing",
      "key": "INDEXING",
      "definition": "Indexing turns a corpus folder into the data structures TriBridRAG can search. It’s corpus-scoped: each corpus has its own storage, graph, and configuration.\n\nDuring indexing, TriBridRAG typically performs:\n• Chunking: split files into searchable chunks (AST/greedy/hybrid)\n• Dense index: compute embeddings and store vectors (pgvector)\n• Sparse index: build keyword search data (PostgreSQL FTS / BM25-style)\n• Optional enrichment: generate chunk summaries/metadata\n• Optional graph build: extract entities/relationships into Neo4j\n\nMany settings in this tab change how the index is built. Those changes only take effect after reindexing the corpus.\n\n• If you change embeddings, chunking, or sparse tokenization: reindex is required\n• If you only change query-time weights/thresholds: reindex is NOT required",
      "category": "general",
      "related": [
        "Chunking Strategy",
        "Embedding Provider",
        "BM25 Tokenizer",
        "Neo4j Connection URI"
      ],
      "links": [
        {
          "text": "pgvector",
          "href": "https://github.com/pgvector/pgvector"
        },
        {
          "text": "PostgreSQL Full Text Search",
          "href": "https://www.postgresql.org/docs/current/textsearch.html"
        }
      ],
      "badges": [
        {
          "text": "Indexing",
          "class": "info"
        },
        {
          "text": "Often requires reindex",
          "class": "reindex"
        }
      ]
    },
    {
      "term": "Indexing Batch Size",
      "key": "INDEXING_BATCH_SIZE",
      "definition": "Number of chunks to process in parallel during the indexing pipeline (chunking, enrichment, embedding, Qdrant upload). Higher values (100-500) maximize throughput on fast networks and powerful machines but increase memory usage and risk batch failures. Lower values (20-50) are more stable and provide better progress visibility. If indexing crashes with OOM or connection errors, reduce this. For large repos (100k+ files), use higher values for efficiency.\n\nRecommended: 100-200 for normal repos, 50-100 for large repos or slow connections, 500+ for small repos on powerful hardware.",
      "category": "embedding",
      "related": [],
      "links": [
        {
          "text": "Batch Processing",
          "href": "https://en.wikipedia.org/wiki/Batch_processing"
        },
        {
          "text": "Qdrant Upload Performance",
          "href": "https://qdrant.tech/documentation/guides/bulk-upload/"
        }
      ],
      "badges": [
        {
          "text": "Performance tuning",
          "class": "info"
        },
        {
          "text": "Memory sensitive",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Indexing Process",
      "key": "INDEXING_PROCESS",
      "definition": "Indexing prepares your code for retrieval: it chunks files, builds a BM25 sparse index, optionally generates dense embeddings, and writes vectors to Qdrant. Re‑run after significant code changes to keep answers fresh.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Okapi BM25",
          "href": "https://en.wikipedia.org/wiki/Okapi_BM25"
        },
        {
          "text": "Qdrant Docs",
          "href": "https://qdrant.tech/documentation/"
        }
      ],
      "badges": []
    },
    {
      "term": "Indexing Workers",
      "key": "INDEXING_WORKERS",
      "definition": "Number of parallel worker threads for CPU-intensive indexing tasks (file parsing, chunking, BM25 indexing). Higher values (4-16) utilize multi-core CPUs better and speed up indexing significantly. Lower values (1-2) reduce CPU load but increase indexing time. Set based on available CPU cores - typically use cores-1 or cores-2 to leave headroom for OS/other processes. For Docker/containers, ensure resource limits allow multiple workers.\n\nRecommended: 4-8 for most systems, 1-2 for low-power machines or containers with CPU limits, 12-16 for powerful servers.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Parallel Processing",
          "href": "https://en.wikipedia.org/wiki/Parallel_computing"
        },
        {
          "text": "Python ThreadPoolExecutor",
          "href": "https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor"
        },
        {
          "text": "Docker CPU Limits",
          "href": "https://docs.docker.com/engine/containers/resource_constraints/#cpu"
        }
      ],
      "badges": [
        {
          "text": "CPU utilization",
          "class": "info"
        },
        {
          "text": "Faster indexing",
          "class": "info"
        }
      ]
    },
    {
      "term": "Excluded Extensions",
      "key": "INDEX_EXCLUDED_EXTS",
      "definition": "Comma-separated file extensions to skip during indexing (e.g., \".png,.jpg,.pdf,.zip\"). Prevents indexing binary files, images, or non-code assets. Reduces index size and improves relevance.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Gitignore Patterns",
          "href": "https://git-scm.com/docs/gitignore"
        },
        {
          "text": "File Extensions",
          "href": "https://en.wikipedia.org/wiki/Filename_extension"
        }
      ],
      "badges": []
    },
    {
      "term": "Indexing Logs Terminal",
      "key": "INDEX_LOGS_TERMINAL",
      "definition": "Open the sliding terminal to stream raw indexer output with the exact repo/skip_dense/enrich settings used for the run.",
      "category": "infrastructure",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Live output",
          "class": "info"
        }
      ]
    },
    {
      "term": "Max File Size (MB)",
      "key": "INDEX_MAX_FILE_SIZE_MB",
      "definition": "Skip files larger than this size (in megabytes) during indexing. Prevents huge generated files or vendor bundles from bloating the index. Typical: 1-5 MB for source code, higher for docs.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "File Size Management",
          "href": "https://en.wikipedia.org/wiki/File_size"
        }
      ],
      "badges": []
    },
    {
      "term": "Index Max Workers",
      "key": "INDEX_MAX_WORKERS",
      "definition": "Maximum number of parallel workers used during indexing. Increase to speed up indexing on multi‑core machines; decrease if you observe system contention. A good starting point is CPU cores − 1.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "concurrent.futures",
          "href": "https://docs.python.org/3/library/concurrent.futures.html"
        },
        {
          "text": "multiprocessing",
          "href": "https://docs.python.org/3/library/multiprocessing.html"
        }
      ],
      "badges": [
        {
          "text": "Performance",
          "class": "info"
        }
      ]
    },
    {
      "term": "Index Profiles",
      "key": "INDEX_PROFILES",
      "definition": "Preset configurations for common workflows: shared (BM25‑only, fast), full (BM25 + embeddings, best quality), dev (small subset). Profiles change multiple parameters at once to match your goal.",
      "category": "infrastructure",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Convenience",
          "class": "info"
        }
      ]
    },
    {
      "term": "Validation Error",
      "key": "INDEX_VALIDATION_ERROR",
      "definition": "Configuration issues that must be fixed before indexing can proceed. Common errors: embedding dimension mismatch with existing index, missing API keys for cloud providers, chunk overlap exceeding chunk size. Fix these issues and try again.",
      "category": "infrastructure",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Blocks indexing",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Validation Warning",
      "key": "INDEX_VALIDATION_WARNING",
      "definition": "Configuration may reduce retrieval quality but indexing can still proceed. Common warnings: skip dense vectors enabled (BM25-only mode), very large chunk sizes (>2000), small chunks with AST strategy. Review and confirm to proceed.",
      "category": "infrastructure",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Quality impact",
          "class": "info"
        }
      ]
    },
    {
      "term": "Auto-Generate Keywords",
      "key": "KEYWORDS_AUTO_GENERATE",
      "definition": "Automatically extract repository keywords from code and documentation during indexing (1=yes, 0=no). When enabled, the system analyzes class names, function names, docstrings, and comments to build a keyword set for routing. This supplements manually-defined keywords in repos.json. Auto-generation is useful for new repos or when you don't know what routing keywords to use. Disable if you prefer full manual control via repos.json.\n\nRecommended: 1 for automatic keyword discovery, 0 for strict manual control.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Multi-repo feature",
          "class": "info"
        },
        {
          "text": "Complements manual keywords",
          "class": "info"
        }
      ]
    },
    {
      "term": "Keywords Boost",
      "key": "KEYWORDS_BOOST",
      "definition": "Score boost multiplier applied to search results that match corpus keywords. Higher values (1.5-2.0) strongly favor keyword matches, lower values (1.1-1.3) provide mild preference. The boost is multiplied with the base retrieval score. Recommended: 1.3 for balanced keyword preference.\n\nSweet spot: 1.3 for balanced systems. Use 1.1-1.2 for mild keyword preference (keyword matches slightly favored). Use 1.5-2.0 when keywords are highly reliable indicators of relevance. Use 2.5+ only when keywords are definitive relevance signals.\n\n• Range: 1.0-3.0 (typical: 1.1-2.0)\n• Mild boost: 1.1-1.2 (slight preference)\n• Balanced: 1.3 (recommended)\n• Strong boost: 1.5-2.0 (strong preference)\n• Effect: Higher = more weight to keyword matches\n• Symptom too low: Keyword matches undervalued\n• Symptom too high: Keyword matches dominate, other signals ignored",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Score Boosting",
          "href": "https://en.wikipedia.org/wiki/Relevance_(information_retrieval)"
        },
        {
          "text": "TF-IDF Scoring",
          "href": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
        },
        {
          "text": "Keyword Extraction",
          "href": "https://github.com/airKlizz/CustomizedTFIDF"
        }
      ],
      "badges": [
        {
          "text": "Scoring",
          "class": "info"
        }
      ]
    },
    {
      "term": "Keywords Max Per Repo",
      "key": "KEYWORDS_MAX_PER_REPO",
      "definition": "Maximum number of repository-specific keywords to extract and store for query routing in multi-repo setups. Higher values (100-200) capture more routing signals but increase memory and may introduce noise. Lower values (20-50) keep routing focused on core concepts. Keywords are extracted from code, docs, and enrichment metadata. Used by the router to determine which repositories are most relevant for a given query.\n\nRecommended: 50-100 for most repos, 150-200 for large multi-domain codebases, 20-30 for focused microservices.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Multi-repo only",
          "class": "info"
        },
        {
          "text": "Auto-generated",
          "class": "info"
        }
      ]
    },
    {
      "term": "Keywords Min Frequency",
      "key": "KEYWORDS_MIN_FREQ",
      "definition": "Minimum term frequency required for a keyword to be included. Terms must appear at least this many times in the corpus to be considered. Higher values (5-10) ensure keywords are common enough to be meaningful, lower values (1-3) allow rare but distinctive terms. Recommended: 3 for balanced filtering.\n\nSweet spot: 3 for most corpora. Use 1-2 when you want to include rare but distinctive terms (e.g., unique function names). Use 5-7 when you want only common, well-established keywords. Use 10+ only for very large corpora.\n\n• Range: 1-10 (typical: 2-5)\n• Rare terms: 1-2 (include distinctive rare terms)\n• Balanced: 3 (recommended)\n• Common terms: 5-7 (only well-established keywords)\n• Effect: Higher = fewer keywords, more common terms only\n• Symptom too low: Rare, potentially noisy keywords included\n• Symptom too high: Important distinctive keywords filtered out",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "TF-IDF Keyword Extraction",
          "href": "https://github.com/tonifuc3m/document_selection_tfidf"
        },
        {
          "text": "Term Frequency",
          "href": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
        },
        {
          "text": "Keyword Extraction",
          "href": "https://github.com/MOoTawaty/TF-IDF-keywords-extraction"
        }
      ],
      "badges": [
        {
          "text": "Keyword Extraction",
          "class": "info"
        }
      ]
    },
    {
      "term": "Keywords Refresh (Hours)",
      "key": "KEYWORDS_REFRESH_HOURS",
      "definition": "How often (in hours) to regenerate repository keywords from code for improved query routing. Lower values keep keywords fresh but increase indexing overhead. Typical: 24-168 hours (1-7 days).",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Keyword Extraction",
          "href": "https://en.wikipedia.org/wiki/Keyword_extraction"
        }
      ],
      "badges": []
    },
    {
      "term": "LangChain API Key",
      "key": "LANGCHAIN_API_KEY",
      "definition": "Alternate env var name used by LangSmith. Treat as an alias for LANGSMITH_API_KEY (external provider). TriBridRAG does not currently export traces to LangSmith.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "LangSmith Setup",
          "href": "https://docs.smith.langchain.com/"
        }
      ],
      "badges": []
    },
    {
      "term": "LangChain Endpoint",
      "key": "LANGCHAIN_ENDPOINT",
      "definition": "LangSmith API endpoint URL (external provider). Stored in config field tracing.langchain_endpoint. Reserved for future integration.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "LangSmith API",
          "href": "https://docs.smith.langchain.com/"
        }
      ],
      "badges": []
    },
    {
      "term": "LangChain Legacy",
      "key": "LANGCHAIN_LEGACY",
      "definition": "DEPRECATED: Legacy/internal environment variable for LangChain tracing metadata specific to the \"tribrid\" repo. Repo-specific tracing keys don't work well with modern LangSmith. Modern approach: use LANGCHAIN_TRACING_V2=true + LANGCHAIN_PROJECT in your environment for proper tracing across all repos.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "LangChain Project",
      "key": "LANGCHAIN_PROJECT",
      "definition": "Project name for organizing traces in LangSmith (external provider). Stored in config field tracing.langchain_project. Reserved for future integration.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "LangSmith Projects",
          "href": "https://docs.smith.langchain.com/tracing/faq#how-do-i-use-projects"
        }
      ],
      "badges": []
    },
    {
      "term": "LangChain Tracing",
      "key": "LANGCHAIN_TRACING_V2",
      "definition": "Reserved for future LangSmith integration (v2 tracing protocol). TriBridRAG currently captures local, in-memory request traces for UI preview and does not export traces to LangSmith yet.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "LangSmith Setup",
          "href": "https://docs.smith.langchain.com/"
        },
        {
          "text": "Tracing Guide",
          "href": "https://docs.smith.langchain.com/tracing"
        },
        {
          "text": "How to Enable",
          "href": "https://docs.smith.langchain.com/tracing/faq#how-do-i-turn-on-tracing"
        }
      ],
      "badges": [
        {
          "text": "Requires API key",
          "class": "info"
        }
      ]
    },
    {
      "term": "LangGraph Final K",
      "key": "LANGGRAPH_FINAL_K",
      "definition": "Documents retrieved for LangGraph pipeline in /answer. Separate from retrieval FINAL_K. Higher = more context, higher cost. Typical: 10–30.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "LangGraph",
          "href": "https://langchain-ai.github.io/langgraph/"
        }
      ],
      "badges": []
    },
    {
      "term": "LangGraph Max Query Rewrites",
      "key": "LANGGRAPH_MAX_QUERY_REWRITES",
      "definition": "Number of query rewrites used inside the LangGraph answer pipeline (/answer). Separate from MAX_QUERY_REWRITES used by general multi-query retrieval. Higher values improve recall but increase latency and LLM cost. Typical: 2-4.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "LangGraph",
          "href": "https://langchain-ai.github.io/langgraph/"
        },
        {
          "text": "Multi‑Query RAG (paper)",
          "href": "https://arxiv.org/abs/2305.14283"
        }
      ],
      "badges": [
        {
          "text": "LangGraph only",
          "class": "info"
        },
        {
          "text": "Higher cost",
          "class": "warn"
        }
      ]
    },
    {
      "term": "LangSmith API Key",
      "key": "LANGSMITH_API_KEY",
      "definition": "API key for LangSmith (external provider). TriBridRAG does not currently export traces to LangSmith; the UI only checks whether this key is set in your environment.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "LangSmith API Keys",
          "href": "https://docs.smith.langchain.com/"
        },
        {
          "text": "Get API Key",
          "href": "https://smith.langchain.com/settings"
        }
      ],
      "badges": []
    },
    {
      "term": "LangTrace API Host",
      "key": "LANGTRACE_API_HOST",
      "definition": "LangTrace API endpoint host (optional). Stored in config field tracing.langtrace_api_host (and surfaced as LANGTRACE_API_HOST in env exports). Reserved for future external trace export.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Langtrace Docs",
          "href": "https://docs.langtrace.ai/"
        }
      ],
      "badges": []
    },
    {
      "term": "LangTrace API Key",
      "key": "LANGTRACE_API_KEY",
      "definition": "API key for LangTrace (external provider). TriBridRAG does not currently export traces to LangTrace; the UI only checks whether this key is set in your environment.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Langtrace Setup",
          "href": "https://docs.langtrace.ai/"
        }
      ],
      "badges": []
    },
    {
      "term": "LangTrace Project ID",
      "key": "LANGTRACE_PROJECT_ID",
      "definition": "Project identifier for LangTrace (optional). Stored in config field tracing.langtrace_project_id (and surfaced as LANGTRACE_PROJECT_ID in env exports). Reserved for future external trace export.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Langtrace Projects",
          "href": "https://docs.langtrace.ai/"
        }
      ],
      "badges": []
    },
    {
      "term": "Latency P99 Threshold (s)",
      "key": "LATENCY_P99_THRESHOLD",
      "definition": "Threshold for alerting on tail latency: if the 99th percentile (p99) of request latency over the monitoring window exceeds this value, an alert fires.\n\np99 focuses on the slowest 1% of requests. It’s useful for catching “everything is fine… except when it isn’t” situations caused by cold starts, slow LLM calls, database contention, or network hiccups.\n\nChoose a threshold that matches your workload. If most requests include an LLM call, a higher p99 is reasonable; for retrieval-only endpoints, p99 should be much lower.\n\n• Units: seconds\n• Typical dev: 10–30s (looser)\n• Typical prod: 2–10s (depends on LLM usage)\n• Symptom too low: noisy alerts during normal LLM slowness\n• Symptom too high: tail latency regressions go unnoticed",
      "category": "general",
      "related": [
        "Error Rate Threshold (%)",
        "Timeout Errors (per 5 min)"
      ],
      "links": [
        {
          "text": "Grafana Alerting",
          "href": "https://grafana.com/docs/grafana/latest/alerting/"
        },
        {
          "text": "Percentile",
          "href": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "badges": [
        {
          "text": "Performance",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Layer Bonus (GUI)",
      "key": "LAYER_BONUS_GUI",
      "definition": "Score boost applied to chunks from GUI/frontend layers when query intent is classified as UI-related. Part of the multi-layer architecture routing system. When users ask \"how does the settings page work?\" or \"where is the login button?\", chunks from directories like frontend/, components/, views/ receive this additive bonus during reranking. Higher values (0.08-0.15) strongly bias toward frontend code. Lower values (0.03-0.06) provide subtle guidance.\n\nSweet spot: 0.06-0.10 for production systems with clear frontend/backend separation. Use 0.10-0.15 for strict layer routing when your architecture is well-organized and layer detection is accurate. Use 0.03-0.06 for loose guidance when layer boundaries are fuzzy. This bonus is only applied when intent classification detects UI/frontend intent from the query.\n\nWorks with repos.json layer_bonuses configuration, which maps intent types to directory patterns. Example: \"ui\" intent boosts frontend/, components/, views/. Combine with LAYER_BONUS_RETRIEVAL for multi-tier architectures (API, service, data layers). Intent detection uses keyword matching and optional LLM classification.\n\n• Range: 0.03-0.15 (typical)\n• Subtle guidance: 0.03-0.06\n• Balanced: 0.06-0.10 (recommended)\n• Strong routing: 0.10-0.15\n• Applied: Only when query intent = UI/frontend\n• Requires: repos.json layer_bonuses configuration",
      "category": "ui",
      "related": [],
      "links": [
        {
          "text": "Architecture-Aware Retrieval",
          "href": "https://arxiv.org/abs/2312.10997"
        }
      ],
      "badges": [
        {
          "text": "Advanced RAG tuning",
          "class": "info"
        },
        {
          "text": "Multi-layer architectures",
          "class": "info"
        }
      ]
    },
    {
      "term": "Layer Bonus (Retrieval)",
      "key": "LAYER_BONUS_RETRIEVAL",
      "definition": "Score boost applied to chunks from backend/API/service layers when query intent is classified as retrieval or data-related. Complements LAYER_BONUS_GUI for multi-tier architecture routing. When users ask \"how do we fetch user data?\" or \"where is the search API?\", chunks from api/, services/, models/, controllers/ receive this bonus during reranking. Helps route queries to the right architectural layer.\n\nSweet spot: 0.06-0.10 for production systems. Use 0.10-0.15 for strong backend routing when API layer is clearly separated. Use 0.03-0.06 for subtle hints when boundaries are less clear. This bonus applies when intent detection identifies backend/API/data queries via keywords like \"fetch\", \"query\", \"API\", \"endpoint\", \"database\".\n\nConfigure layer patterns in repos.json layer_bonuses: map \"retrieval\" intent to api/, routes/, controllers/, services/, etc. The intent classifier examines query terms and (optionally) uses an LLM to categorize intent. Multiple bonuses can apply simultaneously - a query about \"user profile API\" might trigger both LAYER_BONUS_GUI and LAYER_BONUS_RETRIEVAL.\n\n• Range: 0.03-0.15 (typical)\n• Subtle guidance: 0.03-0.06\n• Balanced: 0.06-0.10 (recommended)\n• Strong routing: 0.10-0.15\n• Applied: When query intent = API/backend/retrieval/data\n• Requires: repos.json layer_bonuses with retrieval intent mapping",
      "category": "evaluation",
      "related": [],
      "links": [
        {
          "text": "Multi-Tier Architectures",
          "href": "https://en.wikipedia.org/wiki/Multitier_architecture"
        }
      ],
      "badges": [
        {
          "text": "Advanced RAG tuning",
          "class": "info"
        },
        {
          "text": "Multi-layer architectures",
          "class": "info"
        }
      ]
    },
    {
      "term": "Intent Matrix (Advanced)",
      "key": "LAYER_INTENT_MATRIX",
      "definition": "Advanced JSON map that biases results toward specific architectural layers based on the detected intent of the query.\n\nStructure: { intent: { layer: multiplier } }\n\nWhen a query is classified as an intent (e.g., gui, retrieval, indexer), the corresponding row multiplies layer bonuses for matching files/directories. Values > 1.0 boost that layer for the intent; values < 1.0 penalize. This is “soft routing” for monorepos: UI questions should drift toward web/gui, indexing questions toward indexer, etc.\n\nStart conservative. Large multipliers can overpower actual relevance and produce confidently-wrong results.\n\n• Format: JSON object of objects (numbers only)\n• Multipliers: 1.0 = neutral, > 1.0 boost, < 1.0 penalize\n• Typical range: 0.6–1.3\n• Debugging: if results feel “stuck” in one layer, reduce the highest multipliers\n• Related knobs: Layer Bonus (GUI/Retrieval), Vendor Penalty, Freshness Bonus",
      "category": "general",
      "related": [
        "Layer Bonus (GUI)",
        "Layer Bonus (Retrieval)",
        "Vendor Penalty",
        "Freshness Bonus"
      ],
      "links": [
        {
          "text": "Architecture-Aware Retrieval",
          "href": "https://arxiv.org/abs/2312.10997"
        },
        {
          "text": "Multitier Architecture",
          "href": "https://en.wikipedia.org/wiki/Multitier_architecture"
        }
      ],
      "badges": [
        {
          "text": "Advanced RAG tuning",
          "class": "info"
        },
        {
          "text": "Multi-layer architectures",
          "class": "info"
        }
      ]
    },
    {
      "term": "Log Level",
      "key": "LOG_LEVEL",
      "definition": "Logging verbosity level. Options: DEBUG (verbose, dev), INFO (normal, recommended), WARNING (errors + warnings only), ERROR (errors only). Higher levels reduce noise but may hide useful diagnostics.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Python Logging Levels",
          "href": "https://docs.python.org/3/library/logging.html#logging-levels"
        },
        {
          "text": "Logging Best Practices",
          "href": "https://docs.python.org/3/howto/logging.html"
        }
      ],
      "badges": []
    },
    {
      "term": "Max Chunk Tokens",
      "key": "MAX_CHUNK_TOKENS",
      "definition": "Maximum token length for a single code chunk during AST-based chunking. Limits chunk size to fit within embedding model token limits (typically 512-8192 tokens). Larger chunks (1000-2000 tokens) capture more context per chunk, reducing fragmentation of large functions/classes. Smaller chunks (200-512 tokens) create more granular units, improving precision but potentially losing broader context.\n\nSweet spot: 512-768 tokens for balanced chunking. This fits most embedding models (e.g., OpenAI text-embedding-3 supports up to 8191 tokens, but 512-768 is practical). Use 768-1024 for code with large docstrings or complex classes where context matters. Use 256-512 for tight memory budgets or when targeting very specific code snippets. AST chunking respects syntax, so chunks won't split mid-function even if size limit is hit (falls back to greedy chunking).\n\nToken count is approximate (based on whitespace heuristics, not exact tokenization). Actual embedding input may vary slightly. If a logical unit (function, class) exceeds MAX_CHUNK_TOKENS, the chunker splits it using GREEDY_FALLBACK_TARGET for sub-chunking while preserving structure where possible.\n\n• Range: 200-2000 tokens (typical)\n• Small: 256-512 tokens (precision, tight memory)\n• Balanced: 512-768 tokens (recommended, fits most models)\n• Large: 768-1024 tokens (more context, larger functions)\n• Very large: 1024-2000 tokens (maximum context, risky for some models)\n• Constraint: Must not exceed embedding model token limit",
      "category": "chunking",
      "related": [],
      "links": [
        {
          "text": "Token Limits by Model",
          "href": "https://platform.openai.com/docs/guides/embeddings/embedding-models"
        },
        {
          "text": "cAST Paper",
          "href": "https://arxiv.org/abs/2506.15655"
        },
        {
          "text": "Chunking Size Tradeoffs",
          "href": "https://weaviate.io/blog/chunking-strategies-for-rag"
        },
        {
          "text": "Token Estimation",
          "href": "https://github.com/openai/tiktoken"
        }
      ],
      "badges": [
        {
          "text": "Advanced chunking",
          "class": "info"
        },
        {
          "text": "Requires reindex",
          "class": "reindex"
        }
      ]
    },
    {
      "term": "Max Hops",
      "key": "MAX_HOPS",
      "definition": "Maximum number of relationship traversals in graph search. Higher values explore more connected code but increase latency. Range: 1-5. Default: 2. Set to 1 for direct relationships only.",
      "category": "retrieval",
      "related": [
        "Graph Weight",
        "Neo4j Connection URI",
        "Entity Types"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Max Indexable File Size",
      "key": "MAX_INDEXABLE_FILE_SIZE",
      "definition": "Maximum file size in bytes that will be indexed. Files larger than this limit are skipped during indexing to prevent memory issues and avoid indexing large binary or generated files. Default is 2MB (2,000,000 bytes). Increase for codebases with legitimately large source files; decrease to speed up indexing and reduce memory usage.\n\nSweet spot: 1-2 MB for most codebases. Use 500KB-1MB for memory-constrained environments or when you want to exclude large auto-generated files. Use 2-5MB for codebases with large source files (e.g., bundled assets, data files that should be searchable). Files exceeding this limit are logged as skipped.\n\nExample: A 5MB SQL dump file would be skipped with MAX_INDEXABLE_FILE_SIZE=2000000. To include it, increase to 6000000 (6MB). Large files that are indexed will be chunked normally, but may take longer to process and consume more embedding API tokens.\n\n• Range: 100KB - 10MB (typical)\n• Tight: 100KB - 500KB (skip most large files, fast indexing)\n• Balanced: 1MB - 2MB (recommended, handles normal source files)\n• Large: 2MB - 5MB (include larger source files)\n• Very large: 5MB - 10MB (include data files, maximum coverage)\n• Trade-off: Higher limit = more coverage, slower indexing, more memory",
      "category": "infrastructure",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "File filtering",
          "class": "info"
        },
        {
          "text": "Requires reindex",
          "class": "reindex"
        }
      ]
    },
    {
      "term": "Multi‑Query Rewrites",
      "key": "MAX_QUERY_REWRITES",
      "definition": "Number of LLM‑generated query variations. Each variation runs hybrid retrieval; results are merged and reranked. Higher improves recall but increases latency and API cost. Typical: 2–4.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Multi‑Query Retriever",
          "href": "https://python.langchain.com/docs/how_to/MultiQueryRetriever/"
        },
        {
          "text": "Multi‑Query RAG (paper)",
          "href": "https://arxiv.org/abs/2305.14283"
        }
      ],
      "badges": [
        {
          "text": "Better recall",
          "class": "info"
        },
        {
          "text": "Higher cost",
          "class": "warn"
        }
      ]
    },
    {
      "term": "MCP API Key (Optional)",
      "key": "MCP_API_KEY",
      "definition": "Authentication key for securing MCP server access. Stored in .env file. Leave empty to disable authentication (not recommended for production).",
      "category": "general",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Stored in .env",
          "class": "security"
        }
      ]
    },
    {
      "term": "MCP HTTP Host",
      "key": "MCP_HTTP_HOST",
      "definition": "Bind address for the HTTP MCP server (fast transport). Use 0.0.0.0 to listen on all interfaces, 127.0.0.1 for localhost only, or a specific IP like 192.168.1.100 for LAN access. MCP (Model Context Protocol) enables fast communication between clients and the RAG engine.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "HTTP Host Header Reference",
          "href": "https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Host"
        },
        {
          "text": "Localhost Concept",
          "href": "https://en.wikipedia.org/wiki/Localhost"
        },
        {
          "text": "MCP Specification",
          "href": "https://github.com/modelcontextprotocol/specification"
        }
      ],
      "badges": []
    },
    {
      "term": "MCP HTTP Path",
      "key": "MCP_HTTP_PATH",
      "definition": "URL path for the HTTP MCP endpoint (default /mcp). Example: http://localhost:8013/mcp. Customize for reverse proxies or routing needs. Must match client configuration if changed.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "URL Structure",
          "href": "https://developer.mozilla.org/en-US/docs/Learn/Common_questions/Web_mechanics/What_is_a_URL"
        },
        {
          "text": "URI Standard",
          "href": "https://en.wikipedia.org/wiki/Uniform_Resource_Identifier"
        },
        {
          "text": "MCP Specification",
          "href": "https://github.com/modelcontextprotocol/specification"
        }
      ],
      "badges": []
    },
    {
      "term": "MCP HTTP Port",
      "key": "MCP_HTTP_PORT",
      "definition": "TCP port for HTTP MCP server (default 8013). Must not conflict with other services. Use ports 1024+ without special permissions. MCP enables fast, stateless communication for multi-client scenarios.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Port Numbers",
          "href": "https://en.wikipedia.org/wiki/List_of_TCP_and_UDP_port_numbers"
        },
        {
          "text": "HTTP Basics",
          "href": "https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP"
        },
        {
          "text": "MCP Specification",
          "href": "https://github.com/modelcontextprotocol/specification"
        }
      ],
      "badges": []
    },
    {
      "term": "MCP Server URL",
      "key": "MCP_SERVER_URL",
      "definition": "Complete URL for the HTTP MCP server. Combines host, port, and path into a single endpoint that MCP clients connect to.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Model Context Protocol",
          "href": "https://modelcontextprotocol.io"
        }
      ],
      "badges": []
    },
    {
      "term": "Metrics Enabled",
      "key": "METRICS_ENABLED",
      "definition": "Enable Prometheus metrics collection and /metrics endpoint. When on, exposes query latency, cache hits, error rates, etc. Essential for production monitoring. Minimal overhead.",
      "category": "evaluation",
      "related": [],
      "links": [
        {
          "text": "Prometheus Metrics",
          "href": "https://prometheus.io/docs/concepts/metric_types/"
        },
        {
          "text": "Monitoring Best Practices",
          "href": "https://prometheus.io/docs/practices/naming/"
        }
      ],
      "badges": []
    },
    {
      "term": "Min Chunk Chars",
      "key": "MIN_CHUNK_CHARS",
      "definition": "Minimum character count for a valid chunk. Chunks smaller than this are discarded or merged with adjacent chunks to avoid indexing trivial code fragments (empty functions, single-line comments, import statements). Higher minimums (100-200 chars) filter out noise and reduce index size but may skip small utility functions. Lower minimums (20-50 chars) index everything but include low-value chunks.\n\nSweet spot: 50-100 characters for balanced filtering. Use 100-200 for aggressive noise reduction when you have many trivial functions or auto-generated code. Use 20-50 to index everything, including tiny utilities (useful for finding specific one-liners or constants). This threshold applies after AST chunking - if a logical unit is too small, it's skipped unless PRESERVE_IMPORTS is enabled.\n\nExample: A 2-line import block (30 chars) would be skipped with MIN_CHUNK_CHARS=50 unless PRESERVE_IMPORTS=1. A 5-line utility function (80 chars) would pass the filter. This prevents embedding API calls and index bloat from non-semantic content. Adjust based on your codebase style - functional codebases with many small functions may need lower thresholds.\n\n• Range: 20-300 characters (typical)\n• Very permissive: 20-50 chars (index everything, including tiny snippets)\n• Balanced: 50-100 chars (recommended, filter trivial fragments)\n• Aggressive filtering: 100-200 chars (skip small utilities, focus on substantial code)\n• Very aggressive: 200-300 chars (only meaningful functions/classes)\n• Trade-off: Higher threshold = cleaner index, may miss small but relevant code",
      "category": "chunking",
      "related": [],
      "links": [
        {
          "text": "Code Chunking Best Practices",
          "href": "https://weaviate.io/blog/chunking-strategies-for-rag"
        },
        {
          "text": "cAST Filtering",
          "href": "https://github.com/yilinjz/astchunk#filtering"
        }
      ],
      "badges": [
        {
          "text": "Index quality control",
          "class": "info"
        },
        {
          "text": "Requires reindex",
          "class": "reindex"
        }
      ]
    },
    {
      "term": "Multi‑Query Rewrites",
      "key": "MQ_REWRITES",
      "definition": "Number of query variations to generate for improved recall. Each rewrite searches independently, then results are fused and reranked. For example, query \"auth flow\" might expand to \"authentication flow\", \"login process\", \"user authentication\". Higher values (4-6) improve recall for vague questions like \"Where is X implemented?\" but increase API calls and latency. Start at 2-3 for general use.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Multi-Query RAG",
          "href": "https://arxiv.org/abs/2305.14283"
        },
        {
          "text": "Query Expansion",
          "href": "https://en.wikipedia.org/wiki/Query_expansion"
        },
        {
          "text": "RAG Techniques",
          "href": "https://python.langchain.com/docs/how_to/MultiQueryRetriever/"
        }
      ],
      "badges": [
        {
          "text": "Affects latency",
          "class": "info"
        },
        {
          "text": "Higher cost",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Multi-Query M (RRF Constant)",
      "key": "MULTI_QUERY_M",
      "definition": "Constant \"k\" parameter in Reciprocal Rank Fusion (RRF) formula used to merge results from multiple query rewrites. RRF formula: score = sum(1 / (k + rank_i)) across all query variants. Higher M values (60-100) compress rank differences, treating top-10 and top-20 results more equally. Lower M values (20-40) emphasize top-ranked results, creating steeper rank penalties.\n\nSweet spot: 50-60 for balanced fusion. This is the standard RRF constant used in most production systems. Use 40-50 for more emphasis on top results (good when rewrites are high quality). Use 60-80 for smoother fusion (good when rewrites produce diverse rankings). The parameter is called \"M\" in code but represents the \"k\" constant in academic RRF papers.\n\nRRF fusion happens when MQ_REWRITES > 1: each query variant retrieves results, then RRF merges them by summing reciprocal ranks. Example with M=60: rank-1 result scores 1/61=0.016, rank-10 scores 1/70=0.014. Higher M reduces the gap. This parameter rarely needs tuning - default of 60 works well for most use cases.\n\n• Standard range: 40-80\n• Emphasize top results: 40-50\n• Balanced: 50-60 (recommended, RRF default)\n• Smooth fusion: 60-80\n• Formula: score = sum(1 / (M + rank)) for each query variant\n• Only matters when: MQ_REWRITES > 1 (multi-query enabled)",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Reciprocal Rank Fusion Paper",
          "href": "https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf"
        },
        {
          "text": "RRF in Practice",
          "href": "https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html"
        },
        {
          "text": "Multi-Query RAG",
          "href": "https://arxiv.org/abs/2305.14283"
        },
        {
          "text": "Fusion Strategies",
          "href": "https://arxiv.org/abs/2402.14734"
        }
      ],
      "badges": [
        {
          "text": "Advanced RAG tuning",
          "class": "info"
        },
        {
          "text": "RRF fusion control",
          "class": "info"
        }
      ]
    },
    {
      "term": "Neo4j Connection URI",
      "key": "NEO4J_URI",
      "definition": "Connection URI for Neo4j graph database. Used for entity relationships, graph traversal, and community detection in tri-brid search. Format: bolt://host:7687 or neo4j://host:7687. Enables graph-based retrieval for code navigation.",
      "category": "infrastructure",
      "related": [
        "PostgreSQL pgvector URL",
        "Graph Search",
        "Max Hops"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Netlify API Key",
      "key": "NETLIFY_API_KEY",
      "definition": "API key for the netlify_deploy MCP tool to trigger automated site deployments and builds. Get your personal access token from Netlify dashboard under User Settings > Applications > Personal Access Tokens. Used to programmatically deploy site updates from your workflow.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Netlify: Access Tokens",
          "href": "https://docs.netlify.com/api/get-started/#access-tokens"
        }
      ],
      "badges": []
    },
    {
      "term": "Netlify Domains",
      "key": "NETLIFY_DOMAINS",
      "definition": "Comma-separated list of Netlify site domains for the netlify_deploy MCP tool (e.g., \"mysite.netlify.app,docs.mysite.com\"). When deploying, the tool targets these specific sites. Find your site domains in Netlify dashboard under Site Settings > Domain Management. Multiple domains allow you to deploy to staging and production from the same config.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Netlify Sites",
          "href": "https://docs.netlify.com/domains-https/custom-domains/"
        },
        {
          "text": "Netlify Dashboard",
          "href": "https://app.netlify.com/"
        }
      ],
      "badges": []
    },
    {
      "term": "Local Request Timeout (seconds)",
      "key": "OLLAMA_REQUEST_TIMEOUT",
      "definition": "Maximum total time to wait for a single local (Ollama) generation request to complete. Increase for long answers; decrease to fail fast on slow models or poor connectivity.",
      "category": "generation",
      "related": [],
      "links": [
        {
          "text": "Ollama API: Generate",
          "href": "https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion"
        },
        {
          "text": "HTTP Timeouts",
          "href": "https://developer.mozilla.org/en-US/docs/Web/HTTP/Timeouts"
        }
      ],
      "badges": []
    },
    {
      "term": "Local Stream Idle Timeout (seconds)",
      "key": "OLLAMA_STREAM_IDLE_TIMEOUT",
      "definition": "Maximum idle time allowed between streamed chunks from local (Ollama). If no tokens arrive within this window, the request aborts to prevent hanging streams.",
      "category": "generation",
      "related": [],
      "links": [
        {
          "text": "Streaming Basics",
          "href": "https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream"
        },
        {
          "text": "Ollama Streaming",
          "href": "https://github.com/ollama/ollama/blob/main/docs/api.md#streaming"
        }
      ],
      "badges": []
    },
    {
      "term": "Ollama URL",
      "key": "OLLAMA_URL",
      "definition": "Local inference endpoint for Ollama running on your machine (e.g., http://127.0.0.1:11434/api). Used when GEN_MODEL targets a local model like llama2, mistral, qwen, or neural-chat. Requires Ollama installed and running: ollama serve",
      "category": "generation",
      "related": [],
      "links": [
        {
          "text": "Ollama REST API",
          "href": "https://github.com/ollama/ollama/blob/main/docs/api.md"
        },
        {
          "text": "Ollama Docker Setup",
          "href": "https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image"
        },
        {
          "text": "Ollama Model Library",
          "href": "https://ollama.com/library"
        }
      ],
      "badges": []
    },
    {
      "term": "OpenAI API Key",
      "key": "OPENAI_API_KEY",
      "definition": "API key used for OpenAI-based embeddings and/or generation.",
      "category": "generation",
      "related": [],
      "links": [
        {
          "text": "OpenAI: API Keys",
          "href": "https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key"
        },
        {
          "text": "OpenAI Models",
          "href": "https://platform.openai.com/docs/models"
        }
      ],
      "badges": []
    },
    {
      "term": "OpenAI Base URL",
      "key": "OPENAI_BASE_URL",
      "definition": "ADVANCED: Override the OpenAI API base URL for OpenAI-compatible endpoints. Use cases: local inference servers (LM Studio, vLLM, text-generation-webui), Azure OpenAI (https://YOUR_RESOURCE.openai.azure.com/), proxy services. Default: https://api.openai.com/v1. Useful for development, air-gapped environments, or cost optimization via self-hosted models.",
      "category": "generation",
      "related": [],
      "links": [
        {
          "text": "OpenAI API Reference",
          "href": "https://platform.openai.com/docs/api-reference"
        },
        {
          "text": "Azure OpenAI",
          "href": "https://learn.microsoft.com/en-us/azure/ai-services/openai/"
        },
        {
          "text": "LM Studio Setup",
          "href": "https://lmstudio.ai/docs/local-server"
        },
        {
          "text": "vLLM Compatibility",
          "href": "https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html"
        }
      ],
      "badges": [
        {
          "text": "Advanced",
          "class": "warn"
        },
        {
          "text": "For compatible endpoints only",
          "class": "info"
        }
      ]
    },
    {
      "term": "Auto-Open Browser",
      "key": "OPEN_BROWSER",
      "definition": "Automatically open browser to GUI when server starts (1=yes, 0=no). Convenient for local development, disable for server deployments or headless environments.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Browser Automation",
          "href": "https://en.wikipedia.org/wiki/Browser_automation"
        }
      ],
      "badges": []
    },
    {
      "term": "Out Dir Base",
      "key": "OUT_DIR_BASE",
      "definition": "Where retrieval looks for indices (chunks.jsonl, bm25_index/). Use ./out.noindex-shared for one index across branches so MCP and local tools stay in sync. Stores dense vectors (Qdrant), sparse BM25 index, and indexed chunks. Symptom of mismatch: rag_search returns 0 results.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Directory Concepts",
          "href": "https://en.wikipedia.org/wiki/Directory_(computing)"
        },
        {
          "text": "MCP Protocol Spec",
          "href": "https://github.com/modelcontextprotocol/specification"
        },
        {
          "text": "Storage Management",
          "href": "https://qdrant.tech/documentation/concepts/storage/"
        }
      ],
      "badges": [
        {
          "text": "Requires restart (MCP)",
          "class": "info"
        }
      ]
    },
    {
      "term": "Per-Repository Indexing Configuration",
      "key": "PER_REPO_INDEXING",
      "definition": "Override global indexing settings per repo. Enables optimization for different codebases. Scenarios: docs repos (larger chunks 1500-2000, stemmer), dense code (smaller chunks 500-800, whitespace), mixed (AST + hybrid), legacy (greedy if AST fails). Checked = inherit tribrid_config.json. Unchecked = repos.json overrides take precedence. Unchanged fields still inherit global. Changes apply on reindex.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Monorepo Configuration Patterns",
          "href": "https://monorepo.tools/"
        },
        {
          "text": "Multi-Repo Search Strategies",
          "href": "https://www.aviator.co/blog/monorepo-a-hands-on-guide-for-managing-repositories-and-microservices/"
        },
        {
          "text": "Configuration Override Patterns",
          "href": "https://en.wikipedia.org/wiki/Configuration_file"
        },
        {
          "text": "Cascading Configuration",
          "href": "https://en.wikipedia.org/wiki/Cascading_Style_Sheets#Specificity"
        }
      ],
      "badges": [
        {
          "text": "ADVANCED",
          "class": "warn"
        },
        {
          "text": "PER-REPO",
          "class": "info"
        }
      ]
    },
    {
      "term": "HTTP Port",
      "key": "PORT",
      "definition": "TCP port for the HTTP server that serves the GUI and API endpoints when running serve_rag. Default: 8012. Change if port 8012 is already in use by another service (common conflict: development servers). After changing, access GUI at http://127.0.0.1:&lt;NEW_PORT&gt;. Requires server restart to take effect.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "TCP Ports",
          "href": "https://en.wikipedia.org/wiki/List_of_TCP_and_UDP_port_numbers"
        },
        {
          "text": "Port Conflicts",
          "href": "https://en.wikipedia.org/wiki/Port_scanner"
        }
      ],
      "badges": [
        {
          "text": "Requires restart",
          "class": "warn"
        }
      ]
    },
    {
      "term": "PostgreSQL pgvector URL",
      "key": "POSTGRES_URL",
      "definition": "Connection URL for PostgreSQL with pgvector extension. Used for dense vector storage and similarity search. Format: postgresql://user:pass@host:port/db. The pgvector extension enables efficient vector indexing with HNSW or IVFFlat.",
      "category": "infrastructure",
      "related": [
        "Neo4j Connection URI",
        "Vector Search"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Presence Penalty",
      "key": "PRESENCE_PENALTY",
      "definition": "Penalizes tokens that have already appeared, encouraging the model to introduce new topics/entities. Higher values increase exploration and reduce reuse of the same concepts. Use 0.0-0.4 for factual/code responses; 0.4-0.8 for brainstorming; >0.8 can push the model toward excessive novelty. Presence penalty interacts with TEMPERATURE/TOP_P: higher penalties with high temperature can make answers meander. For RAG, keep modest (<=0.5) to prevent drifting away from cited context.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Presence Penalty API",
          "href": "https://platform.openai.com/docs/api-reference/chat/create#chat-create-presence_penalty"
        },
        {
          "text": "Decoding Trade-offs",
          "href": "https://huggingface.co/blog/how-to-generate"
        },
        {
          "text": "RAG Best Practices",
          "href": "https://langchain-ai.github.io/langgraph/"
        }
      ],
      "badges": [
        {
          "text": "Encourage novelty",
          "class": "info"
        },
        {
          "text": "Risk: drift",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Preserve Imports",
      "key": "PRESERVE_IMPORTS",
      "definition": "Include import/require statements in chunks even if they fall below MIN_CHUNK_CHARS threshold (1=yes, 0=no). When enabled, import blocks become searchable, helping users find dependency usage and module relationships. When disabled, imports are filtered out as low-value content. Enabling increases index size slightly but improves dependency discovery (e.g., \"where do we use requests library?\").\n\nSweet spot: 1 (enabled) for codebases where dependency tracking matters. Use 0 (disabled) to reduce index size and focus on implementation code rather than declarations. Import preservation is especially valuable in polyglot repos (Python, JavaScript, Go) where import patterns reveal architecture. Imports are still visible in full file context; this setting only affects whether they're indexed as standalone chunks.\n\nExample: With PRESERVE_IMPORTS=1, a 3-line import block becomes a searchable chunk even if it's <MIN_CHUNK_CHARS. A query like \"where do we import AuthService?\" will match this chunk. With PRESERVE_IMPORTS=0, the import block is skipped, and only code using AuthService is indexed.\n\n• 0: Disabled - skip import statements, reduce noise, smaller index, focus on implementation\n• 1: Enabled - index imports, discover dependencies, find module usage, slightly larger index (recommended)\n• Use case: Dependency audits, security reviews, architecture analysis\n• Trade-off: Slightly larger index vs better dependency discovery",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Code Structure Analysis",
          "href": "https://en.wikipedia.org/wiki/Dependency_analysis"
        },
        {
          "text": "Module Systems",
          "href": "https://en.wikipedia.org/wiki/Modular_programming"
        }
      ],
      "badges": [
        {
          "text": "Dependency tracking",
          "class": "info"
        },
        {
          "text": "Requires reindex",
          "class": "reindex"
        }
      ]
    },
    {
      "term": "Prometheus Port",
      "key": "PROMETHEUS_PORT",
      "definition": "TCP port for Prometheus metrics endpoint. Exposes /metrics for scraping by Prometheus or Grafana. Default: 9090. Change to avoid conflicts with existing monitoring tools.",
      "category": "infrastructure",
      "related": [],
      "links": [
        {
          "text": "Prometheus Basics",
          "href": "https://prometheus.io/docs/introduction/overview/"
        },
        {
          "text": "Metrics Endpoint",
          "href": "https://prometheus.io/docs/instrumenting/exposition_formats/"
        }
      ],
      "badges": []
    },
    {
      "term": "Qdrant URL",
      "key": "QDRANT_URL",
      "definition": "HTTP URL for your Qdrant vector database. Used for dense vector queries during retrieval. If unavailable, retrieval still works via BM25 (sparse).",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Qdrant Docs: Collections",
          "href": "https://qdrant.tech/documentation/concepts/collections/"
        },
        {
          "text": "Qdrant (GitHub)",
          "href": "https://github.com/qdrant/qdrant"
        }
      ],
      "badges": []
    },
    {
      "term": "RAG Out Base",
      "key": "RAG_OUT_BASE",
      "definition": "Optional override for OUT_DIR_BASE for retrieval-specific output directory. Advanced users can use this to separate indexing output from retrieval search indices while keeping OUT_DIR_BASE for main indexing. Most users should leave empty—use OUT_DIR_BASE only. Primarily for multi-environment setups needing separate retrieval and indexing directories.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Configuration Management",
          "href": "https://12factor.net/config"
        },
        {
          "text": "Storage Concepts",
          "href": "https://qdrant.tech/documentation/concepts/storage/"
        },
        {
          "text": "BM25 Index Storage",
          "href": "https://github.com/BM25S/bm25s"
        }
      ],
      "badges": [
        {
          "text": "Advanced",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Rate Limit Errors (per 5 min)",
      "key": "RATE_LIMIT_ERRORS_THRESHOLD",
      "definition": "Maximum number of rate limit errors (HTTP 429) allowed in a 5-minute window. Rate limits protect against excessive API usage and prevent cost overruns. Common sources: OpenAI API, Cohere, Voyage AI. If this alert fires frequently, consider upgrading API tier or implementing request batching.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Rate Limiting (OpenAI)",
          "href": "https://platform.openai.com/docs/guides/rate-limits"
        },
        {
          "text": "Backoff Strategies",
          "href": "https://aws.amazon.com/builders-library/timeouts-retries-and-backoff-with-jitter/"
        }
      ],
      "badges": [
        {
          "text": "Cost Control",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Redis URL",
      "key": "REDIS_URL",
      "definition": "Connection string for Redis, used for LangGraph checkpoints and optional session memory. The graph runs even if Redis is down (stateless mode).",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Redis Docs",
          "href": "https://redis.io/docs/latest/"
        },
        {
          "text": "LangGraph Checkpoints",
          "href": "https://langchain-ai.github.io/langgraph/concepts/persistence/"
        },
        {
          "text": "Redis Connection URLs",
          "href": "https://redis.io/docs/latest/develop/connect/clients/"
        }
      ],
      "badges": []
    },
    {
      "term": "Relationship Types",
      "key": "RELATIONSHIP_TYPES",
      "definition": "Types of relationships to extract between code entities: calls (function invocations), imports (module dependencies), inherits (class hierarchy), contains (nesting), references (variable usage). Determines graph edges.",
      "category": "retrieval",
      "related": [
        "Entity Types",
        "Max Hops",
        "Neo4j Connection URI"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Active Repository",
      "key": "REPO",
      "definition": "Logical repository name for routing and indexing. MCP and CLI use this to scope retrieval. Must match a repository name defined in repos.json for multi-repo setups. Example: \"tribrid-demo\", \"myapp\", \"cli-tool\". Used for multi-repo RAG systems where each repo has separate indices, keywords, and path boosts.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Namespace Concept",
          "href": "https://en.wikipedia.org/wiki/Namespace"
        },
        {
          "text": "MCP Protocol Spec",
          "href": "https://github.com/modelcontextprotocol/specification"
        },
        {
          "text": "LangSmith Context",
          "href": "https://www.langchain.com/langsmith"
        }
      ],
      "badges": []
    },
    {
      "term": "Repos File",
      "key": "REPOS_FILE",
      "definition": "Path to repos.json that defines repo names, paths, keywords, path boosts, and layer bonuses used for multi-repo routing. Each repo entry includes name, path, optional keywords for boosting, path_boosts for directory-specific relevance, and layer_bonuses for hierarchical retrieval.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "JSON Format Reference",
          "href": "https://www.json.org/json-en.html"
        },
        {
          "text": "Configuration Management",
          "href": "https://github.com/topics/configuration-management"
        },
        {
          "text": "Config File Concepts",
          "href": "https://en.wikipedia.org/wiki/Configuration_file"
        }
      ],
      "badges": []
    },
    {
      "term": "Repo Path (fallback)",
      "key": "REPO_PATH",
      "definition": "Absolute filesystem path to the active repository when repos.json is not configured. This is the directory that will be indexed for code retrieval. Use repos.json instead for multi-repo setups with routing, keywords, and path boosts. Example: /Users/you/projects/myapp or /home/user/code/myrepo",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Path Patterns",
          "href": "https://github.com/github/gitignore"
        },
        {
          "text": "Python pathlib Module",
          "href": "https://docs.python.org/3/library/pathlib.html"
        },
        {
          "text": "File System Paths",
          "href": "https://en.wikipedia.org/wiki/Path_(computing)"
        }
      ],
      "badges": []
    },
    {
      "term": "Repository Root Override",
      "key": "REPO_ROOT",
      "definition": "Override the auto-detected project root directory. TriBridRAG normally detects the repository root automatically by walking up from the current working directory to find .git or pyproject.toml. Use this setting when running in Docker, when TriBridRAG is installed outside the repository, or when you need to force a specific root path. Leave empty to use auto-detection. Example: /workspace/myproject",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Path Resolution",
          "href": "https://en.wikipedia.org/wiki/Path_(computing)#Absolute_and_relative_paths"
        },
        {
          "text": "Docker Volume Mounts",
          "href": "https://docs.docker.com/storage/volumes/"
        }
      ],
      "badges": [
        {
          "text": "Optional",
          "class": "info"
        },
        {
          "text": "Docker-friendly",
          "class": "info"
        }
      ]
    },
    {
      "term": "Active Reranker",
      "key": "RERANKER_ACTIVE",
      "definition": "Route reranking to local vs cloud.\n• local/learning — on-host (includes TriBridRAG learning reranker)\n• cloud — uses provider/model from models.json\n• none/off — disables rerank. If cloud is selected but provider/model are empty, rerank is effectively disabled.",
      "category": "reranking",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Required",
          "class": "info"
        }
      ]
    },
    {
      "term": "Reranker Backend",
      "key": "RERANKER_BACKEND",
      "definition": "Choose the reranking provider to reorder retrieved results by semantic relevance (cross-encoder). Options typically include Cohere Rerank, the built‑in TriBridRAG Learning Reranker, or none. Reranking improves answer quality but adds latency.",
      "category": "reranking",
      "related": [],
      "links": [
        {
          "text": "Cohere Rerank",
          "href": "https://docs.cohere.com/docs/rerank"
        },
        {
          "text": "Sentence‑Transformers (Cross‑Encoders)",
          "href": "https://www.sbert.net/examples/training/cross-encoder/README.html"
        }
      ],
      "badges": [
        {
          "text": "Improves quality",
          "class": "info"
        }
      ]
    },
    {
      "term": "Cloud Model",
      "key": "RERANKER_CLOUD_MODEL",
      "definition": "Provider-scoped rerank model id from models.json. Examples: rerank-3.5 (cohere), rerank-2 (voyage), or any custom id you add. Model list comes from models.json; add entries there to surface more options in this picker.",
      "category": "reranking",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Provider-scoped",
          "class": "info"
        }
      ]
    },
    {
      "term": "Cloud Rerank Provider",
      "key": "RERANKER_CLOUD_PROVIDER",
      "definition": "When RERANKER_MODE=cloud, specifies which API provider to use for reranking. Options: cohere, voyage, jina. Each provider has different pricing and model options—see models.json for available models. Requires the corresponding API key (COHERE_API_KEY, VOYAGE_API_KEY, etc.).",
      "category": "reranking",
      "related": [],
      "links": [
        {
          "text": "Cohere Rerank",
          "href": "https://docs.cohere.com/reference/rerank"
        },
        {
          "text": "Voyage Rerank",
          "href": "https://docs.voyageai.com/docs/reranker"
        },
        {
          "text": "Jina Rerank",
          "href": "https://jina.ai/reranker/"
        }
      ],
      "badges": [
        {
          "text": "Requires API key",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Cloud Reranker Top-N",
      "key": "RERANKER_CLOUD_TOP_N",
      "definition": "Maximum number of candidates to send to cloud reranking APIs (Cohere, Voyage, Jina). Cloud rerankers have rate limits and per-request pricing, so this setting is separate from the local reranker top-N. Lower values reduce API costs and stay within rate limits. Higher values improve recall but increase costs per query.\n\n• Typical range: 20-100 candidates\n• Cost-conscious: 20-30 for budget limits\n• Balanced default: 50 for most workloads\n• High recall: 80-100 for exploratory queries\n• Note: Cloud reranking is billed per candidate, so monitor costs",
      "category": "reranking",
      "related": [],
      "links": [
        {
          "text": "Cohere Rerank API",
          "href": "https://docs.cohere.com/reference/rerank"
        },
        {
          "text": "Voyage Rerank",
          "href": "https://docs.voyageai.com/docs/reranker"
        }
      ],
      "badges": [
        {
          "text": "Cloud API costs",
          "class": "warn"
        },
        {
          "text": "Rate limits apply",
          "class": "info"
        }
      ]
    },
    {
      "term": "Local Reranker Model",
      "key": "RERANKER_LOCAL_MODEL",
      "definition": "When RERANKER_MODE=local, specifies the model to load. Can be:\n• A HuggingFace model ID (e.g., BAAI/bge-reranker-v2-m3)\n• A local filesystem path (e.g., /models/my-reranker)\n• Any sentence-transformers compatible model\n\nPopular options: BAAI/bge-reranker-v2-m3 (high quality), jinaai/jina-reranker-v1-base-en, cross-encoder/ms-marco-MiniLM-L-12-v2 (fast).",
      "category": "reranking",
      "related": [],
      "links": [
        {
          "text": "BGE Reranker",
          "href": "https://huggingface.co/BAAI/bge-reranker-v2-m3"
        },
        {
          "text": "Jina Reranker",
          "href": "https://huggingface.co/jinaai/jina-reranker-v1-base-en"
        },
        {
          "text": "SBERT Cross-Encoders",
          "href": "https://www.sbert.net/docs/cross_encoder/pretrained_models.html"
        }
      ],
      "badges": [
        {
          "text": "Free (no API costs)",
          "class": "info"
        },
        {
          "text": "Requires download",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Reranker Mode",
      "key": "RERANKER_MODE",
      "definition": "Controls which reranking approach is used. Four options:\n\n• none: Disabled—BM25 + vector fusion only, no cross-encoder scoring.\n• local: Any local reranker model you provide (BGE, Jina, etc.).\n• learning: TriBridRAG self-training cross-encoder that improves with your usage patterns.\n• cloud: External API reranking (Cohere, Voyage, Jina).\n\nRecommended: Start with \"learning\" to leverage TriBridRAG's adaptive improvements, or \"cloud\" for lowest latency if you have API budget.",
      "category": "reranking",
      "related": [],
      "links": [
        {
          "text": "Cross-Encoder Overview",
          "href": "https://www.sbert.net/examples/applications/cross-encoder/README.html"
        }
      ],
      "badges": [
        {
          "text": "Controls reranking behavior",
          "class": "info"
        }
      ]
    },
    {
      "term": "Local Reranker (HF)",
      "key": "RERANKER_MODEL",
      "definition": "HuggingFace model name or path for local reranking when RERANK_BACKEND=local or hf. Common options: \"cross-encoder/ms-marco-MiniLM-L-6-v2\" (fast, good quality), \"BAAI/bge-reranker-base\" (higher quality, slower), or path to your fine-tuned model like \"models/cross-encoder-tribrid\". Local reranking is free but slower than Cohere. Ensure model is downloaded before use.",
      "category": "reranking",
      "related": [],
      "links": [
        {
          "text": "Cross-Encoder Models",
          "href": "https://www.sbert.net/docs/cross_encoder/pretrained_models.html"
        },
        {
          "text": "HuggingFace Model Hub",
          "href": "https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads"
        }
      ],
      "badges": [
        {
          "text": "Free (no API costs)",
          "class": "info"
        },
        {
          "text": "Requires download",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Cloud Provider (models.json)",
      "key": "RERANKER_PROVIDER",
      "definition": "Provider id for cloud reranking, loaded dynamically from models.json via /api/models. Examples: cohere, voyage, openai, or any custom provider you add. No hardcoded lists; extend models.json to expose more providers.",
      "category": "reranking",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "models.json-driven",
          "class": "info"
        }
      ]
    },
    {
      "term": "Reranker Timeout",
      "key": "RERANKER_TIMEOUT",
      "definition": "Timeout (seconds) for cloud reranker HTTP calls. Larger timeouts reduce false failures on slow providers; smaller timeouts fail fast when endpoints are slow or unreachable. Applies only to cloud backends.",
      "category": "reranking",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Reliability",
          "class": "info"
        }
      ]
    },
    {
      "term": "Training Batch Size",
      "key": "RERANKER_TRAIN_BATCH",
      "definition": "Batches per gradient step during training. Larger batch sizes stabilize training but require more memory. For Colima or small GPUs/CPUs, use 1–4. If you see the container exit with code -9 (OOM), reduce this value.",
      "category": "embedding",
      "related": [],
      "links": [
        {
          "text": "Memory Tips (HF)",
          "href": "https://huggingface.co/docs/transformers/perf_train_gpu_one"
        },
        {
          "text": "Colima Resources",
          "href": "https://github.com/abiosoft/colima"
        }
      ],
      "badges": [
        {
          "text": "Lower = safer on Colima",
          "class": "info"
        }
      ]
    },
    {
      "term": "Training Epochs",
      "key": "RERANKER_TRAIN_EPOCHS",
      "definition": "Number of full passes over the training triplets for the learning reranker. More epochs can improve quality but risk overfitting when data is small. Start with 1–2 and increase as your mined dataset grows.",
      "category": "reranking",
      "related": [],
      "links": [
        {
          "text": "Fine-tuning Cross-Encoders",
          "href": "https://www.sbert.net/examples/training/cross-encoder/README.html"
        },
        {
          "text": "InputExample format",
          "href": "https://www.sbert.net/docs/package_reference/cross_encoder.html#inputexample"
        }
      ],
      "badges": [
        {
          "text": "Quality vs overfit",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Training Learning Rate",
      "key": "RERANKER_TRAIN_LR",
      "definition": "Learning rate for the cross-encoder optimizer during fine-tuning. This controls the size of weight updates during gradient descent. Standard range for cross-encoder fine-tuning is 1e-6 to 5e-5. Higher learning rates (3e-5, 5e-5) converge faster but risk overshooting optimal weights and causing training instability or divergence. Lower learning rates (1e-6, 5e-6) are safer and more stable but require more epochs to converge.\n\nSweet spot: 2e-5 for most cross-encoder fine-tuning tasks. This is the default used in many SBERT examples and works well for code reranking. Use 1e-5 for conservative training when you have limited data (<500 triplets) or notice training loss oscillating. Use 3e-5 for faster convergence when you have abundant data (>2000 triplets) and stable validation metrics. Always monitor training loss - if it spikes or increases, your learning rate is too high.\n\nCombine with RERANKER_WARMUP_RATIO for optimal results. Warmup gradually increases the learning rate from 0 to your target LR over the first N% of training, preventing early instability. Most practitioners use 2e-5 with 0.1 warmup as a reliable baseline.\n\n• Standard range: 1e-6 to 5e-5\n• Conservative (small data): 1e-5\n• Balanced default: 2e-5 (recommended)\n• Aggressive (large data): 3e-5 to 5e-5\n• Symptom too high: Loss spikes, NaN values, divergence\n• Symptom too low: Slow convergence, minimal improvement",
      "category": "reranking",
      "related": [],
      "links": [
        {
          "text": "Learning Rate Explained",
          "href": "https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/"
        },
        {
          "text": "Fine-tuning Cross-Encoders",
          "href": "https://www.sbert.net/examples/training/cross-encoder/README.html"
        },
        {
          "text": "Learning Rate Schedules",
          "href": "https://huggingface.co/docs/transformers/main_classes/optimizer_schedules"
        },
        {
          "text": "Cross-Encoder Training Guide",
          "href": "https://arxiv.org/abs/1908.10084"
        }
      ],
      "badges": [
        {
          "text": "Advanced ML training",
          "class": "warn"
        },
        {
          "text": "Requires tuning",
          "class": "info"
        }
      ]
    },
    {
      "term": "Training Max Sequence Length",
      "key": "RERANKER_TRAIN_MAXLEN",
      "definition": "Token limit for the cross-encoder during training. Longer sequences increase memory quadratically. If training fails with OOM (-9) under Docker/Colima, set 128–256. Sequences longer than the limit are truncated by the tokenizer and may emit warnings.",
      "category": "reranking",
      "related": [],
      "links": [
        {
          "text": "Tokenization & Truncation",
          "href": "https://huggingface.co/docs/tokenizers/index"
        },
        {
          "text": "Cross-Encoder Training",
          "href": "https://www.sbert.net/examples/training/cross-encoder/README.html"
        }
      ],
      "badges": [
        {
          "text": "Memory sensitive",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Reranker Train Max Length",
      "key": "RERANKER_TRAIN_MAX_LENGTH",
      "definition": "Maximum token length for reranker training examples. Longer sequences may improve context but require more memory and training time. Typical range: 256–1024.",
      "category": "reranking",
      "related": [],
      "links": [
        {
          "text": "Transformers: Tokenization",
          "href": "https://huggingface.co/docs/transformers/main_classes/tokenizer"
        }
      ],
      "badges": []
    },
    {
      "term": "Warmup Ratio",
      "key": "RERANKER_WARMUP_RATIO",
      "definition": "Fraction of total training steps to use for linear learning rate warmup. During warmup, the learning rate gradually increases from 0 to your target RERANKER_TRAIN_LR, preventing early training instability from large gradient updates. After warmup completes, the learning rate follows its normal schedule (typically constant or linear decay). Standard range: 0.0 (no warmup) to 0.2 (20% of training).\n\nSweet spot: 0.1 (10% warmup) for most cross-encoder training. This means if you train for 100 steps, the first 10 steps will gradually increase LR from 0 to your target. Warmup is especially important when fine-tuning from pretrained models, as it prevents catastrophic forgetting early in training. Use 0.05-0.08 for short training runs (<500 steps) and 0.1-0.15 for longer runs (>1000 steps).\n\nWarmup is critical when training with high learning rates (3e-5+) or limited data. Without warmup, the first few batches can cause large weight updates that destabilize the pretrained model. With warmup, training starts gentle and accelerates gradually. Most SBERT training recipes default to 0.1, which works well across domains.\n\n• No warmup: 0.0 (not recommended for fine-tuning)\n• Short training: 0.05-0.08 (e.g., 1-2 epochs, <500 steps)\n• Balanced default: 0.1 (recommended for most cases)\n• Long training: 0.15-0.2 (e.g., 5+ epochs, >2000 steps)\n• Effect: Stabilizes early training, prevents catastrophic forgetting\n• Combines with: RERANKER_TRAIN_LR for optimal convergence",
      "category": "reranking",
      "related": [],
      "links": [
        {
          "text": "Warmup Schedules",
          "href": "https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.get_linear_schedule_with_warmup"
        },
        {
          "text": "Learning Rate Warmup Paper",
          "href": "https://arxiv.org/abs/1706.02677"
        },
        {
          "text": "Fine-tuning Best Practices",
          "href": "https://www.sbert.net/examples/training/cross-encoder/README.html"
        },
        {
          "text": "Scheduler Visualization",
          "href": "https://huggingface.co/docs/transformers/main_classes/optimizer_schedules"
        }
      ],
      "badges": [
        {
          "text": "Advanced ML training",
          "class": "warn"
        },
        {
          "text": "Stabilizes training",
          "class": "info"
        }
      ]
    },
    {
      "term": "Rerank Backend",
      "key": "RERANK_BACKEND",
      "definition": "Reranks fused candidates for better ordering.\n• cohere — best quality, paid (COHERE_API_KEY)\n• local/hf — no cost (ensure model installed)\nDisable only to save cost.",
      "category": "reranking",
      "related": [],
      "links": [
        {
          "text": "Cohere Docs: Rerank",
          "href": "https://docs.cohere.com/reference/rerank"
        },
        {
          "text": "Cohere Python (GitHub)",
          "href": "https://github.com/cohere-ai/cohere-python"
        }
      ],
      "badges": []
    },
    {
      "term": "Rerank Snippet Length",
      "key": "RERANK_INPUT_SNIPPET_CHARS",
      "definition": "Maximum characters from each candidate chunk sent to the reranker. Keeps payloads within provider limits and focuses scoring on the most relevant prefix. Typical range: 400-1200 chars. Use 400-600 when providers reject long inputs or latency is critical; 800-1200 when answers depend on longer doc/context blocks. If set too low, quality drops from missing context; too high increases latency and rerank cost per request.",
      "category": "reranking",
      "related": [],
      "links": [
        {
          "text": "Voyage reranker token limits",
          "href": "https://docs.voyageai.com/docs/reranker"
        },
        {
          "text": "Cohere rerank context length",
          "href": "https://docs.cohere.com/docs/rerank"
        }
      ],
      "badges": [
        {
          "text": "Affects latency/cost",
          "class": "warn"
        },
        {
          "text": "Context guardrail",
          "class": "info"
        }
      ]
    },
    {
      "term": "Reciprocal Rank Fusion (K)",
      "key": "RRF_K_DIV",
      "definition": "Fusion parameter for combining BM25 + vector rankings: score += 1/(K+rank). Lower K increases influence of lower ranks; higher K flattens. Typical: 30–100 (60 recommended).",
      "category": "retrieval",
      "related": [],
      "links": [
        {
          "text": "RRF Paper",
          "href": "https://www.cs.cmu.edu/~jgc/publication/The_Influence_of_Random_Sampling_on_the_Performance_of_Ensembles.pdf"
        }
      ],
      "badges": []
    },
    {
      "term": "Run RAG Evaluation",
      "key": "RUN_EVAL_ANALYSIS",
      "definition": "Execute the full RAG evaluation suite using your current configuration settings. This runs all golden questions through the retrieval pipeline and measures Top-1 and Top-K accuracy. A live terminal will slide down showing real-time progress, and results will automatically appear in the Eval Analysis view when complete.",
      "category": "evaluation",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Uses current config",
          "class": "info"
        },
        {
          "text": "~1-5 min runtime",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Skip Dense Embeddings",
      "key": "SKIP_DENSE",
      "definition": "Skip vector embeddings and Qdrant during indexing to create a fast BM25-only (keyword-only) index. Useful for quick testing, CI/CD pipelines, or when Qdrant is unavailable. BM25-only mode is faster and uses less resources but loses semantic search capability - only exact keyword matches work. Not recommended for production use unless you have a purely keyword-based use case.",
      "category": "retrieval",
      "related": [],
      "links": [
        {
          "text": "Hybrid Search Benefits",
          "href": "https://www.pinecone.io/learn/hybrid-search-intro/"
        }
      ],
      "badges": [
        {
          "text": "Much faster",
          "class": "info"
        },
        {
          "text": "Keyword-only",
          "class": "warn"
        },
        {
          "text": "No semantic search",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Sparse Search Enabled",
      "key": "SPARSE_SEARCH_ENABLED",
      "definition": "Enable or disable sparse (BM25 keyword) search. When enabled, queries use lexical matching to find chunks containing exact keywords, variable names, function names, and error codes. When disabled, only vector and graph search are used. Recommended: enabled for most use cases. Disable only if you want pure semantic retrieval or are troubleshooting BM25 performance.\n\nSweet spot: enabled for production systems. Sparse search excels at exact keyword matching and finding specific code symbols. Disable temporarily if BM25 indexing is unavailable or causing latency issues.\n\n• Enabled: Full tri-brid retrieval (vector + sparse + graph)\n• Disabled: Dual-mode retrieval (vector + graph only)\n• Effect: Controls whether lexical keyword search contributes to results\n• Symptom if disabled: Exact keyword matches may be missed, especially for code symbols",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "BM25 Algorithm",
          "href": "https://en.wikipedia.org/wiki/Okapi_BM25"
        },
        {
          "text": "Hybrid Retrieval",
          "href": "https://www.elastic.co/search-labs/blog/improving-information-retrieval-elastic-stack-hybrid"
        },
        {
          "text": "Sparse vs Dense Retrieval",
          "href": "https://www.pinecone.io/learn/hybrid-search-intro/"
        }
      ],
      "badges": [
        {
          "text": "Core Setting",
          "class": "info"
        }
      ]
    },
    {
      "term": "Sparse Search Top-K",
      "key": "SPARSE_SEARCH_TOP_K",
      "definition": "Number of candidate results to retrieve from BM25 sparse search before fusion. Higher values (75-150) improve recall for exact keyword matches (variable names, function names, error codes) but increase query latency. Lower values (30-50) are faster but may miss exact matches. Must be >= FINAL_K. Recommended: 50 for balanced performance, 75-100 for keyword-heavy queries.\n\nSweet spot: 50-75 for production systems. Use 75-100 when exact keyword matching is critical (e.g., finding specific function names or error codes). Use 30-50 for cost-sensitive scenarios or when initial retrieval quality is already high.\n\n• Range: 10-200 (typical: 30-100)\n• Balanced: 50-75 (recommended)\n• High recall: 75-100 (keyword-heavy queries)\n• Cost-sensitive: 30-50 (faster, lower cost)\n• Effect: Higher = more keyword candidates, better recall, higher latency\n• Symptom too low: Exact keyword matches missed\n• Symptom too high: Slower queries, diminishing returns",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "BM25 Best Practices",
          "href": "https://www.elastic.co/blog/practical-bm25-part-3-considerations-for-picking-b-and-k1-in-elasticsearch"
        },
        {
          "text": "Top-K Retrieval",
          "href": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Top-K_retrieval"
        },
        {
          "text": "BM25 Weighting Scheme",
          "href": "https://xapian.org/docs/bm25.html"
        }
      ],
      "badges": [
        {
          "text": "Affects latency",
          "class": "info"
        },
        {
          "text": "Keyword matches",
          "class": "info"
        }
      ]
    },
    {
      "term": "Sparse Weight",
      "key": "SPARSE_WEIGHT",
      "definition": "Weight assigned to sparse BM25 search results in tri-brid fusion. Higher values prioritize keyword matches. Range: 0.0-1.0. Default: 0.3. Effective for exact identifier matching in code.",
      "category": "retrieval",
      "related": [
        "Tri-Brid Fusion",
        "Vector Weight",
        "Graph Weight"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "System Prompts",
      "key": "SYSTEM_PROMPTS_SUBTAB",
      "definition": "Edit LLM system prompts that control RAG pipeline behavior. These prompts are used for query expansion, chat responses, semantic card generation, code enrichment, and eval analysis. Changes are saved to tribrid_config.json (or per-corpus config) and take effect immediately.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Prompt Engineering",
          "href": "https://www.anthropic.com/news/prompt-engineering"
        }
      ],
      "badges": [
        {
          "text": "Live reload",
          "class": "success"
        }
      ]
    },
    {
      "term": "Containers (running/total)",
      "key": "SYS_STATUS_CONTAINERS",
      "definition": "Shows Docker container health for <span class=\"tt-strong\">this TriBridRAG stack</span> as <span class=\"mono\">running/total</span>.<br><br><span class=\"tt-strong\">running</span>: containers whose state is <span class=\"mono\">running</span><br><span class=\"tt-strong\">total</span>: all containers in the TriBridRAG docker-compose project (including stopped/exited)<br><br>This intentionally excludes unrelated containers on your machine. Internally we identify TriBrid-managed containers via Docker Compose labels (<span class=\"mono\">com.docker.compose.project</span>) and/or the <span class=\"mono\">tribrid-*</span> container name prefix.<br><br><span class=\"tt-strong\">Tip:</span> click the chip to open <span class=\"tt-strong\">Infrastructure → Docker</span> for full container management and logs.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Docker Compose docs",
          "href": "https://docs.docker.com/compose/"
        }
      ],
      "badges": [
        {
          "text": "Operational",
          "class": "info"
        }
      ]
    },
    {
      "term": "Corpora (active selection)",
      "key": "SYS_STATUS_CORPUS",
      "definition": "A <span class=\"tt-strong\">corpus</span> is TriBridRAG’s unit of isolation: each corpus has its own indexing storage (Postgres), graph storage (Neo4j), and per-corpus configuration.<br><br>This System Status row shows the <span class=\"tt-strong\">active corpus</span> and the <span class=\"tt-strong\">total number of corpora</span> registered in this TriBridRAG instance.<br><br><span class=\"tt-strong\">Selection precedence</span> (highest → lowest):<br>1) URL query param <span class=\"mono\">?corpus=</span> (or legacy <span class=\"mono\">?repo=</span>)<br>2) Browser localStorage <span class=\"mono\">tribrid_active_corpus</span><br>3) First corpus in the registry<br><br>Compatibility note: some API fields still use <span class=\"mono\">repo_id</span> as the identifier, but it means <span class=\"tt-strong\">corpus_id</span>.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Core concept",
          "class": "info"
        }
      ]
    },
    {
      "term": "MCP transports (stdio/HTTP)",
      "key": "SYS_STATUS_MCP_SERVERS",
      "definition": "MCP (Model Context Protocol) lets external clients (IDEs, agents, automation) call TriBridRAG tools in a standardized way.<br><br>This System Status chip lists which <span class=\"tt-strong\">inbound MCP transports</span> are available right now:<br>- <span class=\"mono\">py-stdio</span>: Python stdio transport. This is typically <span class=\"tt-strong\">client-spawned</span> (no always-on server). “available” means the required Python MCP runtime is installed and can be launched by an MCP client.<br>- <span class=\"mono\">py-http</span> / <span class=\"mono\">node-http</span>: future HTTP transports (will show host/port and running state when implemented).<br><br><span class=\"tt-strong\">Tip:</span> click the chip to open <span class=\"tt-strong\">Infrastructure → MCP Servers</span> for detailed status and setup guidance.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "MCP specification",
          "href": "https://github.com/modelcontextprotocol/specification"
        },
        {
          "text": "MCP overview",
          "href": "https://modelcontextprotocol.io"
        }
      ],
      "badges": [
        {
          "text": "Integration",
          "class": "info"
        }
      ]
    },
    {
      "term": "Table Name",
      "key": "TABLE_NAME",
      "definition": "Optional override for the pgvector table name where vectors are stored. Defaults to code_chunks_{REPO}. Set this if you maintain multiple profiles, A/B test embedding models, or run parallel indexing. Must be lowercase alphanumeric + underscore. Examples: code_chunks_v2, vectors_staging, embeddings_prod",
      "category": "general",
      "related": [
        "PostgreSQL pgvector URL"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "GUI Theme",
      "key": "THEME_MODE",
      "definition": "Color theme for web GUI. Options: \"light\" (light mode), \"dark\" (dark mode), \"auto\" (follows system preference). Changes appearance immediately when toggled.",
      "category": "ui",
      "related": [],
      "links": [
        {
          "text": "Dark Mode Benefits",
          "href": "https://en.wikipedia.org/wiki/Light-on-dark_color_scheme"
        }
      ],
      "badges": []
    },
    {
      "term": "Thread ID",
      "key": "THREAD_ID",
      "definition": "Unique identifier for conversation session state in LangGraph checkpoints or CLI chat. Use a stable value (e.g., \"session-123\", user email, UUID) to preserve chat history and context across runs. Different thread IDs create separate conversation contexts. Useful for multi-user systems or A/B testing different conversation flows. Stored in Redis when available.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "LangGraph Checkpoints",
          "href": "https://langchain-ai.github.io/langgraph/concepts/persistence/"
        },
        {
          "text": "Thread Management",
          "href": "https://langchain-ai.github.io/langgraph/how-tos/persistence/#threads"
        }
      ],
      "badges": []
    },
    {
      "term": "Timeout Errors (per 5 min)",
      "key": "TIMEOUT_ERRORS_THRESHOLD",
      "definition": "Maximum number of timeout errors allowed in a 5-minute window before triggering an alert. Timeout errors indicate requests that took too long and were forcibly terminated. Common causes: slow LLM APIs, overloaded database, network issues. Typical values: 10-20 for production, 50+ for development.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Timeout Best Practices",
          "href": "https://aws.amazon.com/builders-library/timeouts-retries-and-backoff-with-jitter/"
        }
      ],
      "badges": [
        {
          "text": "Reliability",
          "class": "err"
        }
      ]
    },
    {
      "term": "Top‑K Dense",
      "key": "TOPK_DENSE",
      "definition": "Number of candidate results to retrieve from Qdrant vector (semantic) search before hybrid fusion. Higher values (100-150) improve recall for semantic matches but increase query latency and memory usage. Lower values (40-60) are faster but may miss relevant results. Must be >= FINAL_K. Recommended: 75 for balanced performance, 100-120 for high recall scenarios.",
      "category": "retrieval",
      "related": [],
      "links": [
        {
          "text": "Vector Similarity Search",
          "href": "https://qdrant.tech/documentation/concepts/search/"
        },
        {
          "text": "Semantic Search",
          "href": "https://en.wikipedia.org/wiki/Semantic_search"
        },
        {
          "text": "Top-K Retrieval",
          "href": "https://en.wikipedia.org/wiki/Nearest_neighbor_search#k-nearest_neighbors"
        }
      ],
      "badges": [
        {
          "text": "Affects latency",
          "class": "info"
        },
        {
          "text": "Semantic matches",
          "class": "info"
        }
      ]
    },
    {
      "term": "Top‑K Sparse",
      "key": "TOPK_SPARSE",
      "definition": "Number of candidate results to retrieve from BM25 keyword (lexical) search before hybrid fusion. Higher values (100-150) improve recall for exact keyword matches (variable names, function names, error codes) but increase latency. Lower values (40-60) are faster but may miss exact matches. Must be >= FINAL_K. Recommended: 75 for balanced performance, 100-120 for keyword-heavy queries.",
      "category": "retrieval",
      "related": [],
      "links": [
        {
          "text": "BM25 Algorithm",
          "href": "https://en.wikipedia.org/wiki/Okapi_BM25"
        },
        {
          "text": "BM25S Library (GitHub)",
          "href": "https://github.com/xhluca/bm25s"
        }
      ],
      "badges": [
        {
          "text": "Affects latency",
          "class": "info"
        },
        {
          "text": "Keyword matches",
          "class": "info"
        }
      ]
    },
    {
      "term": "Auto-open LangSmith",
      "key": "TRACE_AUTO_LS",
      "definition": "UI convenience flag intended to auto-open LangSmith after a request (1=yes, 0=no). TriBridRAG does not currently implement LangSmith deep-linking; this setting is reserved for future integration.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "LangSmith Setup",
          "href": "https://docs.smith.langchain.com/"
        }
      ],
      "badges": []
    },
    {
      "term": "Trace Retention",
      "key": "TRACE_RETENTION",
      "definition": "Number of traces to retain in the in-memory ring buffer (10-500). Higher values preserve more history for debugging; lower values use less memory.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Data Retention",
          "href": "https://en.wikipedia.org/wiki/Data_retention"
        }
      ],
      "badges": []
    },
    {
      "term": "Trace Sampling Rate",
      "key": "TRACE_SAMPLING_RATE",
      "definition": "Percentage of requests to trace with LangSmith/observability (0.0-1.0). 1.0 = trace everything (100%), 0.1 = trace 10% of requests, 0.0 = no tracing. Lower sampling reduces LangSmith costs and overhead while still providing visibility into system behavior. Use 1.0 during development/debugging, 0.05-0.2 in production for cost-effective monitoring. Sampling is random - every request has this probability of being traced.\n\nRecommended: 1.0 for development, 0.1-0.2 for production monitoring, 0.05 for high-traffic systems.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "LangSmith Tracing",
          "href": "https://docs.smith.langchain.com/tracing"
        },
        {
          "text": "Sampling Strategies",
          "href": "https://docs.smith.langchain.com/tracing/faq#how-do-i-sample-traces"
        },
        {
          "text": "Trace Costs",
          "href": "https://www.langchain.com/pricing"
        }
      ],
      "badges": [
        {
          "text": "Cost control",
          "class": "info"
        },
        {
          "text": "Observability",
          "class": "info"
        }
      ]
    },
    {
      "term": "Tracing Enabled",
      "key": "TRACING_ENABLED",
      "definition": "Enable TriBridRAG request tracing. This records an in-memory per-request event trace (used by the UI “Routing Trace” preview) for debugging routing/retrieval decisions and latency. This does not export traces to external providers.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Distributed Tracing (concepts)",
          "href": "https://opentelemetry.io/docs/concepts/observability-primer/#distributed-traces"
        }
      ],
      "badges": []
    },
    {
      "term": "Tracing Mode",
      "key": "TRACING_MODE",
      "definition": "Controls tracing behavior. Options: \"off\" (disable tracing), \"local\" (local-only), \"langsmith\" (local traces + reserved for future LangSmith export). Alias: \"none\" is normalized to \"off\".",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "LangSmith",
          "href": "https://docs.smith.langchain.com/"
        }
      ],
      "badges": []
    },
    {
      "term": "Transformers: trust_remote_code",
      "key": "TRANSFORMERS_TRUST_REMOTE_CODE",
      "definition": "SECURITY WARNING: Set to \"true\" only if you completely trust the model source. Allows HuggingFace Transformers to execute arbitrary Python code from model repositories for custom architectures. Malicious models could run harmful code on your system. Only enable for models from verified sources (official HuggingFace, your organization). Required for some specialized models with custom model classes.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Security Notes",
          "href": "https://huggingface.co/docs/transformers/installation#security-notes"
        },
        {
          "text": "Custom Code in Models",
          "href": "https://huggingface.co/docs/transformers/custom_models"
        },
        {
          "text": "Model Security",
          "href": "https://huggingface.co/docs/hub/security"
        }
      ],
      "badges": [
        {
          "text": "Security risk",
          "class": "warn"
        },
        {
          "text": "Only for trusted models",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Edition",
      "key": "TRIBRID_EDITION",
      "definition": "Product edition identifier for feature gating in multi-tier deployments. Values: \"oss\" (open source, all community features), \"pro\" (professional tier with advanced features), \"enterprise\" (full feature set with support). This flag enables/disables certain UI elements and API endpoints based on licensing. Most users should leave this as \"oss\".",
      "category": "general",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Feature gating",
          "class": "info"
        }
      ]
    },
    {
      "term": "Tri-Brid Fusion",
      "key": "TRIBRID_FUSION",
      "definition": "The core search methodology combining three retrieval approaches: vector (dense), sparse (BM25), and graph traversal. Results from each method are fused using RRF or weighted scoring to produce final rankings. Configurable weights allow tuning for different codebases.",
      "category": "retrieval",
      "related": [
        "Vector Weight",
        "Sparse Weight",
        "Graph Weight",
        "RRF K"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Reranker Log Path",
      "key": "TRIBRID_LOG_PATH",
      "definition": "Directory where the reranker writes logs and training progress. Useful for monitoring and resuming experiments. Ensure the path is writable by the server process.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Python logging",
          "href": "https://docs.python.org/3/library/logging.html"
        }
      ],
      "badges": []
    },
    {
      "term": "Reranker Blend Alpha",
      "key": "TRIBRID_RERANKER_ALPHA",
      "definition": "Weight of the cross-encoder reranker score during final fusion. Higher alpha prioritizes semantic pairwise scoring; lower alpha relies more on initial hybrid retrieval (BM25 + dense). Typical range 0.6–0.8. Increasing alpha can improve ordering for nuanced queries but may surface false positives if your model is undertrained.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Cross-Encoder Overview (SBERT)",
          "href": "https://www.sbert.net/examples/applications/cross-encoder/README.html"
        },
        {
          "text": "Reciprocal Rank Fusion (RRF)",
          "href": "https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf"
        },
        {
          "text": "Hybrid Retrieval Concepts",
          "href": "https://qdrant.tech/articles/hybrid-search/"
        }
      ],
      "badges": [
        {
          "text": "Affects ranking",
          "class": "info"
        }
      ]
    },
    {
      "term": "Reranker Batch Size (Inference)",
      "key": "TRIBRID_RERANKER_BATCH",
      "definition": "Batch size used when scoring candidates during rerank. Higher values reduce latency but increase memory. If you see OOM or throttling, lower this value.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Batching Techniques",
          "href": "https://huggingface.co/docs/transformers/v4.44.2/en/perf_train_gpu_one#use-mixed-precision"
        },
        {
          "text": "Latency vs Throughput",
          "href": "https://en.wikipedia.org/wiki/Batch_processing"
        }
      ],
      "badges": [
        {
          "text": "Tune for memory",
          "class": "info"
        }
      ]
    },
    {
      "term": "Reranker Max Sequence Length (Inference)",
      "key": "TRIBRID_RERANKER_MAXLEN",
      "definition": "Maximum token length for each (query, text) pair during live reranking. Larger values increase memory/cost and may not improve quality beyond ~256–384 tokens for code. Use higher values for long comments/docs; lower for tight compute budgets.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Transformers Tokenization",
          "href": "https://huggingface.co/docs/transformers/main/en/tokenizer_summary"
        },
        {
          "text": "Sequence Length vs Memory",
          "href": "https://huggingface.co/docs/transformers/perf_train_gpu_one"
        }
      ],
      "badges": [
        {
          "text": "Performance sensitive",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Triplet Mining Mode",
      "key": "TRIBRID_RERANKER_MINE_MODE",
      "definition": "Strategy for mining training triplets: random, semi‑hard, or hard negatives. Harder negatives improve discriminative power but may be noisier and slower to mine.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Hard Negative Mining",
          "href": "https://sbert.net/examples/training/quora_duplicate_questions/README.html"
        }
      ],
      "badges": [
        {
          "text": "Advanced",
          "class": "info"
        }
      ]
    },
    {
      "term": "Reset Triplets Before Mining",
      "key": "TRIBRID_RERANKER_MINE_RESET",
      "definition": "If enabled, deletes existing mined triplets before starting a new mining run. Use with caution to avoid losing curated datasets.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Destructive",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Reranker Model Path",
      "key": "TRIBRID_RERANKER_MODEL_PATH",
      "definition": "Filesystem path to the trained reranker model checkpoint directory (relative paths recommended). The service loads weights from this path on startup or when reloaded.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Model Checkpoints",
          "href": "https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.from_pretrained"
        }
      ],
      "badges": []
    },
    {
      "term": "Reranker Auto-Reload",
      "key": "TRIBRID_RERANKER_RELOAD_ON_CHANGE",
      "definition": "Automatically reload the local reranker model when RERANKER_MODEL path changes during runtime (1=yes, 0=no). When enabled, the system detects model path changes and hot-reloads the new model without server restart. Useful during development when switching between reranker models or testing fine-tuned versions. In production, disable to avoid unexpected reloads and ensure stability. Model reloading adds 2-5 seconds of latency on first query after change.\n\nRecommended: 1 for development/testing, 0 for production deployments.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Hot Reload Patterns",
          "href": "https://en.wikipedia.org/wiki/Hot_swapping"
        }
      ],
      "badges": [
        {
          "text": "Development feature",
          "class": "info"
        },
        {
          "text": "Disable in production",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Reranker Top-N",
      "key": "TRIBRID_RERANKER_TOPN",
      "definition": "Maximum number of candidates to pass through the cross-encoder reranker stage during retrieval. After hybrid fusion (BM25 + dense), the top-N candidates are reranked using pairwise semantic scoring before final selection. Higher values (50-100) improve recall by considering more candidates but increase reranking latency and compute cost quadratically. Lower values (20-30) are faster but may miss relevant results that scored poorly in initial retrieval but would rank highly after reranking.\n\nSweet spot: 40-60 for most use cases. Use 60-80 for complex queries where initial ranking may be noisy (e.g., ambiguous natural language queries like \"where do we handle payments?\"). Use 20-40 for tight latency budgets or when initial hybrid retrieval is already high-quality. Reranking cost scales with top-N × query length, so monitor inference time when tuning this parameter.\n\nSymptom of too low: Relevant results appear when you increase top-K but not with default settings. Symptom of too high: Reranking takes >500ms and retrieval latency dominates response time. Most production systems use 40-50 as a balanced default.\n\n• Typical range: 20-80 candidates\n• Balanced default: 40-50 for most workloads\n• High recall: 60-80 for exploratory queries\n• Low latency: 20-30 for speed-critical apps\n• Reranking cost: O(top-N × tokens) per query",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Cross-Encoder Reranking",
          "href": "https://www.sbert.net/examples/applications/cross-encoder/README.html"
        },
        {
          "text": "Reranking in RAG",
          "href": "https://arxiv.org/abs/2407.21059"
        },
        {
          "text": "SBERT Reranking Docs",
          "href": "https://www.sbert.net/docs/cross_encoder/pretrained_models.html"
        },
        {
          "text": "Hybrid Search + Rerank",
          "href": "https://qdrant.tech/articles/hybrid-search/"
        }
      ],
      "badges": [
        {
          "text": "Advanced RAG tuning",
          "class": "info"
        },
        {
          "text": "Affects latency",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Synonyms File Path",
      "key": "TRIBRID_SYNONYMS_PATH",
      "definition": "Custom path to the semantic synonyms JSON file. Defaults to data/semantic_synonyms.json if empty. Use this to point to a repository-specific or custom synonym dictionary. The file should contain a JSON object mapping terms to arrays of synonyms (e.g., {\"auth\": [\"authentication\", \"oauth\", \"jwt\"]}).\n\n• Default: data/semantic_synonyms.json\n• Example: /path/to/custom_synonyms.json\n• Format: {\"term\": [\"synonym1\", \"synonym2\", ...]}\n• Works with: USE_SEMANTIC_SYNONYMS toggle",
      "category": "general",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Optional override",
          "class": "info"
        }
      ]
    },
    {
      "term": "Triplets Dataset Path",
      "key": "TRIBRID_TRIPLETS_PATH",
      "definition": "Path to mined triplets used for training the Learning Reranker. Triplets contain (query, positive, negative) examples. Keep under version control or in a reproducible data store.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Triplet Loss",
          "href": "https://en.wikipedia.org/wiki/Triplet_loss"
        },
        {
          "text": "SBERT Training Data",
          "href": "https://www.sbert.net/examples/training/cross-encoder/README.html"
        }
      ],
      "badges": []
    },
    {
      "term": "Triplets Mine Mode",
      "key": "TRIPLETS_MINE_MODE",
      "definition": "Strategy for mining negative examples when constructing training triplets from query logs and feedback. Negative examples are crucial for learning to rank - they teach the model what NOT to retrieve. Three strategies: \"random\" (random negatives from corpus), \"semi-hard\" (negatives that scored moderately but below positives), and \"hard\" (negatives that scored high but are actually irrelevant). Hard negatives are most effective but require careful mining to avoid false negatives.\n\n\"random\": Randomly sample documents from the corpus that aren't in the positive set. Fast and safe but produces easy negatives that don't challenge the model. Use for initial training or small datasets (<200 triplets). Converges quickly but may not improve ranking quality much beyond baseline.\n\n\"semi-hard\" (recommended): Mine negatives that scored in the 40th-70th percentile of retrieval results but weren't marked as relevant. These are plausible but wrong answers. Teaches the model nuanced distinctions. Balances training difficulty and false negative risk. Best for production systems with 500+ triplets.\n\n\"hard\": Use top-ranked results that are actually irrelevant as negatives. Most effective for learning but risky - if your relevance labels are noisy, you may train on false negatives (actually relevant docs mislabeled as negative). Use only with high-confidence human feedback or click data. Produces strongest rerankers when data quality is high.\n\n• random: Safe baseline, fast, easy negatives, less effective\n• semi-hard: Balanced default, good difficulty, low false negative risk (recommended)\n• hard: Maximum difficulty, best results, requires clean labels, high false negative risk\n• Effect on training: Harder negatives = slower convergence but better final quality\n• Combine with: TRIPLETS_MIN_COUNT (need more data for hard negatives)",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Hard Negative Mining",
          "href": "https://arxiv.org/abs/2104.08663"
        },
        {
          "text": "Negative Sampling Strategies",
          "href": "https://arxiv.org/abs/2007.00808"
        },
        {
          "text": "Triplet Mining (ACL 2025)",
          "href": "https://aclanthology.org/2025.acl-industry.72.pdf"
        },
        {
          "text": "Learning to Rank with Negatives",
          "href": "https://www.sbert.net/examples/training/cross-encoder/README.html"
        }
      ],
      "badges": [
        {
          "text": "Advanced training control",
          "class": "warn"
        },
        {
          "text": "Use semi-hard for production",
          "class": "info"
        }
      ]
    },
    {
      "term": "Triplets Min Count",
      "key": "TRIPLETS_MIN_COUNT",
      "definition": "Minimum number of training triplets (query, positive_doc, negative_doc) required to proceed with reranker training. Acts as a data quality gate - training with too few examples leads to severe overfitting and poor generalization. The reranker learns to distinguish relevant from irrelevant results, so it needs diverse examples to learn robust patterns. Standard minimum: 50-100 triplets for proof-of-concept, 500+ for production use.\n\nSweet spot: 200-500 triplets as a training threshold. With 200 triplets, you can run 2-3 epochs without severe overfitting. With 500+, you have enough diversity to learn generalizable patterns. Production systems should target 1000+ triplets from real user queries and feedback for best results. The quality of triplets matters more than quantity - 100 high-quality triplets from actual user interactions beat 500 synthetic triplets.\n\nTriplets are mined from your query logs, feedback data, or golden question sets using the triplet mining tools. Each triplet represents a learning signal: \"query A is more relevant to document B than document C.\" The reranker learns these preferences and generalizes to new queries. If training fails with \"insufficient data,\" increase your mining scope or lower this threshold temporarily for experimentation.\n\n• Absolute minimum: 50 triplets (proof-of-concept only)\n• Development minimum: 100-200 triplets\n• Production minimum: 500+ triplets (recommended)\n• Ideal: 1000-2000+ triplets for robust training\n• Quality over quantity: Real user data > synthetic examples\n• Symptom too low: Overfitting, poor generalization, reranker only works on training queries",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Triplet Loss for Ranking",
          "href": "https://arxiv.org/abs/1503.03832"
        },
        {
          "text": "Hard Negative Mining",
          "href": "https://arxiv.org/abs/2104.08663"
        },
        {
          "text": "Triplet Mining in RAG (ACL 2025)",
          "href": "https://aclanthology.org/2025.acl-industry.72.pdf"
        },
        {
          "text": "Learning to Rank",
          "href": "https://en.wikipedia.org/wiki/Learning_to_rank"
        }
      ],
      "badges": [
        {
          "text": "Data quality gate",
          "class": "warn"
        },
        {
          "text": "Production needs 500+",
          "class": "info"
        }
      ]
    },
    {
      "term": "Edition",
      "key": "TriBridRAG_EDITION",
      "definition": "Product edition identifier for feature gating in multi-tier deployments. Values: \"oss\" (open source, all community features), \"pro\" (professional tier with advanced features), \"enterprise\" (full feature set with support). This flag enables/disables certain UI elements and API endpoints based on licensing. Most users should leave this as \"oss\".",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Reranker Log Path",
      "key": "TriBridRAG_LOG_PATH",
      "definition": "Directory where the reranker writes logs and training progress. Useful for monitoring and resuming experiments. Ensure the path is writable by the server process.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Reranker Blend Alpha",
      "key": "TriBridRAG_RERANKER_ALPHA",
      "definition": "Weight of the cross-encoder reranker score during final fusion. Higher alpha prioritizes semantic pairwise scoring; lower alpha relies more on initial hybrid retrieval (BM25 + dense). Typical range 0.6–0.8. Increasing alpha can improve ordering for nuanced queries but may surface false positives if your model is undertrained.",
      "category": "reranking",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Reranker Batch Size (Inference)",
      "key": "TriBridRAG_RERANKER_BATCH",
      "definition": "Batch size used when scoring candidates during rerank. Higher values reduce latency but increase memory. If you see OOM or throttling, lower this value.",
      "category": "embedding",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Reranker Max Sequence Length (Inference)",
      "key": "TriBridRAG_RERANKER_MAXLEN",
      "definition": "Maximum token length for each (query, text) pair during live reranking. Larger values increase memory/cost and may not improve quality beyond ~256–384 tokens for code. Use higher values for long comments/docs; lower for tight compute budgets.",
      "category": "reranking",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Triplet Mining Mode",
      "key": "TriBridRAG_RERANKER_MINE_MODE",
      "definition": "Strategy for mining training triplets: random, semi\\u2011hard, or hard negatives. Harder negatives improve discriminative power but may be noisier and slower to mine.",
      "category": "reranking",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Reset Triplets Before Mining",
      "key": "TriBridRAG_RERANKER_MINE_RESET",
      "definition": "If enabled, deletes existing mined triplets before starting a new mining run. Use with caution to avoid losing curated datasets.",
      "category": "reranking",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Reranker Model Path",
      "key": "TriBridRAG_RERANKER_MODEL_PATH",
      "definition": "Filesystem path to the trained reranker model checkpoint directory (relative paths recommended). The service loads weights from this path on startup or when reloaded.",
      "category": "reranking",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Reranker Auto-Reload",
      "key": "TriBridRAG_RERANKER_RELOAD_ON_CHANGE",
      "definition": "Automatically reload the local reranker model when RERANKER_MODEL path changes during runtime (1=yes, 0=no). When enabled, the system detects model path changes and hot-reloads the new model without server restart. Useful during development when switching between reranker models or testing fine-tuned versions. In production, disable to avoid unexpected reloads and ensure stability. Model reloading adds 2-5 seconds of latency on first query after change.  Recommended: 1 for development/testing, 0",
      "category": "reranking",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Reranker Top-N",
      "key": "TriBridRAG_RERANKER_TOPN",
      "definition": "Maximum number of candidates to pass through the cross-encoder reranker stage during retrieval. After hybrid fusion (BM25 + dense), the top-N candidates are reranked using pairwise semantic scoring before final selection. Higher values (50-100) improve recall by considering more candidates but increase reranking latency and compute cost quadratically. Lower values (20-30) are faster but may miss relevant results that scored poorly in initial retrieval but would rank highly after reranking.  Swee",
      "category": "reranking",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Synonyms File Path",
      "key": "TriBridRAG_SYNONYMS_PATH",
      "definition": "Custom path to the semantic synonyms JSON file. Defaults to data/semantic_synonyms.json if empty. Use this to point to a repository-specific or custom synonym dictionary. The file should contain a JSON object mapping terms to arrays of synonyms (e.g., {\"auth\": [\"authentication\", \"oauth\", \"jwt\"]}).  \\u2022 Default: data/semantic_synonyms.json \\u2022 Example: /path/to/custom_synonyms.json \\u2022 Format: {\"term\": [\"synonym1\", \"synonym2\", ...]} \\u2022 Works with: USE_SEMANTIC_SYNONYMS toggle",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Triplets Dataset Path",
      "key": "TriBridRAG_TRIPLETS_PATH",
      "definition": "Path to mined triplets used for training the Learning Reranker. Triplets contain (query, positive, negative) examples. Keep under version control or in a reproducible data store.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Semantic Synonyms Expansion",
      "key": "USE_SEMANTIC_SYNONYMS",
      "definition": "Expands queries with curated domain synonyms and abbreviations (e.g., auth → authentication, oauth, jwt). Complements LLM rewrites. Configure in data/semantic_synonyms.json.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Vector Backend",
      "key": "VECTOR_BACKEND",
      "definition": "Selects the vector search backend used for dense retrieval. Postgres (pgvector) is the default backend in TriBridRAG and stores your embedding vectors for fast similarity search. Use this to switch between implementations when benchmarking or troubleshooting.",
      "category": "retrieval",
      "related": [],
      "links": [
        {
          "text": "Qdrant Docs",
          "href": "https://qdrant.tech/documentation/"
        },
        {
          "text": "LangChain Vector Stores",
          "href": "https://python.langchain.com/docs/integrations/vectorstores/"
        }
      ],
      "badges": [
        {
          "text": "Core Setting",
          "class": "info"
        }
      ]
    },
    {
      "term": "Vector Search Enabled",
      "key": "VECTOR_SEARCH_ENABLED",
      "definition": "Enable or disable vector (dense semantic) search using pgvector. When enabled, queries use embedding similarity to find semantically related chunks. When disabled, only sparse (BM25) and graph search are used. Recommended: enabled for most use cases. Disable only if you want pure keyword-based retrieval or are troubleshooting vector search performance.\n\nSweet spot: enabled for production systems. Vector search excels at finding conceptually related content even when exact keywords don't match. Disable temporarily if pgvector is unavailable or causing latency issues.\n\n• Enabled: Full tri-brid retrieval (vector + sparse + graph)\n• Disabled: Dual-mode retrieval (sparse + graph only)\n• Effect: Controls whether semantic similarity search contributes to results\n• Symptom if disabled: Semantic matches may be missed, especially for abstract queries",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "pgvector Documentation",
          "href": "https://github.com/pgvector/pgvector"
        },
        {
          "text": "Vector Search Optimization",
          "href": "https://neon.tech/docs/ai/ai-vector-search-optimization"
        },
        {
          "text": "Semantic Search",
          "href": "https://en.wikipedia.org/wiki/Semantic_search"
        }
      ],
      "badges": [
        {
          "text": "Core Setting",
          "class": "info"
        }
      ]
    },
    {
      "term": "Vector Search Top-K",
      "key": "VECTOR_SEARCH_TOP_K",
      "definition": "Number of candidate results to retrieve from pgvector vector search before fusion. Higher values (75-150) improve recall for semantic matches but increase query latency and memory usage. Lower values (30-50) are faster but may miss relevant results. Must be >= FINAL_K. Recommended: 50 for balanced performance, 75-100 for high recall scenarios.\n\nSweet spot: 50-75 for production systems. Use 75-100 when semantic matching is critical (e.g., finding conceptually similar code patterns). Use 30-50 for cost-sensitive scenarios or when initial retrieval quality is already high.\n\n• Range: 10-200 (typical: 30-100)\n• Balanced: 50-75 (recommended)\n• High recall: 75-100 (semantic-heavy queries)\n• Cost-sensitive: 30-50 (faster, lower cost)\n• Effect: Higher = more semantic candidates, better recall, higher latency\n• Symptom too low: Relevant semantic matches missed\n• Symptom too high: Slower queries, memory pressure, diminishing returns",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "pgvector Optimization",
          "href": "https://neon.tech/blog/optimizing-vector-search-performance-with-pgvector"
        },
        {
          "text": "Top-K Retrieval",
          "href": "https://en.wikipedia.org/wiki/Nearest_neighbor_search#k-nearest_neighbors"
        },
        {
          "text": "Vector Search Performance",
          "href": "https://aws.amazon.com/blogs/database/supercharging-vector-search-performance-and-relevance-with-pgvector-0-8-0-on-amazon-aurora-postgresql/"
        }
      ],
      "badges": [
        {
          "text": "Affects latency",
          "class": "info"
        },
        {
          "text": "Semantic matches",
          "class": "info"
        }
      ]
    },
    {
      "term": "Vector Similarity Threshold",
      "key": "VECTOR_SIMILARITY_THRESHOLD",
      "definition": "Minimum cosine similarity score (0.0-1.0) required to include a vector search result. Results below this threshold are filtered out before fusion. 0.0 = no threshold (all results included). Higher values (0.5-0.7) ensure only highly similar results pass, improving precision but reducing recall. Lower values (0.0-0.3) allow more diverse results.\n\nSweet spot: 0.0 for most use cases (let fusion handle filtering). Use 0.5-0.6 when you want to aggressively filter low-quality semantic matches. Use 0.7+ only for precision-critical scenarios where false positives are costly.\n\n• Range: 0.0-1.0 (typical: 0.0-0.6)\n• No filtering: 0.0 (recommended, let fusion decide)\n• Moderate filtering: 0.4-0.5 (reduce noise)\n• Aggressive filtering: 0.6-0.7 (precision-critical)\n• Effect: Higher = fewer results, higher precision, lower recall\n• Symptom too high: Relevant semantic matches filtered out\n• Symptom too low: Noise included, fusion must handle filtering",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Cosine Similarity Threshold",
          "href": "https://www.emergentmind.com/topics/cosine-similarity-threshold"
        },
        {
          "text": "Vector Similarity Search",
          "href": "https://en.wikipedia.org/wiki/Cosine_similarity"
        },
        {
          "text": "Score Filtering",
          "href": "https://ui.adsabs.harvard.edu/abs/2018arXiv181207695L/abstract"
        }
      ],
      "badges": [
        {
          "text": "Precision tuning",
          "class": "info"
        }
      ]
    },
    {
      "term": "Vector Weight (Hybrid Fusion)",
      "key": "VECTOR_WEIGHT",
      "definition": "Weight assigned to dense vector (semantic embedding) scores during hybrid search fusion. Dense embeddings capture semantic meaning and conceptual similarity, excelling at natural language queries and synonym matching. Higher weights (0.5-0.7) prioritize semantic relevance over exact keywords. Lower weights (0.2-0.4) defer to BM25 lexical matching. The fusion formula: final_score = (BM25_WEIGHT × bm25_score) + (VECTOR_WEIGHT × dense_score).\n\nSweet spot: 0.5-0.6 for balanced hybrid retrieval. Use 0.6-0.7 when users ask conceptual questions (\"how does X work?\", \"what handles Y?\") where synonyms and paraphrasing matter. Use 0.4-0.5 when exact term matching is important alongside semantics. The two weights should sum to approximately 1.0 for normalized scoring.\n\nSymptom of too high: Exact keyword matches (function names, specific terms) rank below semantic near-matches. Symptom of too low: Conceptually relevant results are buried despite being semantically similar. Most production RAG systems balance 0.5 BM25 with 0.5 vector, then fine-tune based on user feedback and eval metrics.\n\n• Range: 0.3-0.7 (typical)\n• Semantic-heavy: 0.6-0.7 (conceptual queries, natural language)\n• Balanced: 0.5-0.6 (recommended for mixed queries)\n• Keyword-heavy: 0.3-0.4 (when precision matters)\n• Should sum with BM25_WEIGHT to ~1.0\n• Affects: Hybrid fusion ranking, semantic vs keyword balance",
      "category": "retrieval",
      "related": [
        "Tri-Brid Fusion",
        "Sparse Weight",
        "Graph Weight"
      ],
      "links": [
        {
          "text": "Dense Embeddings",
          "href": "https://www.sbert.net/docs/pretrained_models.html"
        },
        {
          "text": "Hybrid Search Explained",
          "href": "https://qdrant.tech/articles/hybrid-search/"
        },
        {
          "text": "Semantic Search",
          "href": "https://en.wikipedia.org/wiki/Semantic_search"
        },
        {
          "text": "Embedding Models",
          "href": "https://weaviate.io/blog/how-to-choose-an-embedding-model"
        }
      ],
      "badges": [
        {
          "text": "Advanced RAG tuning",
          "class": "info"
        },
        {
          "text": "Pairs with BM25_WEIGHT",
          "class": "info"
        }
      ]
    },
    {
      "term": "Vendor Mode",
      "key": "VENDOR_MODE",
      "definition": "Controls scoring preference for your code vs third-party library code during reranking. \"prefer_first_party\" (recommended) boosts your app code (+0.06) and penalizes node_modules/vendor libs (-0.08) - best for understanding YOUR codebase. \"prefer_vendor\" does the opposite - useful when debugging library internals or learning from open-source code. Most users want prefer_first_party.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "First-Party vs Third-Party",
          "href": "https://en.wikipedia.org/wiki/First-party_and_third-party_sources"
        }
      ],
      "badges": [
        {
          "text": "Code priority",
          "class": "info"
        }
      ]
    },
    {
      "term": "Vendor Penalty",
      "key": "VENDOR_PENALTY",
      "definition": "Score penalty (negative bonus) applied to third-party library code (node_modules, vendor/, site-packages/, etc.) during reranking when VENDOR_MODE is set to prefer_first_party. Helps prioritize your application code over external dependencies. Typical range: -0.05 to -0.12. Higher penalties (more negative) push library code down the rankings more aggressively.\n\nSweet spot: -0.08 to -0.10 for production systems. Use -0.10 to -0.12 for strong first-party preference when you want library code only as fallback. Use -0.05 to -0.08 for moderate preference when library examples are sometimes helpful. Set to 0.0 to disable vendor detection entirely (all code ranked equally).\n\nVendor detection matches common patterns: node_modules/, vendor/, .venv/, site-packages/, bower_components/, Pods/, third_party/. The penalty is applied during final reranking after hybrid fusion. Pair with path boosts in repos.json to further prioritize your core application directories. Most users want to understand THEIR code first, then library internals.\n\n• Range: -0.12 to 0.0 (negative = penalty)\n• No penalty: 0.0 (rank libraries equally)\n• Moderate preference: -0.05 to -0.08\n• Balanced: -0.08 to -0.10 (recommended)\n• Strong first-party: -0.10 to -0.12\n• Opposite mode: Set VENDOR_MODE=prefer_vendor to boost libraries instead",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Path Patterns",
          "href": "https://github.com/github/gitignore"
        },
        {
          "text": "First-Party vs Third-Party",
          "href": "https://en.wikipedia.org/wiki/First-party_and_third-party_sources"
        }
      ],
      "badges": [
        {
          "text": "Advanced RAG tuning",
          "class": "info"
        },
        {
          "text": "Code priority control",
          "class": "info"
        }
      ]
    },
    {
      "term": "Voyage API Key",
      "key": "VOYAGE_API_KEY",
      "definition": "API key for Voyage AI embeddings when EMBEDDING_TYPE=voyage.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Voyage AI Docs",
          "href": "https://docs.voyageai.com/"
        }
      ],
      "badges": []
    },
    {
      "term": "Voyage Embed Dim",
      "key": "VOYAGE_EMBED_DIM",
      "definition": "Embedding vector dimension when using Voyage embeddings (provider‑specific). Larger dims can improve recall but increase Qdrant storage. Must match the output dimension of your chosen Voyage model (e.g., voyage-code-2 uses 1536 dims).",
      "category": "embedding",
      "related": [],
      "links": [
        {
          "text": "Voyage Embeddings API",
          "href": "https://docs.voyageai.com/docs/embeddings"
        },
        {
          "text": "Vector Dimensionality",
          "href": "https://www.sbert.net/docs/pretrained_models.html#model-overview"
        },
        {
          "text": "Qdrant Storage Config",
          "href": "https://qdrant.tech/documentation/concepts/collections/"
        }
      ],
      "badges": [
        {
          "text": "Requires reindex",
          "class": "reindex"
        }
      ]
    },
    {
      "term": "Voyage Embedding Model",
      "key": "VOYAGE_MODEL",
      "definition": "Voyage AI embedding model when EMBEDDING_TYPE=voyage. Options: \"voyage-code-2\" (1536 dims, optimized for code, recommended), \"voyage-3\" (1024 dims, general-purpose, fast), \"voyage-3-lite\" (512 dims, budget option). Voyage models are specialized for code retrieval and often outperform OpenAI on technical queries. Code-specific models understand programming constructs, API patterns, and documentation better than general embeddings.\n\nRecommended: voyage-code-2 for code-heavy repos, voyage-3 for mixed content (code + docs).",
      "category": "generation",
      "related": [],
      "links": [
        {
          "text": "Voyage Embeddings API",
          "href": "https://docs.voyageai.com/docs/embeddings"
        },
        {
          "text": "voyage-code-2 Details",
          "href": "https://docs.voyageai.com/docs/voyage-code-2"
        },
        {
          "text": "Model Comparison",
          "href": "https://docs.voyageai.com/docs/model-comparison"
        }
      ],
      "badges": [
        {
          "text": "Requires reindex",
          "class": "reindex"
        },
        {
          "text": "Code-optimized",
          "class": "info"
        }
      ]
    },
    {
      "term": "Voyage Rerank Model",
      "key": "VOYAGE_RERANK_MODEL",
      "definition": "Voyage AI reranker model name when RERANK_BACKEND=voyage. Current option: \"rerank-2\" (latest, best quality). Voyage rerankers are cross-encoders that score (query, document) pairs for precise relevance ranking. Generally more accurate than open-source rerankers but costs per API call. Use when retrieval quality is critical and budget allows. Pricing is per rerank request (typically $0.05-0.10 per 1000 candidates).\n\nRecommended: Use Voyage reranking for production systems with quality requirements; use local rerankers (RERANKER_MODEL) for development/testing.",
      "category": "reranking",
      "related": [],
      "links": [
        {
          "text": "Voyage Rerank API",
          "href": "https://docs.voyageai.com/docs/reranker"
        },
        {
          "text": "rerank-2 Details",
          "href": "https://docs.voyageai.com/docs/rerank-2"
        },
        {
          "text": "Pricing",
          "href": "https://docs.voyageai.com/docs/pricing"
        }
      ],
      "badges": [
        {
          "text": "Costs API calls",
          "class": "warn"
        },
        {
          "text": "High quality",
          "class": "info"
        }
      ]
    },
    {
      "term": "Max tokens (response limit)",
      "key": "chat.max_tokens",
      "definition": "Hard cap on how long a single chat response is allowed to be. If the model hits this limit, the answer will end early even if it hasn’t finished the thought.<br><br>Raising this gives room for long explanations, multi-step reasoning, and larger code examples. It also tends to increase latency and cost, and it can crowd out retrieved context on smaller-context models.<br><br>If you notice answers cutting off mid-sentence or skipping important details, increase it. If you want snappier, shorter replies, decrease it.<br><br>Default: <span class=\"mono\">4096</span>. Range: <span class=\"mono\">100–16384</span>.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Vision enabled",
      "key": "chat.multimodal.vision_enabled",
      "definition": "Enables image attachments for chat messages. When on, the UI can send images along with your message to a vision-capable model.<br><br>If your selected model/provider doesn’t support vision, images won’t be useful (and may be ignored or rejected depending on the provider).<br><br>Images can increase latency and cost. If you don’t need screenshots/diagrams in chat, turning this off keeps the interface simpler.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Auto-index conversations",
      "key": "chat.recall.auto_index",
      "definition": "<span class=\"tt-strong\">Recall</span> is TriBridRAG’s persistent chat memory. It stores your conversation as a special internal corpus (typically <span class=\"mono\">recall_default</span>) so it can be searched like any other source.<br><br>When auto-indexing is enabled, TriBridRAG schedules a background indexing pass after each completed exchange so the latest messages become searchable Recall memory.<br><br>Indexing only runs when Recall is actually selected in Sources — that keeps Recall “opt-in” at the moment you’re using it.<br><br>If you disable this, existing Recall memory can still be queried, but new messages won’t be added automatically.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Index delay (seconds)",
      "key": "chat.recall.index_delay_seconds",
      "definition": "How long TriBridRAG waits before running the background Recall indexing job after a response completes.<br><br>A shorter delay makes it more likely that Recall can immediately “remember” the last exchange if your next message references it. A longer delay smooths out background work if you’re sending messages quickly (less embedding/DB churn right away).<br><br>Default: <span class=\"mono\">5</span>. Range: <span class=\"mono\">1–60</span>.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Deep on explicit reference",
      "key": "chat.recall_gate.deep_on_explicit_reference",
      "definition": "When enabled, the gate escalates to <span class=\"tt-strong\">deep</span> mode if your message explicitly references earlier conversation (“we discussed…”, “you mentioned…”, “last time…”, “remember when…”).<br><br>This is where Recall tends to pay for itself: you’re telling the assistant that the answer depends on prior context, so pulling more history is usually worth it.",
      "category": "retrieval",
      "related": [
        "Default Recall Intensity",
        "Deep top_k"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Deep recency weight",
      "key": "chat.recall_gate.deep_recency_weight",
      "definition": "Recency bias for deep Recall. Deep mode is often about retrieving decisions and conclusions from the recent thread, so this usually runs higher than the standard value.<br><br>Raise it if “what did we decide?” should strongly prefer the latest discussion. Lower it if deep mode should reach further back in time and not over-favor the last few turns.<br><br>Default: <span class=\"mono\">0.5</span>. Range: <span class=\"mono\">0.0–1.0</span>.",
      "category": "retrieval",
      "related": [
        "Deep top_k",
        "Standard recency weight"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Deep top_k",
      "key": "chat.recall_gate.deep_top_k",
      "definition": "How many Recall snippets to include in <span class=\"tt-strong\">deep</span> mode.<br><br>Deep is designed for “look back” questions. Larger values help capture multi-message decisions and longer threads, but they can also crowd out code context if you’re using RAG at the same time.<br><br>Default: <span class=\"mono\">10</span>. Range: <span class=\"mono\">3–30</span>.",
      "category": "retrieval",
      "related": [
        "Deep recency weight",
        "Standard top_k"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Default intensity",
      "key": "chat.recall_gate.default_intensity",
      "definition": "The “no strong signal” fallback. When the gate doesn’t see a clear reason to skip/light/deep, it uses this intensity.<br><br>Choosing a default is mostly about what you want normal chat to feel like:<br>• <span class=\"tt-strong\">standard</span> is the balanced baseline<br>• <span class=\"tt-strong\">light</span> keeps Recall very cheap and avoids over-injecting history<br>• <span class=\"tt-strong\">deep</span> makes Recall more present, which can help long-running collaborations but can also flood the prompt with old context<br>• <span class=\"tt-strong\">skip</span> effectively disables Recall unless you manually override per message<br><br>This only affects Recall. Code corpora retrieval is unchanged.",
      "category": "retrieval",
      "related": [
        "Recall Gate Enabled",
        "Deep on Explicit Reference"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Enable smart gating (Recall)",
      "key": "chat.recall_gate.enabled",
      "definition": "Recall is powerful, but querying it on every single message can add noise and slow chat down. Smart gating makes Recall <span class=\"tt-strong\">message-aware</span>: it looks at cheap signals (patterns + approximate word count) and decides when Recall is worth querying.<br><br>When enabled, each message gets an intensity:<br>• <span class=\"tt-strong\">skip</span>: don’t query Recall at all<br>• <span class=\"tt-strong\">light</span>: quick check using sparse-only Recall + small <span class=\"mono\">top_k</span><br>• <span class=\"tt-strong\">standard</span>: normal Recall query with balanced <span class=\"mono\">top_k</span> and recency bias<br>• <span class=\"tt-strong\">deep</span>: larger <span class=\"mono\">top_k</span> and stronger recency bias for “what did we decide?” moments<br><br>This gate only affects <span class=\"tt-strong\">Recall</span>. Any non-Recall corpora you’ve checked are still queried as usual.<br><br>If you turn gating off, TriBridRAG stops making per-message decisions and will query Recall whenever it’s checked, using the default intensity.",
      "category": "retrieval",
      "related": [
        "Default Recall Intensity",
        "Skip Greetings (Recall Gate)"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Light for short questions",
      "key": "chat.recall_gate.light_for_short_questions",
      "definition": "Enables a cheap Recall check for short questions that look like follow-ups (for example: “what about caching?” right after a discussion).<br><br>Light mode is intentionally minimal: it uses sparse-only Recall and a small <span class=\"mono\">top_k</span>, which is often enough to surface the one or two relevant prior messages without flooding the prompt.",
      "category": "retrieval",
      "related": [
        "Light top_k",
        "Skip Standalone Questions (Recall Gate)"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Light top_k",
      "key": "chat.recall_gate.light_top_k",
      "definition": "How many Recall snippets to include when intensity is <span class=\"tt-strong\">light</span>.<br><br>Light is meant to be a quick sanity check. Keeping this small avoids pulling too much history into the prompt. If light mode feels like it’s “almost” finding the right memory, increasing this a bit is usually the first adjustment to try.<br><br>Default: <span class=\"mono\">3</span>. Range: <span class=\"mono\">1–10</span>.",
      "category": "retrieval",
      "related": [
        "Light for Short Questions",
        "Standard top_k"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Show decision in status bar",
      "key": "chat.recall_gate.show_gate_decision",
      "definition": "Shows the selected Recall intensity and a short human-readable reason in the chat status bar after each message (for example: “Greeting — skipping Recall”).<br><br>Useful when you’re tuning Recall behavior or trying to understand why a particular answer did (or didn’t) use chat memory. This does not change retrieval; it only makes the decision visible.",
      "category": "ui",
      "related": [
        "Recall Gate Enabled",
        "Show Raw Signals"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Show raw signals (dev)",
      "key": "chat.recall_gate.show_signals",
      "definition": "Adds a “signals” expander in the chat debug footer that dumps the full RecallPlan JSON (signals, decision, and overrides).<br><br>This is meant for tuning and debugging. It’s intentionally noisy and will expose internal heuristics like approximate token count and pattern matches.",
      "category": "ui",
      "related": [
        "Show Gate Decision",
        "Recall Gate Enabled"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Skip greetings/acknowledgments",
      "key": "chat.recall_gate.skip_greetings",
      "definition": "Skips Recall for “pure conversation glue” like “hi”, “thanks”, “ok”, “got it”, and quick farewells. These almost never need memory, and pulling Recall for them tends to add irrelevant snippets.<br><br>If your workflow uses very short messages as meaningful follow-ups (for example, you often say “yes” to confirm a technical plan and expect the assistant to remember it), you may prefer to disable this so Recall stays in play more often.",
      "category": "retrieval",
      "related": [
        "Recall Gate Enabled",
        "Skip Max Tokens (Recall Gate)"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Max short-statement tokens",
      "key": "chat.recall_gate.skip_max_tokens",
      "definition": "A small threshold used by the gate to decide when a very short <span class=\"tt-strong\">statement</span> should trigger a light Recall check.<br><br>Messages at or under this length (approximate word-count) and not phrased as questions are often follow-ups like “do that”, “use the second one”, “ship it”. A light Recall check can help the assistant stay aligned with what you were just discussing without doing a heavy memory pull.<br><br>Lower values make fewer short statements trigger Recall. Higher values make the gate more eager to do a light check.<br><br>Default: <span class=\"mono\">4</span>. Range: <span class=\"mono\">1–20</span>.",
      "category": "retrieval",
      "related": [
        "Skip Greetings (Recall Gate)",
        "Light for Short Questions"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Skip standalone questions",
      "key": "chat.recall_gate.skip_standalone_questions",
      "definition": "Skips Recall for questions that usually make sense without chat history (patterns like “what is…”, “how does…”, “explain…”, “where is…”). This keeps Recall from injecting unrelated older context into otherwise clean questions.<br><br>If your conversations frequently build on prior decisions and even “basic-looking” questions depend on earlier context, disable this and rely on intensity controls to manage how much Recall is pulled in.",
      "category": "retrieval",
      "related": [
        "Recall Gate Enabled",
        "Deep on Explicit Reference"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Skip when RAG active",
      "key": "chat.recall_gate.skip_when_rag_active",
      "definition": "When you already have one or more code corpora checked, you’re asking for repo-grounded answers. In that situation, Recall can sometimes crowd out code context or mix in old conversation threads.<br><br>Enabling this makes Recall “all-or-nothing”: if any non-Recall corpora are checked, Recall is skipped entirely. Leaving it off allows RAG + Recall to cooperate (code + prior decisions) when that’s useful.",
      "category": "retrieval",
      "related": [
        "Recall Gate Enabled",
        "Skip Standalone Questions (Recall Gate)"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Standard recency weight",
      "key": "chat.recall_gate.standard_recency_weight",
      "definition": "Balances “most similar” vs “most recent” when ranking Recall results.<br><br>At <span class=\"mono\">0.0</span>, Recall ranking is purely similarity-based. As you raise it, newer messages get a stronger boost, which often matches how people reference conversations (“the thing we just talked about”).<br><br>If Recall keeps surfacing old-but-related snippets when you meant “the latest decision”, raise this. If it keeps over-prioritizing recent noise, lower it.<br><br>Default: <span class=\"mono\">0.3</span>. Range: <span class=\"mono\">0.0–1.0</span>.",
      "category": "retrieval",
      "related": [
        "Deep recency weight",
        "Standard top_k"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Standard top_k",
      "key": "chat.recall_gate.standard_top_k",
      "definition": "How many Recall snippets to include in the normal <span class=\"tt-strong\">standard</span> mode.<br><br>Higher values give the model more history to work with, but it also increases the chance of dragging in off-topic messages and eating into the context window.<br><br>Default: <span class=\"mono\">5</span>. Range: <span class=\"mono\">1–20</span>.",
      "category": "retrieval",
      "related": [
        "Standard recency weight",
        "Light top_k"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "System prompt (base)",
      "key": "chat.system_prompt_base",
      "definition": "This is the <span class=\"tt-strong\">legacy</span> system prompt path. TriBridRAG normally chooses one of the four “state” prompts (Direct / RAG / Recall / RAG+Recall) based on what context is actually present for the message.<br><br>If the selected state prompt is empty, TriBridRAG falls back to composing a prompt as:<br>• base prompt (this field)<br>• plus an internal Recall suffix when Recall context is present<br>• plus an internal RAG suffix when code context is present<br><br>Use the base prompt for the stable “personality” and hard constraints you always want. Keep it tight: every extra paragraph here competes with retrieved context and with the answer length.<br><br>If you want full control, fill in all four state prompts. If you want the old behavior for a specific state, leave that state prompt blank so this base+suffix fallback runs.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "System prompt: Direct (no context)",
      "key": "chat.system_prompt_direct",
      "definition": "Used when the message is answered <span class=\"tt-strong\">without any retrieval context</span> — no code corpora results and no Recall snippets.<br><br>In this state the model only sees the user’s message, so this prompt should set expectations clearly: it can explain general concepts, but it should not pretend it has looked at the user’s repo unless sources are enabled.<br><br>A good Direct prompt usually encourages a tight feedback loop: if the user asks about “our code”, it should suggest enabling corpora (and/or Recall) or asking for a file path so the answer can be grounded.<br><br>If this prompt is blank, TriBridRAG falls back to the base prompt composition for this state.",
      "category": "generation",
      "related": [
        "Chat System Prompt (RAG Only)",
        "Chat System Prompt (Recall Only)",
        "Chat System Prompt (RAG + Recall)"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "System prompt: RAG only",
      "key": "chat.system_prompt_rag",
      "definition": "Used when code corpora are queried and retrieval returns results. The model receives code snippets inside <span class=\"mono\">&lt;rag_context&gt;...&lt;/rag_context&gt;</span>.<br><br>Each snippet is labeled with a file path and line range (for example: <span class=\"mono\">server/auth/middleware.py:45-78</span>) and followed by a fenced code block.<br><br>This prompt should keep the model disciplined:<br>• treat retrieved code as the source of truth<br>• cite file paths and line ranges when making claims<br>• say what’s missing when the snippets don’t fully answer the question<br>• avoid inventing code that isn’t shown<br><br>If blank, TriBridRAG falls back to base prompt composition for this state.",
      "category": "generation",
      "related": [
        "Chat System Prompt (Direct)",
        "Chat System Prompt (RAG + Recall)"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "System prompt: RAG + Recall",
      "key": "chat.system_prompt_rag_and_recall",
      "definition": "Used when both code context and Recall context are present. The model receives:<br>• <span class=\"mono\">&lt;rag_context&gt;...&lt;/rag_context&gt;</span> (code snippets)<br>• <span class=\"mono\">&lt;recall_context&gt;...&lt;/recall_context&gt;</span> (prior conversation)<br><br>This is the “ongoing collaboration” state: code context anchors what the repo does today, while Recall brings in decisions, preferences, and prior conclusions.<br><br>If the two disagree, the safest behavior is to acknowledge the mismatch and prioritize the retrieved code for implementation reality, while still honoring the user’s stated intent from Recall.<br><br>If blank, TriBridRAG falls back to base prompt composition for this state.",
      "category": "generation",
      "related": [
        "Chat System Prompt (RAG Only)",
        "Chat System Prompt (Recall Only)",
        "Recall Gate Enabled"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "System prompt: Recall only",
      "key": "chat.system_prompt_recall",
      "definition": "Used when Recall (chat memory) is queried and returns results, but no code corpora results are present. The model receives conversation snippets inside <span class=\"mono\">&lt;recall_context&gt;...&lt;/recall_context&gt;</span>.<br><br>Recall is search-based: it returns the most relevant fragments, not a perfect transcript. This prompt should encourage the model to use the snippets as shared context while staying careful about uncertainty and missing history.<br><br>If blank, TriBridRAG falls back to base prompt composition for this state.",
      "category": "generation",
      "related": [
        "Chat System Prompt (Direct)",
        "Recall Gate Enabled"
      ],
      "links": [],
      "badges": []
    },
    {
      "term": "Temperature (with retrieval)",
      "key": "chat.temperature",
      "definition": "Controls how “random” generation is when <span class=\"tt-strong\">any sources are enabled</span> (RAG corpora and/or Recall). Retrieval decides what context is shown; temperature decides how strictly the model sticks to a single best continuation.<br><br>Lower values tend to produce more consistent, literal, citation-friendly answers. Higher values can be useful for exploring options, but they also make it easier for the model to drift away from the retrieved context.<br><br>Practical tuning:<br>• <span class=\"mono\">0.0–0.3</span>: precise code Q&A, fewer creative leaps (common default)<br>• <span class=\"mono\">0.4–0.7</span>: balanced explanations + light ideation<br>• <span class=\"mono\">0.8+</span>: brainstorming (expect more variance)<br><br>Default: <span class=\"mono\">0.3</span>. Range: <span class=\"mono\">0.0–2.0</span>.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Temperature (no retrieval)",
      "key": "chat.temperature_no_retrieval",
      "definition": "Controls generation when the message is answered in “Direct chat” — no corpora checked (no retrieval context). This is intentionally separate so direct conversation can feel more fluid without making retrieval-grounded answers overly creative.<br><br>If you want direct-chat answers to stay conservative and literal, lower this. If you use Direct chat for exploration and planning, keeping it higher often feels better.<br><br>As soon as you enable corpora and/or Recall for a message, TriBridRAG uses <span class=\"mono\">chat.temperature</span> instead.<br><br>Default: <span class=\"mono\">0.7</span>. Range: <span class=\"mono\">0.0–2.0</span>.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Recall intensity (next message)",
      "key": "chat_recall_intensity",
      "definition": "Per-message override for how aggressively to query <span class=\"tt-strong\">Recall</span> (chat memory) on the <span class=\"tt-strong\">next</span> send.<br><br>Options:<br>• <span class=\"tt-strong\">auto</span>: let the Recall gate decide per message<br>• <span class=\"tt-strong\">skip this message</span>: don’t query Recall at all<br>• <span class=\"tt-strong\">light</span>: sparse-only Recall with small <span class=\"mono\">top_k</span><br>• <span class=\"tt-strong\">standard</span>: normal Recall query<br>• <span class=\"tt-strong\">deep</span>: larger Recall pull with stronger recency bias<br><br>This only matters when Recall is checked as a source. After you send a message with a non-auto selection, the UI resets back to <span class=\"tt-strong\">auto</span>.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Neo4j Auto-Create Databases",
      "key": "neo4j_auto_create_databases",
      "definition": "Automatically create per-corpus Neo4j databases when missing (Enterprise multi-database only). When enabled, creating a corpus will create its Neo4j database automatically if it doesn't exist. When disabled, databases must be created manually before indexing. Requires Neo4j Enterprise Edition and database_mode=\"per_corpus\".\n\nSweet spot: enabled for Enterprise deployments. Auto-creation simplifies corpus management and ensures databases exist when needed. Disable if you need manual control over database creation or want to pre-create databases with custom settings.\n\n• Enabled: Automatic database creation on corpus creation\n• Disabled: Manual database creation required\n• Requirements: Enterprise Edition, per_corpus mode\n• Effect: Controls automatic database provisioning\n• Symptom if disabled: Database not found errors when indexing new corpus",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Neo4j Multi-Database",
          "href": "https://assets.neo4j.com/Official-Materials/Multi+DB+Considerations.pdf"
        },
        {
          "text": "Database Isolation",
          "href": "https://neo4j.com/docs/operations-manual/current/scalability/concepts/"
        },
        {
          "text": "Enterprise Features",
          "href": "https://neo4j.com/docs/operations-manual/current/scalability/concepts/"
        }
      ],
      "badges": [
        {
          "text": "Enterprise",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Neo4j Database Name",
      "key": "neo4j_database",
      "definition": "Neo4j database name used when database_mode is \"shared\". All corpora share this single database with corpus_id filtering. Default: \"neo4j\". For \"per_corpus\" mode, this is ignored and databases are named using the prefix + corpus_id pattern. Must be a valid Neo4j database name.\n\nSweet spot: \"neo4j\" for default installations, custom name for shared multi-corpus setups. Use descriptive names that indicate purpose (e.g., \"tribrid_shared\"). For per_corpus mode, this setting is ignored.\n\n• Default: neo4j (standard database name)\n• Shared mode: Single database name for all corpora\n• Per-corpus mode: Ignored (uses prefix + corpus_id)\n• Effect: Determines database name in shared mode\n• Symptom wrong name: Database not found errors",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Neo4j Multi-Database",
          "href": "https://assets.neo4j.com/Official-Materials/Multi+DB+Considerations.pdf"
        },
        {
          "text": "Database Isolation",
          "href": "https://neo4j.com/docs/operations-manual/current/scalability/concepts/"
        },
        {
          "text": "Database Management",
          "href": "https://neo4j.com/docs/cypher-manual/current/administration/databases/"
        }
      ],
      "badges": [
        {
          "text": "Shared Mode",
          "class": "info"
        }
      ]
    },
    {
      "term": "Neo4j Database Mode",
      "key": "neo4j_database_mode",
      "definition": "Database isolation strategy: \"shared\" uses a single Neo4j database for all corpora (Community-compatible, requires corpus_id filtering), \"per_corpus\" uses separate databases per corpus (Enterprise multi-database, better isolation). Shared mode works with Neo4j Community Edition, per_corpus requires Enterprise Edition.\n\nSweet spot: \"shared\" for Community Edition or small deployments, \"per_corpus\" for Enterprise Edition with strict isolation needs. Per_corpus mode provides better performance (no filtering) and isolation but requires Enterprise licensing.\n\n• Shared: Single database, corpus_id filtering, Community-compatible\n• Per-corpus: Separate databases, no filtering needed, Enterprise required\n• Effect: Determines database isolation strategy\n• Symptom wrong mode: Performance issues (shared) or licensing errors (per_corpus without Enterprise)",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Neo4j Multi-Database",
          "href": "https://assets.neo4j.com/Official-Materials/Multi+DB+Considerations.pdf"
        },
        {
          "text": "Database Isolation",
          "href": "https://neo4j.com/docs/operations-manual/current/scalability/concepts/"
        },
        {
          "text": "Enterprise Features",
          "href": "https://neo4j.com/docs/operations-manual/current/scalability/concepts/"
        }
      ],
      "badges": [
        {
          "text": "Enterprise",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Neo4j Database Prefix",
      "key": "neo4j_database_prefix",
      "definition": "Prefix applied to per-corpus database names when database_mode is \"per_corpus\". Database names are constructed as: {prefix}{corpus_id} (sanitized). Default: \"tribrid_\". The prefix helps identify TriBridRAG-managed databases and prevents conflicts with other applications. Must be a valid database name prefix.\n\nSweet spot: \"tribrid_\" for standard deployments, custom prefix for multi-tenant or organizational needs. Use descriptive prefixes that indicate purpose or organization. Database names are sanitized (lowercase, alphanumeric + underscore only).\n\n• Default: tribrid_\n• Per-corpus mode: Applied to corpus_id to form database name\n• Shared mode: Ignored\n• Effect: Determines database naming pattern in per_corpus mode\n• Symptom wrong prefix: Database naming conflicts or confusion",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Neo4j Multi-Database",
          "href": "https://assets.neo4j.com/Official-Materials/Multi+DB+Considerations.pdf"
        },
        {
          "text": "Database Isolation",
          "href": "https://neo4j.com/docs/operations-manual/current/scalability/concepts/"
        },
        {
          "text": "Database Naming",
          "href": "https://neo4j.com/docs/cypher-manual/current/administration/databases/"
        }
      ],
      "badges": [
        {
          "text": "Per-corpus Mode",
          "class": "info"
        }
      ]
    },
    {
      "term": "Neo4j Password",
      "key": "neo4j_password",
      "definition": "Neo4j database authentication password. Required for basic authentication along with the username. Default: empty (must be set). For security, use environment variables rather than storing passwords in config files. The password must match the Neo4j user account.\n\nSweet spot: Store in environment variable (e.g., NEO4J_PASSWORD), not in config files. Use strong passwords for production deployments. Rotate passwords regularly. For initial setup, Neo4j requires password change on first login.\n\n• Security: Use environment variables, never store in config files\n• Production: Strong passwords, regular rotation\n• Initial setup: Neo4j requires password change on first login\n• Effect: Determines authentication credentials\n• Symptom wrong password: Authentication failures",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Neo4j Authentication",
          "href": "https://neo4j.com/docs/java-manual/current/connect-advanced/"
        },
        {
          "text": "Security Best Practices",
          "href": "https://neo4j.com/product/neo4j-graph-database/security"
        },
        {
          "text": "Connection Configuration",
          "href": "https://neo4j.com/docs/browser-manual/current/operations/dbms-connection/"
        }
      ],
      "badges": [
        {
          "text": "Security",
          "class": "err"
        }
      ]
    },
    {
      "term": "Neo4j Connection URI",
      "key": "neo4j_uri",
      "definition": "Neo4j database connection URI using Bolt protocol. Format: &lt;SCHEME&gt;://&lt;HOST&gt;[:&lt;PORT&gt;]. Schemes: \"bolt\" (no encryption), \"bolt+s\" (TLS with CA cert), \"bolt+ssc\" (TLS with self-signed cert), \"neo4j\" (routing-aware), \"neo4j+s\" (routing + TLS). Default: bolt://localhost:7687. Use \"neo4j\" schemes for cluster routing, \"bolt\" schemes for direct connections.\n\nSweet spot: bolt://localhost:7687 for local development, neo4j+s://your-cluster:7687 for production clusters. Use \"bolt+s\" or \"neo4j+s\" for encrypted connections. Use \"bolt+ssc\" or \"neo4j+ssc\" for self-signed certificates.\n\n• Local dev: bolt://localhost:7687\n• Production cluster: neo4j+s://cluster-host:7687\n• Direct connection: bolt://host:7687\n• Encrypted: bolt+s://host:7687 (CA cert) or bolt+ssc://host:7687 (self-signed)\n• Effect: Determines how the system connects to Neo4j\n• Symptom wrong URI: Connection failures, authentication errors",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Neo4j URI Schemes",
          "href": "https://neo4j.com/docs/upgrade-migration-guide/current/version-4/migration/drivers/new-uri-schemes/"
        },
        {
          "text": "Bolt Protocol",
          "href": "https://neo4j.com/docs/bolt/current/"
        },
        {
          "text": "Connection Configuration",
          "href": "https://neo4j.com/docs/java-manual/current/connect-advanced/"
        }
      ],
      "badges": [
        {
          "text": "Core Setting",
          "class": "info"
        }
      ]
    },
    {
      "term": "Neo4j Username",
      "key": "neo4j_user",
      "definition": "Neo4j database authentication username. Default: \"neo4j\". Used for basic authentication along with the password. For Kerberos or bearer token authentication, different configuration is required. The username must match a Neo4j user account with appropriate permissions.\n\nSweet spot: \"neo4j\" for default installations, custom username for production deployments. Use a dedicated service account with minimal required permissions rather than the admin account. Store credentials securely (environment variables, not in config files).\n\n• Default: neo4j (initial admin user)\n• Production: Custom service account with minimal permissions\n• Security: Use environment variables, not config files\n• Effect: Determines authentication identity\n• Symptom wrong user: Authentication failures",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "Neo4j Authentication",
          "href": "https://neo4j.com/docs/java-manual/current/connect-advanced/"
        },
        {
          "text": "Connection Configuration",
          "href": "https://neo4j.com/docs/browser-manual/current/operations/dbms-connection/"
        },
        {
          "text": "Security Best Practices",
          "href": "https://neo4j.com/product/neo4j-graph-database/security"
        }
      ],
      "badges": [
        {
          "text": "Security",
          "class": "warn"
        }
      ]
    },
    {
      "term": "Repository Keywords",
      "key": "repo_keywords",
      "definition": "Comma-separated keywords for query routing to this repository. When users ask questions containing these keywords, this repo is prioritized. Examples: \"auth,authentication,login\" or \"payment,stripe,billing\". Choose terms users naturally use when asking about this repo's domain. Helps multi-repo setups route queries to the right codebase.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Multi-repo only",
          "class": "info"
        }
      ]
    },
    {
      "term": "Layer Bonuses",
      "key": "repo_layerbonuses",
      "definition": "JSON object mapping intent types to architecture layer bonuses for smart routing. Example: {\"ui\": {\"frontend\": 0.1, \"components\": 0.08}, \"api\": {\"routes\": 0.1, \"controllers\": 0.08}}. When users ask UI questions, code in frontend/ gets a +0.1 boost. Advanced feature for multi-tier architectures. Leave empty if not needed.",
      "category": "general",
      "related": [],
      "links": [
        {
          "text": "JSON Format",
          "href": "https://www.json.org/json-en.html"
        }
      ],
      "badges": [
        {
          "text": "Advanced",
          "class": "warn"
        },
        {
          "text": "Multi-repo only",
          "class": "info"
        }
      ]
    },
    {
      "term": "Repository Path",
      "key": "repo_path",
      "definition": "Absolute filesystem path to the repository directory to be indexed under this logical repo name. Example: /Users/you/projects/myapp or /home/dev/backend. This directory will be scanned for code files during indexing. Use repos.json to configure multiple repositories with different paths, keywords, and routing rules.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Path Boosts",
      "key": "repo_pathboosts",
      "definition": "Comma-separated directory path substrings to boost in search rankings for this repo. Examples: \"src/,app/,lib/\" boosts code in those directories. Use this to prioritize your main application code over tests, docs, or vendor code. Partial matches work - \"api/\" matches \"src/api/\", \"backend/api/\", etc. Boosts are applied during reranking.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": [
        {
          "text": "Affects ranking",
          "class": "info"
        }
      ]
    },
    {
      "term": "REPO_PATH (legacy)",
      "key": "tribrid_PATH",
      "definition": "DEPRECATED: Legacy environment variable for setting the repository path. This is repo-specific and only works for a repo named \"tribrid\". Modern approach: use REPO_PATH for single repos or configure repos.json for multi-repo setups with proper routing. Kept for backwards compatibility - will be removed in future versions.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Repo Path Boosts (CSV)",
      "key": "tribrid_PATH_BOOSTS",
      "definition": "DEPRECATED: Legacy comma-separated path boosts for the \"tribrid\" repository only (e.g., \"app/,lib/,config/\"). Repo-specific environment variables like this don't scale for multi-repo setups. Modern approach: configure path boosts in repos.json per-repo settings. Kept for backwards compatibility.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    },
    {
      "term": "Streaming responses",
      "key": "ui.chat_streaming_enabled",
      "definition": "Streams tokens as they’re generated (instead of waiting for the full answer). This makes chat feel faster and lets you start reading immediately.<br><br>If streaming fails for a provider, the chat UI falls back to a normal non-streaming request automatically.<br><br>This is about delivery, not retrieval: it doesn’t change which sources are queried or what context is selected.",
      "category": "general",
      "related": [],
      "links": [],
      "badges": []
    }
  ]
}
